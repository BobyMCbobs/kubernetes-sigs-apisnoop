#+TITLE: Focused Tests
#+TODO: TEST TESTING PASSED | FAILED

* Purpose
 Investigate tests based on the number of distinct endpionts they hit.  Tests that are only hitting a small proprotion of endpoints are likely written with a focus on them, instead of doing blanket hits.   From here, we can look at tests tat are not conformance, that are hitting stable/core endpoints that are _also_ not hit by any conformance test.  These would likely be good tests to investigate for promotion
* Process
** Take a look at tests and the unique endpoints they hit
   Our audit event view has all the useragents, we can reduce these down to just those that start with `e2e` to grab our tests.
   
   #+NAME: Number of tests
   #+begin_src sql-mode
     SELECT
       COUNT(DISTINCT useragent)
       FROM
           audit_event
      WHERE
        useragent LIKE 'e2e.test%'
        AND job != 'live'
        ;
   #+end_src

   IF we look at a sampling of this, we can see how the useragent is structured.  The test is always given after '--'.
   
   #+NAME: Test Sample
   #+begin_src sql-mode
     SELECT
       DISTINCT useragent
       FROM
           audit_event
      WHERE
             useragent LIKE 'e2e.test%'
         AND job != 'live'
      LIMIT 25
            ;
   #+end_src

   #+RESULTS: Test Sample
   #+begin_src sql-mode
                                                                                                                                            useragent                                                                                                                                         
   -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set [NodeConformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull from private registry with secret [NodeConformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image [NodeConformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull from private registry without secret [NodeConformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull image from invalid registry [NodeConformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
   (25 rows)

   #+end_src

 This means we can user postgres' split_part on useragent to get just the test name.
 
   #+NAME: Test Name Sample
   #+begin_src sql-mode
     SELECT
       DISTINCT split_part(useragent, '--', 2) as test
       FROM
           audit_event
      WHERE
             useragent LIKE 'e2e.test%'
         AND job != 'live'
      LIMIT 10
            ;
   #+end_src

   #+RESULTS: Test Name Sample
   #+begin_src sql-mode
                                                                                                                    test                                                                                                                  
   ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

     [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
     [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
     [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
     [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set [NodeConformance]
   (10 rows)

   #+end_src
   
   Sweet, we have cleanly displayed tests.  Next is to see how many endpoints these tests hit.
   
** See if there are already focused tests.
   
   For each test, I want a count of the distinct endpoints it hits.  We can do this with a postgres count filter.
   
   Ordering by count, descending, will give us the tests that hit the _most_ endpoints.
   #+NAME: Test and Count, Highest 
   #+begin_src sql-mode
     SELECT DISTINCT
       COUNT(distinct operation_id) FILTER(where useragent = audit_event.useragent) as distinct_endpoints,
       split_part(useragent, '--', 2) as test
       FROM
           audit_event
      WHERE
             useragent LIKE 'e2e.test%'
         AND job != 'live'
        GROUP BY useragent
            ORDER BY distinct_endpoints DESC
      LIMIT 25
            ;
   #+end_src

   #+RESULTS: Test and Count, Highest
   #+begin_src sql-mode
    distinct_endpoints |                                                                                          test                                                                                           
   --------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
                   122 |  [sig-cli] Kubectl client kubectl get output should contain custom columns for each resource
                    64 |  [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
                    59 |  [sig-network] Services should create endpoints for unready pods
                    38 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
                    35 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
                    34 |  [sig-storage] CSI mock volume CSI attach test using mock driver should not require VolumeAttach for drivers without attachment
                    34 |  [sig-storage] CSI mock volume CSI attach test using mock driver should require VolumeAttach for drivers with attachment
                    34 |  [sig-storage] CSI mock volume CSI online volume expansion should expand volume without restarting pod if attach=off, nodeExpansion=on
                    34 |  [sig-storage] CSI mock volume CSI Volume expansion should expand volume by restarting pod if attach=off, nodeExpansion=on
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volumes should store data
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source
                    34 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumes should store data
                    33 |  [sig-storage] CSI mock volume CSI workload information using mock driver should be passed when podInfoOnMount=true
                    33 |  [sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when podInfoOnMount=false
                    33 |  [sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when podInfoOnMount=nil
                    33 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
                    33 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
                    33 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory
                    33 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]
                    33 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]
                    33 |  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path
   (25 rows)

   #+end_src
   
   Then the tests that hit the _least_
   #+NAME: Test and Count, Lowest
   #+begin_src sql-mode
     SELECT DISTINCT
       COUNT(distinct operation_id) FILTER(where useragent = audit_event.useragent) as distinct_endpoints,
       split_part(useragent, '--', 2) as test
       FROM
           audit_event
      WHERE
             useragent LIKE 'e2e.test%'
         AND job != 'live'
        GROUP BY useragent
            ORDER BY distinct_endpoints ASC
      LIMIT 25
            ;
   #+end_src

   #+RESULTS: Test and Count, Lowest
   #+begin_src sql-mode
    distinct_endpoints |                                                                    test                                                                    
   --------------------+--------------------------------------------------------------------------------------------------------------------------------------------
                     3 |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"
                     3 |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json,application/vnd.kubernetes.protobuf"
                     3 |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf"
                     3 |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf,application/json"
                     6 | 
                     7 |  [k8s.io] Probing container should be restarted with a docker exec liveness probe with timeout 
                     7 |  [k8s.io] [sig-node] crictl should be able to run crictl on the node
                     7 |  [k8s.io] [sig-node] SSH should SSH to all nodes and run commands
                     7 |  [sig-api-machinery] Servers with support for Table transformation should return generic metadata details across all namespaces for nodes
                     7 |  [sig-auth] PodSecurityPolicy should forbid pod creation when no PSP is available
                     7 |  [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
                     7 |  [sig-cli] Kubectl client Kubectl apply apply set/view last-applied
                     7 |  [sig-cli] Kubectl client Kubectl apply should reuse port when apply to an existing SVC
                     7 |  [sig-cli] Kubectl client Kubectl cluster-info dump should check if cluster-info dump succeeds
                     7 |  [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]
                     7 |  [sig-cli] Kubectl client Kubectl create quota should reject quota with invalid scopes
                     7 |  [sig-cli] Kubectl client Kubectl get componentstatuses should get componentstatuses
                     7 |  [sig-cli] Kubectl client Proxy server should support 
                     7 |  [sig-cli] Kubectl client Proxy server should support proxy with 
                     7 |  [sig-instrumentation] MetricsGrabber should grab all metrics from API server.
                     7 |  [sig-network] [sig-windows] Networking Granular Checks: Pods should function for intra-pod communication: http
                     7 |  [sig-network] [sig-windows] Networking Granular Checks: Pods should function for node-pod communication: http
                     7 |  [sig-network] [sig-windows] Networking Granular Checks: Pods should function for node-pod communication: udp
                     7 |  [sig-scheduling] Multi-AZ Clusters should spread the pods of a replication controller across zones
                     7 |  [sig-scheduling] Multi-AZ Clusters should spread the pods of a service across zones
   (25 rows)

   #+end_src

   
   So this looks like the lowest # of distinct endpoints hit by a test is 3.  

I want to do a quick sanity check, to validate this count filter.  I'll grab two tests from the above views and list their distinct operation_id's.  The number of records should match the count.
   
One with three:
   #+NAME: Test with distinct_endpoint count of 3
   #+begin_src sql-mode
     select distinct
       operation_id
       from audit_event
      where useragent like '%[sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"'
            and job != 'live'
            ;
-- records returns should be 3
   #+end_src

   #+RESULTS: Test with distinct_endpoint count of 3
   #+begin_src sql-mode
           operation_id         
   -----------------------------
    listCoreV1NamespacedService
    listCoreV1Node
    readCoreV1NamespacedService
   (3 rows)

   #+end_src
   
And one with 7:
   
   #+NAME: Test with distinct_endpoint count of 7
   #+begin_src sql-mode
          select distinct
            operation_id
            from audit_event
           where useragent like '%[sig-scheduling] Multi-AZ Clusters should spread the pods of a replication controller across zones'
                 and job != 'live'
                 ;
     -- records returns should be 3
   #+end_src

   #+RESULTS: Test with distinct_endpoint count of 7
   #+begin_src sql-mode
                     operation_id                  
   ------------------------------------------------
    createAuthorizationV1SubjectAccessReview
    createCoreV1Namespace
    createRbacAuthorizationV1NamespacedRoleBinding
    deleteCoreV1Namespace
    listCoreV1NamespacedServiceAccount
    listCoreV1Node
    readCoreV1Namespace
   (7 rows)

   #+end_src

   The numbers match, and the logic is simple enough, that I feel confident in the approach.
** Check out Distribution of distinct endpoints
   
   I am curious  on the distribution in the tests, if there's a common number of endpoints that are hit.  I want to start focusing on just stable endpoints, as these will be the onees with highest priority of becoming conformant.  I'll run some postgres averages on the above query.
   
   #+NAME: stats for tests that hit stable|core endpoints
   #+begin_src sql-mode
     select
       count(distinct test) as total_tests,
       MAX(distinct_endpoints) as max_endpoints_hit_by_test,
       MIN(distinct_endpoints) as min_endpoints_hit_by_test,
       AVG(distinct_endpoints) as avg_endpoints_hit_by_test
           FROM (
     SELECT 
       COUNT(distinct ae.operation_id) FILTER(where useragent = ae.useragent) as distinct_endpoints,
       split_part(useragent, '--', 2) as test,
       useragent
       FROM
           audit_event ae
      JOIN api_operation_material ao ON (ae.operation_id = ao.operation_id)
      WHERE
         useragent LIKE 'e2e.test%'
         AND ae.job != 'live'
         AND ao.level = 'stable'
        GROUP BY useragent
            ORDER BY distinct_endpoints DESC
           ) as tests
            ;
   #+end_src

   #+RESULTS: stats for tests that hit stable endpoints
   #+begin_src sql-mode
    total_tests | max_endpoints_hit_by_test | min_endpoints_hit_by_test | avg_endpoints_hit_by_test 
   -------------+---------------------------+---------------------------+---------------------------
            828 |                        80 |                         3 |       14.0374396135265700
   (1 row)

   #+end_src

   There is more we can do with distribution and means and such, but I'd say looking at tests that hit 14 or less endpoints would be useful.
   
   Now, let's run similar queries on our endpoins.
   
** Check out low-tested endpoints.  
   
   For these, I'll also focus on just stable endpoints.
   
   #+NAME: Low Tested Endpoints
   #+begin_src sql-mode
     SELECT
          COUNT(distinct ae.useragent) FILTER(where ae.operation_id = ae.operation_id) as distinct_tests,
            ae.operation_id
            FROM
            audit_event ae
            JOIN api_operation_material ao on (ae.operation_id = ao.operation_id)
            WHERE
            useragent LIKE 'e2e.test%'
            AND ae.job != 'live'
            AND ao.level = 'stable'
            GROUP BY ae.operation_id 
            ORDER BY distinct_tests DESC
            ;
   #+end_src

   #+RESULTS: Low Tested Endpoints
   #+begin_src sql-mode
    distinct_tests |                             operation_id                              
   ----------------+-----------------------------------------------------------------------
               828 | listCoreV1Node
               823 | createCoreV1Namespace
               823 | readCoreV1Namespace
               823 | listCoreV1NamespacedServiceAccount
               823 | deleteCoreV1Namespace
               822 | createAuthorizationV1SubjectAccessReview
               822 | createRbacAuthorizationV1NamespacedRoleBinding
               582 | readCoreV1NamespacedPod
               560 | createCoreV1NamespacedPod
               452 | deleteCoreV1NamespacedPod
               393 | listCoreV1NamespacedPod
               303 | readCoreV1NamespacedPodLog
               241 | deleteCoreV1NamespacedPersistentVolumeClaim
               237 | createCoreV1NamespacedPersistentVolumeClaim
               232 | readCoreV1NamespacedPersistentVolumeClaim
               211 | readCoreV1PersistentVolume
               204 | readCoreV1Node
               186 | connectCoreV1PostNamespacedPodExec
               161 | createCoreV1PersistentVolume
               161 | deleteCoreV1PersistentVolume
               117 | deleteStorageV1StorageClass
               116 | createStorageV1StorageClass
                98 | createCoreV1NamespacedService
                67 | deleteCoreV1NamespacedService
                62 | createCoreV1NamespacedConfigMap
                60 | createRbacAuthorizationV1ClusterRoleBinding
                58 | deleteRbacAuthorizationV1NamespacedRoleBinding
                56 | deleteRbacAuthorizationV1ClusterRoleBinding
                49 | createCoreV1NamespacedSecret
                47 | deleteAppsV1NamespacedStatefulSet
                47 | createAppsV1NamespacedStatefulSet
                41 | createCoreV1NamespacedServiceAccount
                40 | createRbacAuthorizationV1ClusterRole
                39 | createRbacAuthorizationV1NamespacedRole
                39 | deleteCoreV1NamespacedServiceAccount
                38 | deleteRbacAuthorizationV1ClusterRole
                38 | readCoreV1NamespacedService
                37 | deleteRbacAuthorizationV1NamespacedRole
                34 | createAppsV1NamespacedDeployment
                33 | listAppsV1NamespacedReplicaSet
                32 | readAppsV1NamespacedDeployment
                31 | createCoreV1NamespacedReplicationController
                31 | listCoreV1NamespacedEndpoints
                30 | listRbacAuthorizationV1ClusterRole
                28 | createApiextensionsV1CustomResourceDefinition
                27 | deleteAppsV1NamespacedDeployment
                27 | deleteApiextensionsV1CustomResourceDefinition
                27 | deleteCoreV1NamespacedSecret
                23 | createCoreV1NamespacedEndpoints
                23 | deleteCoreV1NamespacedEndpoints
                19 | replaceCoreV1NamespacedPersistentVolumeClaim
                17 | readStorageV1StorageClass
                17 | replaceCoreV1Namespace
                17 | deleteCoreV1NamespacedReplicationController
                17 | readCoreV1NamespacedResourceQuota
                17 | readCoreV1NamespacedReplicationController
                17 | deleteCoreV1NamespacedConfigMap
                16 | createCoreV1NamespacedResourceQuota
                15 | createAppsV1NamespacedReplicaSet
                12 | listAppsV1NamespacedDeployment
                12 | replaceCoreV1NamespacedService
                12 | replaceCoreV1NamespacedConfigMap
                11 | readBatchV1NamespacedJob
                11 | replaceCoreV1NamespacedPod
                10 | listCoreV1NamespacedResourceQuota
                10 | createAdmissionregistrationV1ValidatingWebhookConfiguration
                10 | createBatchV1NamespacedJob
                 9 | replaceAppsV1NamespacedStatefulSet
                 9 | listCoreV1PersistentVolume
                 9 | deleteAdmissionregistrationV1ValidatingWebhookConfiguration
                 9 | createAdmissionregistrationV1MutatingWebhookConfiguration
                 9 | listCoreV1NamespacedPersistentVolumeClaim
                 9 | readAppsV1NamespacedStatefulSet
                 9 | listAppsV1NamespacedStatefulSet
                 8 | createCoreV1NamespacedPodEviction
                 8 | readCoreV1NamespacedEndpoints
                 8 | deleteAdmissionregistrationV1MutatingWebhookConfiguration
                 7 | listStorageV1StorageClass
                 7 | readCoreV1NamespacedConfigMap
                 6 | listCoreV1NamespacedConfigMap
                 6 | getAPIVersions
                 5 | listCoreV1NamespacedReplicationController
                 5 | deleteBatchV1NamespacedJob
                 5 | listBatchV1NamespacedJob
                 5 | replaceAppsV1NamespacedDeployment
                 4 | listCoreV1NamespacedService
                 4 | getAdmissionregistrationV1APIResources
                 4 | getApiextensionsV1APIResources
                 4 | readAppsV1NamespacedReplicaSet
                 4 | getCoreAPIVersions
                 3 | getAutoscalingV1APIResources
                 3 | createCoreV1NamespacedPodTemplate
                 3 | readCoreV1NamespacedServiceAccount
                 3 | getCoordinationV1APIResources
                 3 | patchApiextensionsV1CustomResourceDefinition
                 3 | getBatchV1APIResources
                 3 | getApiregistrationV1APIResources
                 3 | getSchedulingV1APIResources
                 3 | readStorageV1VolumeAttachment
                 3 | getStorageV1APIResources
                 3 | getCoreV1APIResources
                 3 | readCoordinationV1NamespacedLease
                 3 | getAppsV1APIResources
                 3 | getNetworkingV1APIResources
                 3 | getAuthenticationV1APIResources
                 3 | getRbacAuthorizationV1APIResources
                 3 | getAuthorizationV1APIResources
                 3 | readApiextensionsV1CustomResourceDefinition
                 3 | deleteCoreV1NamespacedResourceQuota
                 2 | replaceCoreV1NamespacedResourceQuota
                 2 | deleteApiregistrationV1APIService
                 2 | deleteCoordinationV1NamespacedLease
                 2 | listCoordinationV1NamespacedLease
                 2 | createSchedulingV1PriorityClass
                 2 | readCoreV1NamespacedSecret
                 2 | createCoordinationV1NamespacedLease
                 2 | deleteSchedulingV1PriorityClass
                 2 | replaceApiextensionsV1CustomResourceDefinition
                 2 | patchCoreV1NamespacedPod
                 2 | patchCoreV1Node
                 2 | listCoreV1NamespacedPodTemplate
                 2 | deleteAppsV1NamespacedReplicaSet
                 2 | createApiregistrationV1APIService
                 2 | replaceCoreV1NamespacedReplicationController
                 2 | replaceCoreV1NamespacedSecret
                 2 | connectCoreV1GetNamespacedPodPortforward
                 1 | createCoreV1NamespacedLimitRange
                 1 | createAutoscalingV1NamespacedHorizontalPodAutoscaler
                 1 | replaceAdmissionregistrationV1MutatingWebhookConfiguration
                 1 | replaceAdmissionregistrationV1ValidatingWebhookConfiguration
                 1 | replaceApiextensionsV1CustomResourceDefinitionStatus
                 1 | replaceCoreV1NodeStatus
                 1 | replaceAppsV1NamespacedReplicaSet
                 1 | createAuthorizationV1SelfSubjectAccessReview
                 1 | replaceAppsV1NamespacedStatefulSetScale
                 1 | replaceCoordinationV1NamespacedLease
                 1 | createAuthenticationV1TokenReview
                 1 | createAppsV1NamespacedDaemonSet
                 1 | replaceCoreV1NamespacedLimitRange
                 1 | createAppsV1NamespacedControllerRevision
                 1 | replaceCoreV1NamespacedReplicationControllerScale
                 1 | replaceCoreV1NamespacedServiceAccount
                 1 | deleteAdmissionregistrationV1CollectionValidatingWebhookConfiguration
                 1 | listAdmissionregistrationV1MutatingWebhookConfiguration
                 1 | listAdmissionregistrationV1ValidatingWebhookConfiguration
                 1 | listApiextensionsV1CustomResourceDefinition
                 1 | listAppsV1NamespacedDaemonSet
                 1 | getApiextensionsAPIGroup
                 1 | getAdmissionregistrationAPIGroup
                 1 | replaceCoreV1Node
                 1 | listCoreV1Namespace
                 1 | deleteStorageV1VolumeAttachment
                 1 | deleteStorageV1CSINode
                 1 | listCoreV1NamespacedLimitRange
                 1 | deleteNetworkingV1NamespacedNetworkPolicy
                 1 | connectCoreV1GetNamespacedPodExec
                 1 | listCoreV1NamespacedSecret
                 1 | deleteCoreV1NamespacedPodTemplate
                 1 | deleteCoreV1NamespacedLimitRange
                 1 | listCoreV1PodForAllNamespaces
                 1 | deleteCoordinationV1CollectionNamespacedLease
                 1 | deleteAutoscalingV1NamespacedHorizontalPodAutoscaler
                 1 | logFileListHandler
                 1 | patchAdmissionregistrationV1MutatingWebhookConfiguration
                 1 | patchAdmissionregistrationV1ValidatingWebhookConfiguration
                 1 | patchApiextensionsV1CustomResourceDefinitionStatus
                 1 | patchCoordinationV1NamespacedLease
                 1 | patchCoreV1NamespacedConfigMap
                 1 | patchCoreV1NamespacedPodStatus
                 1 | readAdmissionregistrationV1MutatingWebhookConfiguration
                 1 | readAdmissionregistrationV1ValidatingWebhookConfiguration
                 1 | readApiextensionsV1CustomResourceDefinitionStatus
                 1 | readApiregistrationV1APIService
                 1 | deleteAppsV1NamespacedDaemonSet
                 1 | readAppsV1NamespacedStatefulSetScale
                 1 | deleteAppsV1NamespacedControllerRevision
                 1 | deleteApiextensionsV1CollectionCustomResourceDefinition
                 1 | readCoreV1NamespacedLimitRange
                 1 | deleteAdmissionregistrationV1CollectionMutatingWebhookConfiguration
                 1 | createStorageV1VolumeAttachment
                 1 | createStorageV1CSINode
                 1 | readCoreV1NamespacedReplicationControllerScale
                 1 | createNetworkingV1NamespacedNetworkPolicy
   (183 rows)

   #+end_src
   
   And another sanity check.
   #+NAME: distinct tests hitting endpoint, should be 9
   #+begin_src sql-mode
              SELECT distinct
                useragent
                FROM
                    audit_event
               WHERE
     operation_id = 'listCoreV1PersistentVolume'
                 AND useragent like 'e2e.test%'
                 ;
   #+end_src
   

   #+RESULTS: distinct tests hitting endpoint, should be 9
   #+begin_src sql-mode
                                                                                                                   useragent                                                                                                                 
   ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should adopt matching orphans and release non-matching pods
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should implement legacy replacement when the update strategy is OnDelete
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should not deadlock when a pod's predecessor fails
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications with PVCs
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should provide basic identity
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
   (9 rows)

   #+end_src

   #+NAME: distinct tests hitting endpoint, should be 1
   #+begin_src sql-mode
              SELECT distinct
                useragent
                FROM
                    audit_event
               WHERE
     operation_id = 'readCoreV1NamespacedLimitRange'
                 AND useragent like 'e2e.test%'
                 ;
   #+end_src

   #+RESULTS: distinct tests hitting endpoint, should be 1
   #+begin_src sql-mode
                                                                                 useragent                                                                               
   ----------------------------------------------------------------------------------------------------------------------------------------------------------------------
    e2e.test/v1.18.0 (linux/amd64) kubernetes/af1baaa -- [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied.
   (1 row)

   #+end_src
   
   Feel good with this.

** List the focused tests
   This is to create a query of tests we can join with our list of endpoints, to start to find some focused test\endpoint pariings.  Here we want to list all tests that hit less than 10 endpoints, and the distinct endpoints they hit.  This will end up being a long query, as it will show all possible pairs, and so I will not run its results.
   
   #+NAME: focused tests 
   #+begin_src sql-mode
     WITH tests as (
       SELECT DISTINCT
         COUNT(distinct operation_id) FILTER(where useragent = audit_event.useragent) as distinct_endpoints,
         split_part(useragent, '--', 2) as test,
         useragent
         FROM
             audit_event
        WHERE
              useragent LIKE 'e2e.test%'
          AND job != 'live'
        GROUP BY useragent
     )
     SELECT DISTINCT
       audit_event.operation_id,
       test, 
       tests.distinct_endpoints
       FROM tests
         JOIN
         audit_event on (audit_event.useragent = tests.useragent)
      WHERE distinct_endpoints < 10
       ORDER BY distinct_endpoints asc, test
            ;
   #+end_src

** List distinct endpoints
Similarly, we want to list the endpoints and the tests that hit them.

Both of these lists will become CTE's in our larger postgres query, but it's good to define them now explicitly here.

   #+NAME: List Low Tested Endpoints
   #+begin_src sql-mode
     WITH stable_endpoints AS (
     SELECT DISTINCT 
          COUNT(distinct ae.useragent) FILTER(where ae.operation_id = ae.operation_id) as distinct_tests,
            ae.operation_id
            FROM
            audit_event ae
            JOIN endpoint_coverage ec on (ae.operation_id = ec.operation_id)
            WHERE
            useragent LIKE 'e2e.test%'
            AND ae.job != 'live'
            AND ec.level = 'stable'
      GROUP BY ae.operation_id
      ORDER BY operation_id
     )       
     SELECT
       operation_id
       FROM
           stable_endpoints
           WHERE distinct_tests < 5
         ;

   #+end_src

   
   Now we will list the same # of distinctly tested endpoints, but grabbing the test that hits them.  We cannot do it in the above query, because it will create too many duplicate rows.  We also wanna limit the audit events we look at, so we are not seeing any general hit.
   
   #+NAME: List Low Tested Endpoints
   #+begin_src sql-mode
     WITH stable_endpoints AS (
     SELECT
          COUNT(distinct ae.useragent) FILTER(where ae.operation_id = ae.operation_id) as distinct_tests,
            ae.operation_id
            FROM
            audit_event ae
            JOIN endpoint_coverage ec on (ae.operation_id = ec.operation_id)
            WHERE
            useragent LIKE 'e2e.test%'
            AND ae.job != 'live'
            AND ec.level = 'stable'
            GROUP BY ae.operation_id
     )
     SELECT DISTINCT
       stable_endpoints.operation_id,
       split_part(ae.useragent, '--', 2) as test,
       distinct_tests
       FROM
           stable_endpoints
           JOIN
           audit_event ae on (ae.operation_id = stable_endpoints.operation_id)
           WHERE distinct_tests < 5
            AND ae.useragent like 'e2e.test%'
            ORDER BY operation_id
            ;
   #+end_src


** Combinee and Except
   
   So now we want to combine these two tables.  We ahve a list of tests that hit a small amount of endpoints, and the endpoints they hit.  We also have a list of endpoints that are hit by a small amount of tests, and the tests they hit.
   
   We can just select tests from the top and do an intersect clause for the endpoints at the bottom.  This would return the rows that are in the top and bottom.
   
   In the main intersect clause, we can set whatever numbers we want for the where clause, to expand or focus our net as needed.  I am also filtering the tests to only non-conformance ones  as the purpose of the view is to promote tests.

  To start, I want to look at the pairing of tests that hit less than 5 endpoints, and the endpoints they hit that hit by less than 5 tests.  5 is slightly arbitrary, except I know from the distribution above that there will be test results, but not that many.
   
   #+NAME: focused tests and endpoints
   #+begin_src sql-mode
     WITH tests as (
       SELECT DISTINCT
         COUNT(distinct operation_id) FILTER(where useragent = audit_event.useragent) as distinct_endpoints,
         split_part(useragent, '--', 2) as test,
         useragent
         FROM
             audit_event
        WHERE
              useragent LIKE 'e2e.test%'
          AND job != 'live'
        GROUP BY useragent
     ), stable_endpoints AS (
     SELECT
          COUNT(distinct ae.useragent) FILTER(where ae.operation_id = ae.operation_id) as distinct_tests,
            ae.operation_id
            FROM
            audit_event ae
            JOIN endpoint_coverage ec on (ae.operation_id = ec.operation_id)
            WHERE
            useragent LIKE 'e2e.test%'
            AND ae.job != 'live'
            AND ec.level = 'stable'
            GROUP BY ae.operation_id
     )
    
     (SELECT DISTINCT
       audit_event.operation_id,
       test
       FROM tests
         JOIN
         audit_event on (audit_event.useragent = tests.useragent)
      WHERE distinct_endpoints < 5)
       INTERSECT
     (SELECT DISTINCT
       stable_endpoints.operation_id,
       split_part(ae.useragent, '--', 2) as test
       FROM
           stable_endpoints
           JOIN
           audit_event ae on (ae.operation_id = stable_endpoints.operation_id)
           -- WHERE stable_endpoints.operation_id = 'createAuthenticationV1TokenReview'
           WHERE distinct_tests < 5
            AND ae.useragent like 'e2e.test%')
            ;
   #+end_src

   #+RESULTS: focused tests and endpoints
   #+begin_src sql-mode
           operation_id         |                                                                    test                                                                    
   -----------------------------+--------------------------------------------------------------------------------------------------------------------------------------------
    listCoreV1NamespacedService |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"
    listCoreV1NamespacedService |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json,application/vnd.kubernetes.protobuf"
    listCoreV1NamespacedService |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf,application/json"
    listCoreV1NamespacedService |  [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf"
   (4 rows)

   #+end_src
   
   Great, we have 4 tests that all hit a single endpoint, and judging from their description are thematically connected.  As a bonus, these tests are not yet conformant (we'll write a query below that filters to non-cofnor.  To doulbe-check our query though, let's look through each one and make sure they are hitting less than 5 endpoints each.
   
   #+NAME: Investigating test 1
   #+begin_src sql-mode
     SELECT DISTINCT
       operation_id
       FROM audit_event
      WHERE
        useragent like '%[sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"'
   ;
   #+end_src

   #+RESULTS: Investigating test 1
   #+begin_src sql-mode
           operation_id         
   -----------------------------
    listCoreV1NamespacedService
    listCoreV1Node
    readCoreV1NamespacedService
   (3 rows)

   #+end_src
   
   #+NAME: Investigating test 2
   #+begin_src sql-mode
     SELECT DISTINCT
       operation_id
       FROM audit_event
      WHERE
        useragent like '%[sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json,application/vnd.kubernetes.protobuf"'
   ;
   #+end_src

   #+RESULTS: Investigating test 2
   #+begin_src sql-mode
           operation_id         
   -----------------------------
    listCoreV1NamespacedService
    listCoreV1Node
    readCoreV1NamespacedService
   (3 rows)

   #+end_src
   
   I have a feeling they are all gonna be three, and the same three. but let's be explicit.
   
   #+NAME: Investigating test 3
   #+begin_src sql-mode
       SELECT DISTINCT
         operation_id
         FROM audit_event
        WHERE
          useragent like '%[sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf,application/json"'
     ;
   #+end_src

   #+RESULTS: Investigating test 3
   #+begin_src sql-mode
           operation_id         
   -----------------------------
    listCoreV1NamespacedService
    listCoreV1Node
    readCoreV1NamespacedService
   (3 rows)

   #+end_src
   
   #+NAME: Investigating test 4
   #+begin_src sql-mode
       SELECT DISTINCT
         operation_id
         FROM audit_event
        WHERE
          useragent like '%[sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf"'
     ;
   #+end_src

   #+RESULTS: Investigating test 4
   #+begin_src sql-mode
           operation_id         
   -----------------------------
    listCoreV1NamespacedService
    listCoreV1Node
    readCoreV1NamespacedService
   (3 rows)

   #+end_src
   
   As expected.  Now, we want to make sure listCorev1NamespacedService is _also_ only hit by less than 5 tests.
   
   #+NAME: Investigating endpoint
   #+begin_src sql-mode
       SELECT DISTINCT
         split_part(ae.useragent, '--',2) as test
         FROM audit_event ae
        WHERE
          ae.operation_id = 'listCoreV1NamespacedService'
          AND useragent like 'e2e.test%'
     ;
   #+end_src

   #+RESULTS: Investigating endpoint
   #+begin_src sql-mode
                                                                       test                                                                    
   --------------------------------------------------------------------------------------------------------------------------------------------
     [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"
     [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json,application/vnd.kubernetes.protobuf"
     [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf"
     [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/vnd.kubernetes.protobuf,application/json"
   (4 rows)

   #+end_src
   
   Beautiful.  It's the same results as before, but we know our combined query is working.  This means we could continue to use this setting various filters as needed.  In the meantime, let's take a look at these tests and see if they are valid for promotion.
   
* Checking Whether our chosen tests are valid for promotion   
  
  The guide for conformance tests is as follows:

#+BEGIN_QUOTE
Conformance tests currently test only GA, non-optional features or APIs. More specifically, a test is eligible for promotion to conformance if:

    it tests only GA, non-optional features or APIs (e.g., no alpha or beta endpoints, no feature flags required, no deprecated features)
    it works for all providers (e.g., no SkipIfProviderIs/SkipUnlessProviderIs calls)
    it is non-privileged (e.g., does not require root on nodes, access to raw network interfaces, or cluster admin permissions)
    it works without access to the public internet (short of whatever is required to pre-pull images for conformance tests)
    it works without non-standard filesystem permissions granted to pods
    it does not rely on any binaries that would not be required for the linux kernel or kubelet to run (e.g., can't rely on git)
    where possible, it does not depend on outputs that change based on OS (nslookup, ping, chmod, ls)
    any container images used within the test support all architectures for which kubernetes releases are built
    it passes against the appropriate versions of kubernetes as spelled out in the conformance test version skew policy
    it is stable and runs consistently (e.g., no flakes), and has been running for at least one release cycle
    new conformance tests or updates to conformance tests for additional scenarios are only allowed before code freeze dates set by the release team to allow enough soak time of the changes and gives folks a chance to kick the tires either in the community CI or their own infrastructure to make sure the tests are robust

Examples of features which are not currently eligible for conformance tests:

    node/platform-reliant features, eg: multiple disk mounts, GPUs, high density, etc.
    optional features, eg: policy enforcement
    cloud-provider-specific features, eg: GCE monitoring, S3 Bucketing, etc.
    anything that requires a non-default admission plugin

Conformance tests are intended to be stable and backwards compatible according to the standard API deprecation policies. Therefore any test that relies on specific output that is not subject to the deprecation policy cannot be promoted to conformance. Examples of tests which are not eligible to conformance:

    anything that checks specific Events are generated, as we make no guarantees about the contents of events, nor their delivery
        If a test depends on events it is recommended to change the test to use an informer pattern and watch specific resource changes instead.
    anything that checks optional Condition fields, such as Reason or Message, as these may change over time (however it is reasonable to verify these fields exist or are non-empty)
        If the test is checking for specific conditions or reasons, it is considered overly specific and it is recommended to simply look for pass/failure criteria where possible, and output the condition/reason for debugging purposes only.

Examples of areas we may want to relax these requirements once we have a sufficient corpus of tests that define out of the box functionality in all reasonable production worthy environments:

    tests may need to create or set objects or fields that are alpha or beta that bypass policies that are not yet GA, but which may reasonably be enabled on a conformant cluster (e.g., pod security policy, non-GA scheduler annotations)
#+END_QUOTE
  
I want to break this quote into todo's, and see how many we can confirm here before investigating the test itself.

** [sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"
   
*** PASSED it tests only GA, non-optional features or APIs (e.g., no alpha or beta endpoints, no feature flags required, no deprecated features)
    We can check this using our api queries.  We'll run through all op_id's that our test touches, and check their properties and parameters. 
    If any op_id is alpha or beta, or deprecated, then the test is not a candidate for promotion.
    If any op_id has a required parameter that is turned on with a feature gate, it is also not a candidate for promotion.
    
    #+NAME: Tests only GA, non opt features or APIs
    #+begin_src sql-mode
      SELECT DISTINCT
        ae.operation_id,
        ao.level,
        ao.deprecated,
        ap.param_name,
        ap.required,
        CASE
            WHEN (ap.param_description ~ 'feature gate') then true
        ELSE false
            END as feature_gate
        FROM audit_event ae
               JOIN api_operation_material ao on (ae.operation_id = ao.operation_id)
               JOIN api_operation_parameter_material ap on (ae.operation_id = ap.param_op)
       WHERE
         ae.useragent like '%[sig-api-machinery] client-go should negotiate watch and report errors with accept "application/json"'
         ;
    #+end_src

    #+RESULTS: Tests only GA, non opt features or APIs
    #+begin_src sql-mode
            operation_id         | level  | deprecated |     param_name      | required | feature_gate 
    -----------------------------+--------+------------+---------------------+----------+--------------
     listCoreV1NamespacedService | stable | f          | allowWatchBookmarks | f        | t
     listCoreV1NamespacedService | stable | f          | continue            | f        | f
     listCoreV1NamespacedService | stable | f          | fieldSelector       | f        | f
     listCoreV1NamespacedService | stable | f          | labelSelector       | f        | f
     listCoreV1NamespacedService | stable | f          | limit               | f        | f
     listCoreV1NamespacedService | stable | f          | resourceVersion     | f        | f
     listCoreV1NamespacedService | stable | f          | timeoutSeconds      | f        | f
     listCoreV1NamespacedService | stable | f          | watch               | f        | f
     listCoreV1Node              | stable | f          | allowWatchBookmarks | f        | t
     listCoreV1Node              | stable | f          | continue            | f        | f
     listCoreV1Node              | stable | f          | fieldSelector       | f        | f
     listCoreV1Node              | stable | f          | labelSelector       | f        | f
     listCoreV1Node              | stable | f          | limit               | f        | f
     listCoreV1Node              | stable | f          | resourceVersion     | f        | f
     listCoreV1Node              | stable | f          | timeoutSeconds      | f        | f
     listCoreV1Node              | stable | f          | watch               | f        | f
     readCoreV1NamespacedService | stable | f          | exact               | f        | f
     readCoreV1NamespacedService | stable | f          | export              | f        | f
    (18 rows)

    #+end_src
    
    So far so good!

*** TESTING it is stable and runs consistently (e.g., no flakes), and has been running for at least one release cycle
    We can check this using test grid.
    
    Checking if flaky

- Navigate to [[https://testgrid.k8s.io/sig-release-master-blocking#gce-cos-master-default][this filter]on testgrid.k8s.io.
- Find the test using "Include Filter by Regex" under the "Options" drop-down menu
- Once the test is isolated then select "Super Compact" under the "Size" drop-down menu
- What you want to see is all green on the right. (If there are some gaps, that is okay.)

Check if  Slow

- Continuing from above, turn on the "test-duration-minutes" graph under the "Graph" drop-down menu
- If you don't see any changes then refresh the page
- Now there should be a graph showing how long the test took to run.
- 5 minutes is the threshold for a Slow test

*** TEST it works for all providers (e.g., no SkipIfProviderIs/SkipUnlessProviderIs calls)
*** TEST it is non-privileged (e.g., does not require root on nodes, access to raw network interfaces, or cluster admin permissions)
*** TEST it works without access to the public internet (short of whatever is required to pre-pull images for conformance tests)
*** TEST it works without non-standard filesystem permissions granted to pods
*** TEST it does not rely on any binaries that would not be required for the linux kernel or kubelet to run (e.g., can't rely on git)
*** TEST where possible, it does not depend on outputs that change based on OS (nslookup, ping, chmod, ls)
*** TEST any container images used within the test support all architectures for which kubernetes releases are built
*** TEST it passes against the appropriate versions of kubernetes as spelled out in the conformance test version skew policy
* Conclusions | Next Steps
* Footnotes


** 250: api_schema view
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/250_view_api_schema.up.sql
  :END:
*** Create

 #+NAME: api_schema view
 #+BEGIN_SRC sql-mode 
   CREATE OR REPLACE VIEW "public"."api_schema" AS 
    SELECT 
       bjs.bucket,
       bjs.job,
       d.key AS schema_name,
       (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'kind'::text) AS k8s_kind,
       (d.value ->> 'type'::text) AS resource_type,
       (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'version'::text) AS k8s_version,
       (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'group'::text) AS k8s_group,
       ARRAY(SELECT jsonb_array_elements_text(d.value -> 'required')) as required_fields,
       (d.value -> 'properties'::text) AS properties,
       d.value
      FROM bucket_job_swagger bjs
        , jsonb_each((bjs.swagger -> 'definitions'::text)) d(key, value)
      GROUP BY bjs.bucket, bjs.job, d.key, d.value;

 #+END_SRC

 #+RESULTS: api_schema view
 #+begin_src sql-mode
   CREATE VIEW
 #+end_src

** 260: api_schema_field view
  :PROPERTIES:
  :header-args:sql-mode+: :tangle ../apps/hasura/migrations/260_view_api_schema_field.up.sql
  :END:
*** Create

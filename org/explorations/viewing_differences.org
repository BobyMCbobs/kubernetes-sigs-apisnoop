#+TITLE: Viewing Differences
#+TODO: TODO IN-PROGRESS | DONE

* Purpose
  Explore how we could view the differences between two test runs, in preparation for a prow bot that wants to show an increase or decrease in coverage
* [6/9] Goals
** DONE Take two most recent audit events and display percentage of stable coverage for both
   CLOSED: [2019-10-27 Sun 21:35]
   This is a view we've kind of already made in a previous ticket, and can bring back here.

   #+NAME: Stats for Last Two Runs
   #+BEGIN_SRC sql-mode :results replace 
     select
       ,*
       FROM
           stable_endpoint_stats
      ORDER BY 
        date DESC
      LIMIT 2
            ;
   #+END_SRC

   #+RESULTS: Stats for Last Two Runs
   #+begin_src sql-mode
            job         |    date    | total_endpoints | test_hits | conf_hits | percent_tested | percent_conf_tested 
   ---------------------+------------+-----------------+-----------+-----------+----------------+---------------------
    1181584183475048448 | 2019-10-08 |             430 |       165 |       114 |          38.37 |               26.51
    1178464478988079104 | 2019-09-30 |             430 |       171 |       124 |          39.77 |               28.84
   (2 rows)

   #+end_src

   This is simplistic, but good enough to build comparison views.  Likely in our prow job, we'll be creating a db that only has two runs in it, and so this limit wouldn't be necessary.
   
** DONE Display the increase or decrease in coverage between two.
   CLOSED: [2019-10-27 Sun 22:37]
   If we were making a site or doc, we could just subtract the two fields.  I'm not sure we have to do this in a direct sql query, but it could be nice to have a "difference view"  that shows 
   #+NAME: Difference View
   #+BEGIN_SRC sql-mode :results replace 
     with last_two_runs as (
       select
         ,*
         FROM
             stable_endpoint_stats
        ORDER BY 
          date DESC
        LIMIT 2
     ), new_coverage as (
       SELECT *
         FROM last_two_runs
        order by date desc
        limit 1
     ), old_coverage as (
       SELECT *
         FROM last_two_runs
        order by date asc
        limit 1
     )
         (
           select
             'test hits' as category,
             old_coverage.test_hits as old_coverage,
             new_coverage.test_hits as new_coverage,
             (new_coverage.test_hits - old_coverage.test_hits) as change_in_number,
             (new_coverage.percent_tested - old_coverage.percent_tested) as change_in_percent
             from old_coverage
                  , new_coverage
         )
         UNION
         (
           select
             'conf hits' as category,
             old_coverage.conf_hits as old_coverage,
             new_coverage.conf_hits as new_coverage,
             (new_coverage.conf_hits - old_coverage.conf_hits) as change_in_number,
             (new_coverage.percent_conf_tested - old_coverage.percent_conf_tested) as change_in_percent
             from 
                 old_coverage
               , new_coverage
         )
         ;
   #+END_SRC

   #+RESULTS: Difference View
   #+begin_src sql-mode
    category  | old_coverage | new_coverage | change_in_number | change_in_percent 
   -----------+--------------+--------------+------------------+-------------------
    conf hits |          124 |          114 |              -10 |             -2.33
    test hits |          171 |          165 |               -6 |             -1.40
   (2 rows)

   #+end_src
   
   Not a bad lil view!
** DONE List all tests within a single audit run
   CLOSED: [2019-10-27 Sun 22:50]
   This is a transitional view, mostly to play with getting the tests and formatting them to just be the test description
   #+NAME: Distinct Tests in a Run
   #+begin_src sql-mode
     WITH latest_job as (
       SELECT
         job
           FROM
               bucket_job_swagger
        ORDER BY
          job_timestamp DESC
          limit 1
       )
     SELECT DISTINCT
       split_part(useragent, '--', 2) as test
       FROM
           audit_event
      INNER JOIN latest_job ON (audit_event.job = latest_job.job) 
      WHERE audit_event.useragent like 'e2e.test%'
        AND audit_event.useragent like '%--%'
            ;

   #+end_src

   #+RESULTS: Distinct Tests in a Run
   #+begin_src sql-mode
                                                                                                                    test                                                                                                                  
   ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
     [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
     [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
     [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
     [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
     [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set [NodeConformance]
     [k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull from private registry with secret [NodeConformance]
     [k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from docker hub [NodeConformance]
     [k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from gcr.io [NodeConformance]
     [k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull from private registry without secret [NodeConformance]
     [k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull image from invalid registry [NodeConformance]
     [k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull non-existing image from gcr.io [NodeConformance]
     [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
     [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
     [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
     [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
     [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
     [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
     [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
     [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
     [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
     [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
     [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
     [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
     [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
     [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
     [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
     [k8s.io] Lease lease API should be available [Conformance]
     [k8s.io] NodeLease when the NodeLease feature is enabled the kubelet should create and update a lease in the kube-node-lease namespace
     [k8s.io] NodeLease when the NodeLease feature is enabled the kubelet should report node status infrequently
     [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
     [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]
     [k8s.io] Pods should be updated [NodeConformance] [Conformance]
     [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]
     [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]
     [k8s.io] Pods should support pod readiness gates [NodeFeature:PodReadinessGate]
     [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
     [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
     [k8s.io] PrivilegedPod [NodeConformance] should enable privileged commands [LinuxOnly]
     [k8s.io] Probing container should be restarted with a docker exec liveness probe with timeout 
     [k8s.io] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
     [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
     [k8s.io] Probing container should be restarted with a local redirect http liveness probe
     [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
     [k8s.io] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
     [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
     [k8s.io] Probing container should *not* be restarted with a non-local redirect http liveness probe
     [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance]
     [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
     [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
     [k8s.io] Security Context When creating a container with runAsNonRoot should not run with an explicit root user ID [LinuxOnly]
     [k8s.io] Security Context When creating a container with runAsNonRoot should not run without a specified user ID
     [k8s.io] Security Context When creating a container with runAsNonRoot should run with an explicit non-root user ID [LinuxOnly]
     [k8s.io] Security Context When creating a container with runAsNonRoot should run with an image specified user ID
     [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 0 [LinuxOnly] [NodeConformance]
     [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
     [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
     [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with readonly rootfs when readOnlyRootFilesystem=true [LinuxOnly] [NodeConformance]
     [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
     [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when not explicitly set and uid != 0 [LinuxOnly] [NodeConformance]
     [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when true [LinuxOnly] [NodeConformance]
     [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
     [k8s.io] [sig-node] AppArmor load AppArmor profiles can disable an AppArmor profile, using unconfined
     [k8s.io] [sig-node] AppArmor load AppArmor profiles should enforce an AppArmor profile
     [k8s.io] [sig-node] crictl should be able to run crictl on the node
     [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
     [k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s.
     [k8s.io] [sig-node] Mount propagation should propagate mounts to the host
     [k8s.io] [sig-node] NodeProblemDetector [DisabledForLargeClusters] should run without error
     [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]
     [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
     [k8s.io] [sig-node] PreStop graceful pod terminated should wait until preStop hook completes the process
     [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]
     [k8s.io] [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly]
     [k8s.io] [sig-node] Security Context should support container.SecurityContext.RunAsUser [LinuxOnly]
     [k8s.io] [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly]
     [k8s.io] [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser [LinuxOnly]
     [k8s.io] [sig-node] Security Context should support pod.Spec.SecurityContext.SupplementalGroups [LinuxOnly]
     [k8s.io] [sig-node] SSH should SSH to all nodes and run commands
     [k8s.io] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should not launch unsafe, but not explicitly enabled sysctls on the node
     [k8s.io] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should reject invalid sysctls
     [k8s.io] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support sysctls
     [k8s.io] Sysctls [LinuxOnly] [NodeFeature:Sysctls] should support unsafe sysctls which are actually whitelisted
     [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
     [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
     [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
     [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage][NodeFeature:VolumeSubpathEnvExpansion]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
     [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
     [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
     [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
     [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
     [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
     [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
     [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
     [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
     [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
     [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
     [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
     [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
     [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
     [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
     [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
     [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
     [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
     [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
     [sig-api-machinery] Discovery Custom resource should have storage version hash
     [sig-api-machinery] Garbage collector should delete jobs and pods created by cronjob
     [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
     [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
     [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
     [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
     [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
     [sig-api-machinery] Garbage collector should orphan pods created by rc if deleteOptions.OrphanDependents is nil
     [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
     [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
     [sig-api-machinery] Garbage collector should support cascading deletion of custom resources
     [sig-api-machinery] Garbage collector should support orphan deletion of custom resources
     [sig-api-machinery] Generated clientset should create pods, set the deletionTimestamp and deletionGracePeriodSeconds of the pod
     [sig-api-machinery] Generated clientset should create v1beta1 cronJobs, delete cronJobs, watch cronJobs
     [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
     [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
     [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a custom resource.
     [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim. [sig-storage]
     [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim with a storage class. [sig-storage]
     [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
     [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
     [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
     [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
     [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
     [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
     [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
     [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
     [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
     [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]
     [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]
     [sig-api-machinery] Servers with support for API chunking should return chunks of results for list calls
     [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
     [sig-api-machinery] Servers with support for Table transformation should return chunks of table results for list calls
     [sig-api-machinery] Servers with support for Table transformation should return generic metadata details across all namespaces for nodes
     [sig-api-machinery] Servers with support for Table transformation should return pod details
     [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
     [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
     [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
     [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
     [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
     [sig-apps] CronJob should delete successful/failed finished jobs with limit of one job
     [sig-apps] CronJob should not emit unexpected warnings
     [sig-apps] CronJob should remove from active list jobs that have been deleted
     [sig-apps] CronJob should replace jobs when ReplaceConcurrent
     [sig-apps] CronJob should schedule multiple jobs concurrently
     [sig-apps] Deployment deployment reaping should cascade to its replica sets and pods
     [sig-apps] Deployment deployment should delete old replica sets [Conformance]
     [sig-apps] Deployment deployment should support proportional scaling [Conformance]
     [sig-apps] Deployment deployment should support rollover [Conformance]
     [sig-apps] Deployment iterative rollouts should eventually progress
     [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
     [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
     [sig-apps] Deployment test Deployment ReplicaSet orphaning and adoption regarding controllerRef
     [sig-apps] DisruptionController evictions: enough pods, absolute => should allow an eviction
     [sig-apps] DisruptionController evictions: enough pods, replicaSet, percentage => should allow an eviction
     [sig-apps] DisruptionController evictions: maxUnavailable allow single eviction, percentage => should allow an eviction
     [sig-apps] DisruptionController evictions: maxUnavailable deny evictions, integer => should not allow an eviction
     [sig-apps] DisruptionController evictions: no PDB => should allow an eviction
     [sig-apps] DisruptionController evictions: too few pods, absolute => should not allow an eviction
     [sig-apps] DisruptionController evictions: too few pods, replicaSet, percentage => should not allow an eviction
     [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it
     [sig-apps] DisruptionController should create a PodDisruptionBudget
     [sig-apps] DisruptionController should update PodDisruptionBudget status
     [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
     [sig-apps] Job should delete a job [Conformance]
     [sig-apps] Job should fail to exceed backoffLimit
     [sig-apps] Job should fail when exceeds active deadline
     [sig-apps] Job should remove pods when job is deleted
     [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
     [sig-apps] Job should run a job to completion when tasks sometimes fail and are not locally restarted
     [sig-apps] Job should run a job to completion when tasks succeed
     [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
     [sig-apps] ReplicaSet should serve a basic image on each replica with a private image
     [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
     [sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota
     [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
     [sig-apps] ReplicationController should release no longer matching pods [Conformance]
     [sig-apps] ReplicationController should serve a basic image on each replica with a private image
     [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
     [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
     [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should adopt matching orphans and release non-matching pods
     [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
     [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should implement legacy replacement when the update strategy is OnDelete
     [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should not deadlock when a pod's predecessor fails
     [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
     [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
     [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications with PVCs
     [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should provide basic identity
     [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
     [sig-auth] Certificates API should support building a client with a CSR
     [sig-auth] Metadata Concealment should run a check-metadata-concealment job to completion
     [sig-auth] PodSecurityPolicy should allow pods under the privileged policy.PodSecurityPolicy
     [sig-auth] PodSecurityPolicy should enforce the restricted policy.PodSecurityPolicy
     [sig-auth] PodSecurityPolicy should forbid pod creation when no PSP is available
     [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
     [sig-auth] ServiceAccounts should ensure a single API token exists
     [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
     [sig-autoscaling] DNS horizontal autoscaling [DisabledForLargeClusters] kube-dns-autoscaler should scale kube-dns pods in both nonfaulty and faulty scenarios
     [sig-cli] Kubectl alpha client Kubectl run CronJob should create a CronJob
     [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
     [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
     [sig-cli] Kubectl client Kubectl apply apply set/view last-applied
     [sig-cli] Kubectl client Kubectl apply should apply a new configuration to an existing RC
     [sig-cli] Kubectl client Kubectl apply should reuse port when apply to an existing SVC
     [sig-cli] Kubectl client Kubectl client-side validation should create/apply a CR with unknown fields for CRD with no validation schema
     [sig-cli] Kubectl client Kubectl client-side validation should create/apply a valid CR for CRD with validation schema
     [sig-cli] Kubectl client Kubectl client-side validation should create/apply a valid CR with arbitrary-extra properties for CRD with partially-specified validation schema
     [sig-cli] Kubectl client Kubectl cluster-info dump should check if cluster-info dump succeeds
     [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]
     [sig-cli] Kubectl client Kubectl copy should copy a file from a running Pod
     [sig-cli] Kubectl client Kubectl create quota should create a quota without scopes
     [sig-cli] Kubectl client Kubectl create quota should create a quota with scopes
     [sig-cli] Kubectl client Kubectl create quota should reject quota with invalid scopes
     [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for cronjob
     [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
     [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
     [sig-cli] Kubectl client Kubectl get componentstatuses should get componentstatuses
     [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
     [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]
     [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
     [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
     [sig-cli] Kubectl client Kubectl rolling-update should support rolling-update to same image  [Conformance]
     [sig-cli] Kubectl client Kubectl run 
     [sig-cli] Kubectl client Kubectl run CronJob should create a CronJob
     [sig-cli] Kubectl client Kubectl run default should create an rc or deployment from an image  [Conformance]
     [sig-cli] Kubectl client Kubectl run deployment should create a deployment from an image  [Conformance]
     [sig-cli] Kubectl client Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]
     [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
     [sig-cli] Kubectl client Kubectl run rc should create an rc from an image  [Conformance]
     [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
     [sig-cli] Kubectl client Proxy server should support 
     [sig-cli] Kubectl client Proxy server should support proxy with 
     [sig-cli] Kubectl client Simple pod should contain last line of the log
     [sig-cli] Kubectl client Simple pod should handle in-cluster config
     [sig-cli] Kubectl client Simple pod should return command exit codes
     [sig-cli] Kubectl client Simple pod should support exec
     [sig-cli] Kubectl client Simple pod should support exec through an HTTP proxy
     [sig-cli] Kubectl client Simple pod should support exec through kubectl proxy
     [sig-cli] Kubectl client Simple pod should support exec using resource/name
     [sig-cli] Kubectl client Simple pod should support inline execution and attach
     [sig-cli] Kubectl client Simple pod should support port-forward
     [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
     [sig-cli] Kubectl client Update Demo should do a rolling update of a replication controller  [Conformance]
     [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
     [sig-cli] Kubectl Port forwarding With a server listening on 0.0.0.0 should support forwarding over websockets
     [sig-cli] Kubectl Port forwarding With a server listening on 0.0.0.0 that expects a client request should support a client that connects, sends DATA, and disconnects
     [sig-cli] Kubectl Port forwarding With a server listening on 0.0.0.0 that expects a client request should support a client that connects, sends NO DATA, and disconnects
     [sig-cli] Kubectl Port forwarding With a server listening on 0.0.0.0 that expects NO client request should support a client that connects, sends DATA, and disconnects
     [sig-cli] Kubectl Port forwarding With a server listening on localhost should support forwarding over websockets
     [sig-cli] Kubectl Port forwarding With a server listening on localhost that expects a client request should support a client that connects, sends DATA, and disconnects
     [sig-cli] Kubectl Port forwarding With a server listening on localhost that expects a client request should support a client that connects, sends NO DATA, and disconnects
     [sig-cli] Kubectl Port forwarding With a server listening on localhost that expects NO client request should support a client that connects, sends DATA, and disconnects
     [sig-instrumentation] Cadvisor should be healthy on every node.
     [sig-instrumentation] MetricsGrabber should grab all metrics from a ControllerManager.
     [sig-instrumentation] MetricsGrabber should grab all metrics from a Kubelet.
     [sig-instrumentation] MetricsGrabber should grab all metrics from API server.
     [sig-instrumentation] MetricsGrabber should grab all metrics from a Scheduler.
     [sig-network] DNS should provide DNS for ExternalName services [Conformance]
     [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
     [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
     [sig-network] DNS should provide DNS for services  [Conformance]
     [sig-network] DNS should provide DNS for the cluster  [Conformance]
     [sig-network] DNS should provide DNS for the cluster [Provider:GCE]
     [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
     [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
     [sig-network] DNS should resolve DNS of partial qualified names for the cluster [LinuxOnly]
     [sig-network] DNS should support configurable pod DNS nameservers [Conformance]
     [sig-network] DNS should support configurable pod resolv.conf
     [sig-network] Firewall rule should have correct firewall rules for e2e cluster
     [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
     [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
     [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
     [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
     [sig-network] Networking Granular Checks: Services should function for client IP based session affinity: http [LinuxOnly]
     [sig-network] Networking Granular Checks: Services should function for client IP based session affinity: udp [LinuxOnly]
     [sig-network] Networking Granular Checks: Services should function for endpoint-Service: http
     [sig-network] Networking Granular Checks: Services should function for endpoint-Service: udp
     [sig-network] Networking Granular Checks: Services should function for node-Service: http
     [sig-network] Networking Granular Checks: Services should function for node-Service: udp
     [sig-network] Networking Granular Checks: Services should function for pod-Service: http
     [sig-network] Networking Granular Checks: Services should function for pod-Service: udp
     [sig-network] Networking Granular Checks: Services should update endpoints: http
     [sig-network] Networking Granular Checks: Services should update endpoints: udp
     [sig-network] Networking should check kube-proxy urls
     [sig-network] Networking should provide unchanging, static URL paths for kubernetes api services
     [sig-network] Network should set TCP CLOSE_WAIT timeout
     [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]
     [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
     [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
     [sig-network] Service endpoints latency should not be very high  [Conformance]
     [sig-network] Services should allow pods to hairpin back to themselves through services
     [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
     [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
     [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
     [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
     [sig-network] Services should be able to create a functioning NodePort service [Conformance]
     [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly]
     [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly]
     [sig-network] Services should be able to up and down services
     [sig-network] Services should be able to update service type to NodePort listening on same port number but different protocols
     [sig-network] Services should be rejected when no endpoints exist
     [sig-network] Services should check NodePort out-of-range
     [sig-network] Services should create endpoints for unready pods
     [sig-network] Services should have session affinity work for NodePort service [LinuxOnly]
     [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly]
     [sig-network] Services should implement service.kubernetes.io/headless
     [sig-network] Services should implement service.kubernetes.io/service-proxy-name
     [sig-network] Services should preserve source pod IP for traffic thru service cluster IP [LinuxOnly]
     [sig-network] Services should prevent NodePort collisions
     [sig-network] Services should provide secure master service  [Conformance]
     [sig-network] Services should release NodePorts on delete
     [sig-network] Services should serve a basic endpoint from pods  [Conformance]
     [sig-network] Services should serve multiport endpoints from pods  [Conformance]
     [sig-network] [sig-windows] Networking Granular Checks: Pods should function for intra-pod communication: http
     [sig-network] [sig-windows] Networking Granular Checks: Pods should function for intra-pod communication: udp
     [sig-network] [sig-windows] Networking Granular Checks: Pods should function for node-pod communication: http
     [sig-network] [sig-windows] Networking Granular Checks: Pods should function for node-pod communication: udp
     [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
     [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
     [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
     [sig-node] ConfigMap should update ConfigMap successfully
     [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
     [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
     [sig-node] Downward API should provide host IP and pod IP as an env var if pod uses host network [LinuxOnly]
     [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
     [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
     [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
     [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass
     [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass
     [sig-node] RuntimeClass should reject a Pod requesting a RuntimeClass with an unconfigured handler
     [sig-node] RuntimeClass should reject a Pod requesting a RuntimeClass with conflicting node selector
     [sig-node] RuntimeClass should run a Pod requesting a RuntimeClass with a configured handler [NodeFeature:RuntimeHandler]
     [sig-node] RuntimeClass should run a Pod requesting a RuntimeClass with scheduling [NodeFeature:RuntimeHandler] 
     [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied.
     [sig-scheduling] Multi-AZ Clusters should spread the pods of a replication controller across zones
     [sig-scheduling] Multi-AZ Clusters should spread the pods of a service across zones
     [sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should only be allowed to provision PDs in zones where nodes exist
     [sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should schedule pods in the same zones as statically provisioned PVs
     [sig-scheduling] PreemptionExecutionPath runs ReplicaSets to verify preemption running path
     [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
     [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
     [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] ConfigMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeFeature:FSGroup]
     [sig-storage] ConfigMap should be consumable from pods in volume as non-root with FSGroup [LinuxOnly] [NodeFeature:FSGroup]
     [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
     [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root with FSGroup [LinuxOnly] [NodeFeature:FSGroup]
     [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
     [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
     [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
     [sig-storage] CSI mock volume CSI attach test using mock driver should not require VolumeAttach for drivers without attachment
     [sig-storage] CSI mock volume CSI attach test using mock driver should preserve attachment policy when no CSIDriver present
     [sig-storage] CSI mock volume CSI attach test using mock driver should require VolumeAttach for drivers with attachment
     [sig-storage] CSI mock volume CSI online volume expansion should expand volume without restarting pod if attach=off, nodeExpansion=on
     [sig-storage] CSI mock volume CSI online volume expansion should expand volume without restarting pod if attach=on, nodeExpansion=on
     [sig-storage] CSI mock volume CSI Volume expansion should expand volume by restarting pod if attach=off, nodeExpansion=on
     [sig-storage] CSI mock volume CSI Volume expansion should expand volume by restarting pod if attach=on, nodeExpansion=on
     [sig-storage] CSI mock volume CSI Volume expansion should expand volume without restarting pod if nodeExpansion=off
     [sig-storage] CSI mock volume CSI Volume expansion should not expand volume if resizingOnDriver=off, resizingOnSC=on
     [sig-storage] CSI mock volume CSI workload information using mock driver contain ephemeral=true when using inline volume
     [sig-storage] CSI mock volume CSI workload information using mock driver should be passed when podInfoOnMount=true
     [sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when CSIDriver does not exist
     [sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when podInfoOnMount=false
     [sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when podInfoOnMount=nil
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volumes should store data
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumes should store data
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: inline ephemeral CSI volume] ephemeral should create read-only inline ephemeral volume
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: inline ephemeral CSI volume] ephemeral should create read/write inline ephemeral volume
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: inline ephemeral CSI volume] ephemeral should support multiple inline ephemeral volumes
     [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: inline ephemeral CSI volume] ephemeral should support two pods which share the same volume
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (block volmode)] volumes should store data
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] volumes should store data
     [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
     [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
     [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
     [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
     [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
     [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
     [sig-storage] Downward API volume should provide podname as non-root with fsgroup and defaultMode [LinuxOnly] [NodeFeature:FSGroup]
     [sig-storage] Downward API volume should provide podname as non-root with fsgroup [LinuxOnly] [NodeFeature:FSGroup]
     [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
     [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
     [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
     [sig-storage] Dynamic Provisioning Invalid AWS KMS key should report an error and create no PV
     [sig-storage] Dynamic Provisioning [k8s.io] GlusterDynamicProvisioner should create and delete persistent volumes [fast]
     [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
     [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] EmptyDir volumes when FSGroup is specified [LinuxOnly] [NodeFeature:FSGroup] files with FSGroup ownership should support (root,0644,tmpfs)
     [sig-storage] EmptyDir volumes when FSGroup is specified [LinuxOnly] [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is non-root
     [sig-storage] EmptyDir volumes when FSGroup is specified [LinuxOnly] [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is root
     [sig-storage] EmptyDir volumes when FSGroup is specified [LinuxOnly] [NodeFeature:FSGroup] nonexistent volume subPath should have the correct mode and owner using FSGroup
     [sig-storage] EmptyDir volumes when FSGroup is specified [LinuxOnly] [NodeFeature:FSGroup] volume on default medium should have the correct mode using FSGroup
     [sig-storage] EmptyDir volumes when FSGroup is specified [LinuxOnly] [NodeFeature:FSGroup] volume on tmpfs should have the correct mode using FSGroup
     [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
     [sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : configmap
     [sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : projected
     [sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : secret
     [sig-storage] Flexvolumes should be mountable when attachable
     [sig-storage] Flexvolumes should be mountable when non-attachable
     [sig-storage] GCP Volumes GlusterFS should be mountable
     [sig-storage] GCP Volumes NFSv3 should be mountable for NFSv3
     [sig-storage] GCP Volumes NFSv4 should be mountable for NFSv4
     [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] HostPath should support r/w [NodeConformance]
     [sig-storage] HostPath should support subPath [NodeConformance]
     [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)(allowExpansion)] volume-expand Verify if offline PVC expansion works
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand should resize volume when PVC is edited while pod is using it
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)(allowExpansion)] volume-expand Verify if offline PVC expansion works
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volume-expand should not allow expansion of pvcs without AllowVolumeExpansion property
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (delayed binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (delayed binding)] topology should provision a volume and schedule a pod with AllowedTopologies
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext3)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext4)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (immediate binding)] topology should fail to schedule a pod which has topologies that conflict with AllowedTopologies
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (immediate binding)] topology should provision a volume and schedule a pod with AllowedTopologies
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext3)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext4)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with pvc data source
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
     [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (filesystem volmode)] volumeMode should not mount / map unused volumes in a pod
     [sig-storage] Mounted volume expand Should verify mounted devices can be resized
     [sig-storage] PersistentVolumes GCEPD should test that deleting a PVC before the pod does not cause pod deletion to fail on PD detach
     [sig-storage] PersistentVolumes GCEPD should test that deleting the Namespace of a PVC and Pod causes the successful detach of Persistent Disk
     [sig-storage] PersistentVolumes GCEPD should test that deleting the PV before the pod does not cause pod deletion to fail on PD detach
     [sig-storage] PersistentVolumes-local  Pod with node different from PV's NodeAffinity should fail scheduling due to different NodeAffinity
     [sig-storage] PersistentVolumes-local  Pod with node different from PV's NodeAffinity should fail scheduling due to different NodeSelector
     [sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
     [sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
     [sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and read from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and write from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: block] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
     [sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
     [sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
     [sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and read from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and write from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
     [sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and read from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and write from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
     [sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and read from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and write from pod1
     [sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
     [sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
     [sig-storage] PersistentVolumes NFS when invoking the Recycle reclaim policy should test that a PV becomes Available and is clean after the PVC is deleted.
     [sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 2 PVs and 4 PVCs: test write access
     [sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 3 PVs and 3 PVCs: test write access
     [sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PV and a pre-bound PVC: test write access
     [sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and a pre-bound PV: test write access
     [sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and non-pre-bound PV: test write access
     [sig-storage] PersistentVolumes NFS with Single PV - PVC pairs should create a non-pre-bound PV and PVC: test write access 
     [sig-storage] PersistentVolumes:vsphere should test that deleting a PVC before the pod does not cause pod deletion to fail on vsphere volume detach
     [sig-storage] PersistentVolumes:vsphere should test that deleting the Namespace of a PVC and Pod causes the successful detach of vsphere volume
     [sig-storage] PersistentVolumes:vsphere should test that deleting the PV before the pod does not cause pod deletion to fail on vspehre volume detach
     [sig-storage] Pod Disks should be able to delete a non-existent PD without error
     [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
     [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
     [sig-storage] Projected configMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Projected configMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeFeature:FSGroup]
     [sig-storage] Projected configMap should be consumable from pods in volume as non-root with FSGroup [LinuxOnly] [NodeFeature:FSGroup]
     [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
     [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root with FSGroup [LinuxOnly] [NodeFeature:FSGroup]
     [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
     [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
     [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
     [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
     [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
     [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
     [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
     [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
     [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
     [sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup and defaultMode [LinuxOnly] [NodeFeature:FSGroup]
     [sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup [LinuxOnly] [NodeFeature:FSGroup]
     [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
     [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
     [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
     [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
     [sig-storage] Projected secret should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance]
     [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
     [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
     [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
     [sig-storage] PVC Protection Verify "immediate" deletion of a PVC that is not in active use by a pod
     [sig-storage] PVC Protection Verify that PVC in active use by a pod is not removed immediately
     [sig-storage] PVC Protection Verify that scheduling of a pod that uses PVC that is being deleted fails and the pod becomes Unschedulable
     [sig-storage] PV Protection Verify "immediate" deletion of a PV that is not bound to a PVC
     [sig-storage] PV Protection Verify that PV bound to a PVC is not removed immediately
     [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
     [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
     [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
     [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
     [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
     [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
     [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]
     [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
     [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]
     [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]
     [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]
     [sig-storage] Volume limits should verify that all nodes have volume limits
     [sig-storage] Volume Placement should create and delete pod with multiple volumes from different datastore
     [sig-storage] Volume Placement should create and delete pod with multiple volumes from same datastore
     [sig-storage] Volume Placement should create and delete pod with the same volume source attach/detach to different worker nodes
     [sig-storage] Volume Placement should create and delete pod with the same volume source on the same worker node
     [sig-storage] Volume Placement test back to back pod creation and deletion with different volume sources on the same worker node
     [sig-storage] Volumes ConfigMap should be mountable
     [sig-storage] vsphere statefulset vsphere statefulset testing
     [sig-storage] Zone Support Verify a pod fails to get scheduled when conflicting volume topology (allowedTopologies) and pod scheduling constraints(nodeSelector) are specified
     [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on allowed zones specified in storage class 
     [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on a VSAN capability, datastore and compatible zone specified in storage class
     [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on multiple zones specified in storage class 
     [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on multiple zones specified in the storage class. (No shared datastores exist among both zones)
     [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on the allowed zones and datastore specified in storage class
     [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on the allowed zones and storage policy specified in storage class
     [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on the allowed zones, datastore and storage policy specified in storage class
     [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV with storage policy specified in storage class in waitForFirstConsumer binding mode
     [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV with storage policy specified in storage class in waitForFirstConsumer binding mode with allowedTopologies
     [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV with storage policy specified in storage class in waitForFirstConsumer binding mode with multiple allowedTopologies
     [sig-storage] Zone Support Verify a pod is created on a non-Workspace zone and attached to a dynamically created PV, based on the allowed zones and storage policy specified in storage class
     [sig-storage] Zone Support Verify a PVC creation fails when multiple zones are specified in the storage class without shared datastores among the zones in waitForFirstConsumer binding mode
     [sig-storage] Zone Support Verify dynamically created pv with allowed zones specified in storage class, shows the right zone information on its labels
     [sig-storage] Zone Support Verify dynamically created pv with multiple zones specified in the storage class, shows both the zones on its labels
     [sig-storage] Zone Support Verify PVC creation fails if no zones are specified in the storage class (No shared datastores exist among all the nodes)
     [sig-storage] Zone Support Verify PVC creation fails if only datastore is specified in the storage class (No shared datastores exist among all the nodes)
     [sig-storage] Zone Support Verify PVC creation fails if only storage policy is specified in the storage class (No shared datastores exist among all the nodes)
     [sig-storage] Zone Support Verify PVC creation fails if the availability zone specified in the storage class have no shared datastores under it.
     [sig-storage] Zone Support Verify PVC creation with an invalid VSAN capability along with a compatible zone combination specified in storage class fails
     [sig-storage] Zone Support Verify PVC creation with compatible policy and datastore without any zones specified in the storage class fails (No shared datastores exist among all the nodes)
     [sig-storage] Zone Support Verify PVC creation with incompatible datastore and zone combination specified in storage class fails
     [sig-storage] Zone Support Verify PVC creation with incompatible storage policy along with compatible zone and datastore combination specified in storage class fails
     [sig-storage] Zone Support Verify PVC creation with incompatible storagePolicy and zone combination specified in storage class fails
     [sig-storage] Zone Support Verify PVC creation with incompatible zone along with compatible storagePolicy and datastore combination specified in storage class fails
     [sig-storage] Zone Support Verify PVC creation with invalid zone specified in storage class fails
   (835 rows)

   #+end_src
   
   very good.

** DONE List all tests in most recent test run that are not in second most recent test run.
   CLOSED: [2019-10-27 Sun 23:40]
   Now we want to take this and do an EXCEPT clause,
   we can grab the last two in the same way as above, except we don't need the stats coverage and that's a slow view.  Instead, let's grab the jobs from bucket_job_swagger.
   
   This would be the last two jobs from bjs
   #+begin_src sql-mode
   select job, job_timestamp from bucket_job_swagger order by job_timestamp desc limit 2;
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
            job         |    job_timestamp    
   ---------------------+---------------------
    1181584183475048448 | 2019-10-08 15:37:48
    1178464478988079104 | 2019-09-30 01:01:01
   (2 rows)

   #+end_src
   
   and the last two from stable_endpoint_stats
   #+begin_src sql-mode
   select job, date from stable_endpoint_stats order by date limit 2;
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
            job         |    date    
   ---------------------+------------
    1173412183980118017 | 2019-09-16
    1178464478988079104 | 2019-09-30
   (2 rows)

   #+end_src
   
   they are the same!
   
   So then, the query would be a basic except query.
   
   #+NAME: added tests
   #+begin_src sql-mode
     with last_two_runs as (
       select
         job, job_timestamp
         FROM
             bucket_job_swagger
        ORDER BY 
          job_timestamp DESC
        LIMIT 2
     ),
       new_run as (
         SELECT 
           job
           FROM last_two_runs
          order by job_timestamp DESC
          limit 1
       ),
       old_run as (
         SELECT
           job
           FROM
               last_two_runs
          order by job_timestamp asc
          limit 1
       )
           (
             SELECT DISTINCT
               split_part(useragent, '--', 2) as test
               FROM
                   audit_event
                   INNER JOIN new_run on (audit_event.job = new_run.job)
           )
           EXCEPT
           (
             SELECT DISTINCT
               split_part(useragent, '--', 2) as test
               FROM
                   audit_event
                   INNER JOIN old_run on (audit_event.job = old_run.job)
           )
           ;
   #+end_src

   #+RESULTS: added tests
   #+begin_src sql-mode
                                                                 test                                                               
   ---------------------------------------------------------------------------------------------------------------------------------
     [sig-network] Services should be able to update service type to NodePort listening on same port number but different protocols
   (1 row)

   #+end_src
** DONE list all tests in second most recent test run that are not in most recent test run
   CLOSED: [2019-10-27 Sun 23:40]
   This would be nearly identical to our added tests, just changing the left and right table.
   #+NAME: removed tests
   #+begin_src sql-mode
     with last_two_runs as (
       select
         job, job_timestamp
         FROM
             bucket_job_swagger
        ORDER BY 
          job_timestamp DESC
        LIMIT 2
     ),
       new_run as (
         SELECT 
           job
           FROM last_two_runs
          order by job_timestamp DESC
          limit 1
       ),
       old_run as (
         SELECT
           job
           FROM
               last_two_runs
          order by job_timestamp asc
          limit 1
       )
           (
             SELECT DISTINCT
               split_part(useragent, '--', 2) as test
               FROM
                   audit_event
                   INNER JOIN old_run on (audit_event.job = old_run.job)
           )
           EXCEPT
           (
             SELECT DISTINCT
               split_part(useragent, '--', 2) as test
               FROM
                   audit_event
                   INNER JOIN new_run on (audit_event.job = new_run.job)
           )
           ;
   #+end_src

   #+RESULTS: removed tests
   #+begin_src sql-mode
                                                        test                                                      
   ---------------------------------------------------------------------------------------------------------------
     [sig-network] Services should be able to update NodePorts with two same port numbers but different protocols
     [sig-network] Services should use same NodePort with same port but different protocols
   (2 rows)

   #+end_src
   
** DONE Create view that lists the job, date, test name, for both.  This should show us the additions and removals.
   CLOSED: [2019-10-27 Sun 23:40]
   
   We can combine all these together by doing a union of the two except queries.  We could save these as their own views, to make this qauery shorter, but I don't think we'll have any gains in readability (as you'll have to cross reference other views.
   #+NAME: Change in Tests
   #+begin_src sql-mode
      with last_two_runs as (
        select
          job, job_timestamp
          FROM
              bucket_job_swagger
         ORDER BY 
           job_timestamp DESC
         LIMIT 2
      ),
        new_run as (
        SELECT 
          job
          FROM last_two_runs
         order by job_timestamp DESC
         limit 1
        ),
        old_run as (
        SELECT
          job
          FROM
              last_two_runs
         order by job_timestamp asc
         limit 1
      )
     (
      SELECT
        test,
        'added' as status
        FROM
            (
              (
                SELECT DISTINCT
                  split_part(useragent, '--', 2) as test
                  FROM
                      audit_event
                      INNER JOIN new_run on (audit_event.job = new_run.job)
              )
              EXCEPT
              (
                SELECT DISTINCT
                  split_part(useragent, '--', 2) as test
                  FROM
                      audit_event
                      INNER JOIN old_run on (audit_event.job = old_run.job)
              )
            ) added_tests
       )
     UNION
          (
            SELECT
              test,
              'removed' as status
              FROM
                  (
                    (
                      SELECT DISTINCT
                        split_part(useragent, '--', 2) as test
                        FROM
                            audit_event
                            INNER JOIN old_run on (audit_event.job = old_run.job)
                    )
                    EXCEPT
                    (
                      SELECT DISTINCT
                        split_part(useragent, '--', 2) as test
                        FROM
                            audit_event
                            INNER JOIN new_run on (audit_event.job = new_run.job)
                    )
                  ) removed_tests
          )
            ;

   #+end_src

   #+RESULTS: Change in Tests
   #+begin_src sql-mode
                                                                 test                                                               | status  
   ---------------------------------------------------------------------------------------------------------------------------------+---------
     [sig-network] Services should be able to update NodePorts with two same port numbers but different protocols                   | removed
     [sig-network] Services should be able to update service type to NodePort listening on same port number but different protocols | added
     [sig-network] Services should use same NodePort with same port but different protocols                                         | removed
   (3 rows)

   #+end_src

   It's a verbose query, but it is fairly quick (especially since I dont' think I've indexed the audit events).
   
   If it's accurate, then the test count for our old job should be 1 higher than our new one.
   
   We can test this quickly
   #+NAME: Test counts for old and new job
   #+begin_src sql-mode
     with last_two_runs as (
       select
         job, job_timestamp
         FROM
             bucket_job_swagger
        ORDER BY 
          job_timestamp DESC
        LIMIT 2
     )
     SELECT DISTINCT
       job_timestamp,
       audit_event.job,
       count(distinct useragent) FILTER (
         WHERE useragent like 'e2e.test%' AND useragent LIKE '%--%'
       ) as test_number
       FROM
           audit_event
           INNER JOIN last_two_runs ON (audit_event.job = last_two_runs.job)
        GROUP BY audit_event.job, job_timestamp
           ;

   #+end_src

   #+RESULTS: Test counts for old and new job
   #+begin_src sql-mode
       job_timestamp    |         job         | test_number 
   ---------------------+---------------------+-------------
    2019-09-30 01:01:01 | 1178464478988079104 |         836
    2019-10-08 15:37:48 | 1181584183475048448 |         835
   (2 rows)

   #+end_src
   
   And that is exactly the case!
   
   So we now have a nice query that shows the tests that were added and removed between two jobs.
   I don't think an aggregation on thi sis useful (e.g. :# of tests in old versus new), as a decrease may hide a useful refactor.
   In these runs, for example, it looks like two tests were combined into a new, better written test.  As long as endpint coverage remains the same, or increases, then a drop in tests might be a good thing...as it'd mean a sharpening of tests focus.
   
** TODO List Endpoints covered in new not covered in old
    
** TODO list endpoints covered in old but not new
** TODO create view that lists endpoint coverage, and whether coverage was added or removed
* Conclusion || Next Steps
* Footnotes
  #+NAME: Connect org to postgres
  #+BEGIN_SRC emacs-lisp :results silent
    (sql-connect "apisnoop" (concat "*SQL: postgres:data*"))
  #+END_SRC
  : You are connected to database "apisnoop" as user "apisnoop" on host "localhost" at port "10041".
  #+NAME: Test Connection
  #+BEGIN_SRC sql-mode :results silent
  \conninfo
  #+END_SRC

#+TITLE: Ticket 65
#+AUTHOR: Zach Mandeville


* Ticket
  [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/65][Gitlab Link]]
  
  #+BEGIN_QUOTE
  We can model our function to load swagger files off the way we load our audit events, where we create a temporary table of data, and then use this to grab metadata and place it in the correct columns.  This will allow us to include the content-hash and url and more for each swagger in its table.
  #+END_QUOTE
* Strategy
  Our audit events are going to be coming from specific buckets and jobs, the artifacts of which hold a metadata.json file that has far more metadata than can be found in the swagger file.  With the swagger, we mostly want to know what the swagger was at the particular commit run for this audit event.  This bucket/job metadata will also help us in doing cross coverage in general.
  
  So the strategy is to create a new table for bucket/job metadata and use the details from that to determine the swagger we pull down.  We can then do a simple join on the swagger to get its commit hash from the metadata.
* Process
** Curl a sample metadata file
   We'll use the most recent job/bucket from apisnoop, following the spyglass link to get to its artifacts.
   
  #+NAME Curl Sample Metadata
  #+BEGIN_SRC shell
  curl 'https://storage.googleapis.com/kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce/1178464478988079104/finished.json'
  #+END_SRC

  #+RESULTS:
  #+begin_EXAMPLE
  {
    "timestamp": 1569805261, 
    "version": "v1.17.0-alpha.0.1912+2b795b9825e485", 
    "result": "SUCCESS", 
    "passed": true, 
    "job-version": "v1.17.0-alpha.0.1912+2b795b9825e485", 
    "metadata": {
      "node_os_image": "cos-73-11647-163-0", 
      "infra-commit": "dba192364", 
      "master_os_image": "cos-73-11647-163-0", 
      "job-version": "v1.17.0-alpha.0.1912+2b795b9825e485", 
      "pod": "f53ddc96-e317-11e9-9585-baa219f9d09d", 
      "revision": "v1.17.0-alpha.0.1912+2b795b9825e485"
    }
  }
  #+end_EXAMPLE
  
  The commit hash is the ~2b795b9825e485~ appended to the version.  We can view the swagger for this commit at this url:
  https://raw.githubusercontent.com/kubernetes/kubernetes/2b795b9825e485/api/openapi-spec/swagger.json
  
  We can load our metadata into our db using the COPY FROM PROGRAM command.  This will just copy over our data as a json blob, so we want to create a temporary table to feed that into, then select from that to generate our actual table.
** Create job_bucket Table
#+NAME: job_bucket_full
#+BEGIN_SRC sql-mode :results silent
  DROP TABLE job_bucket;
  CREATE TABLE job_bucket (
      id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
      ingested_at timestamp DEFAULT CURRENT_TIMESTAMP,
      bucket text,
      job text,
      commit_hash text,
      passed text,
      result text,
      pod text,
      infra_commit text,
      job_version text,
      job_timestamp text,
      node_os_image text,
      master_os_image text 
  );
#+END_SRC
 
** Create python script for curl > temp table > job_bucket flow
    Am using the audit_event loading as a guide, though put in an explicit exception to help catch sql bugs.
    #+BEGIN_SRC python

             ${metadata.result} as result,
             ${metadata['metadata']['infra-commit']} as infra_commit,
             ${metadata['metadata']['pod']} as pod,
             ${metadata['metadata']['job-version']} as job_version,
             to_timestamp(${metadata['timestamp']}::bigint) AT TIME ZONE 'UTC' as job_timestamp,
             ${metadata['metadata']['node_os_image']} as node_os_image,
             ${metadata['metadata']['master_os_image']} as master_os_image

    #+END_SRC

   
#+NAME: load_job_bucket_via_curl.py
#+BEGIN_SRC python :results output
  try:
      from urllib.request import urlopen, urlretrieve
      from string import Template
      import json
      bucket = 'ci-kubernetes-e2e-gci-gce' # for debugging right now
      job = '1178464478988079104' # for debugging right now
      metadata_url = ''.join(['https://storage.googleapis.com/kubernetes-jenkins/logs/', bucket, '/', job, '/finished.json'])
      metadata = json.loads(urlopen(metadata_url).read().decode('utf-8'))
      commit_hash = metadata["version"].split("+")[1]
      swagger_url =  ''.join(['https://raw.githubusercontent.com/kubernetes/kubernetes/', commit_hash, '/api/openapi-spec/swagger.json']) 
      swagger = json.loads(urlopen(swagger_url).read().decode('utf-8'))
      passed = metadata['passed']
      sql = Template("""
   INSERT INTO job_bucket(
             bucket,
             job,
             commit_hash, 
             passed)
     SELECT

             '$bucket' as bucket,
             '$job' as job,
             '$commit_hash' as commit_hash,
             '$passed' as passed
      """).substitute(metadata = metadata, bucket = bucket, job = job, commit_hash = commit_hash, passed = passed)
      rv = plpy.execute(sql)
      return "it worked!"
  except Exception as err:
      return Template("something went wrong, likely this: ${error}").substitute(error = err)
#+END_SRC

#+RESULTS: load_job_bucket_via_curl.py

 #+NAME: load_job_bucket_via_curl_old.py
 #+BEGIN_SRC python :eval never :exports code
         try:
             from string import Template
             metadata = ''.join([
                 'curl ',
                 'https://storage.googleapis.com/kubernetes-jenkins/logs/',
                 bucket,
                 '/',
                 job,
                 '/finished.json | jq -c .'])
             curlswagger= ''.join([
                'IFS="+"; ',
                'version=$(',
                metadata,
                ' | jq -c .version); ',
                'commit=${version##*+}; ',
                'commit_hash=${commit%\"*}; ',
                'swagger_link="https://raw.githubusercontent.com/kubernetes/kubernetes/${commit_hash}/api/openapi-spec/swagger.json"; ',
                'curl ${swagger_link} | jq -c .'])
             sql = Template("""
   DROP TABLE raw_job_bucket_import;
   CREATE TEMPORARY TABLE raw_job_bucket_import (data jsonb not null, swagger jsonb);
   COPY raw_job_bucket_import (data) FROM PROGRAM '$curl' (DELIMITER e'\x02', FORMAT 'csv', QUOTE e'\x01');
   INSERT INTO job_bucket(
             bucket,
             job,
             commit_hash,
             passed,
             result,
             infra_commit,
             pod,
             job_version,
             job_timestamp,
             node_os_image,
             master_os_image)
     SELECT
             '${bucket}' as bucket,
             '${job}' as job,
             split_part((raw.data ->> 'job-version'), '+', 2) as commit_hash,
             CASE (raw.data ->> 'passed') WHEN 'true' THEN TRUE ELSE FALSE END as passed,
             (raw.data ->> 'result') as result,
             (raw.data -> 'metadata' ->> 'infra-commit') as infra_commit,
             (raw.data -> 'metadata' ->> 'pod') as pod,
             (raw.data -> 'metadata' ->> 'job-version') as job_version,
             (to_timestamp((raw.data ->>'timestamp')::bigint) AT TIME ZONE 'UTC') as job_timestamp,
             (raw.data -> 'metadata' ->> 'node_os_image') as node_os_image,
             (raw.data -> 'metadata' ->> 'master_os_image') as master_os_image,
             (raw.swagger) as swagger
       FROM raw_job_bucket_import raw;
             """).substitute(curl =  metadata, bucket = bucket, job = job, curlswagger = curlswagger)
             rv = plpy.execute(sql)
             return "it worked!"
         except Exception as err:
             return Template("something went wrong, likely this: ${error}").substitute(error = err)
 #+END_SRC
 
 The only real manipulation of the data we do is:
- convert the passed value back into a boolean (it comes in as text)

- convert the unix timestamp into a postgres timestamp

- create commit hash by splitting job_version on '+' and grabbing the last part.
 
We can now create the sql function 
  #+NAME: load_job_bucket_via_curl.sql
  #+BEGIN_SRC sql-mode :noweb yes
    set role dba;
    DROP FUNCTION IF EXISTS load_job_bucket_via_curl;
    CREATE OR REPLACE FUNCTION load_job_bucket_via_curl(bucket text, job text)
    RETURNS text AS $$
    <<load_job_bucket_via_curl.py>>
    $$ LANGUAGE plpython3u ;
    reset role;
  #+END_SRC

  #+RESULTS: load_job_bucket_via_curl.sql
  #+begin_src sql-mode
  SET
  DROP FUNCTION
  apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# CREATE FUNCTION
  RESET
  #+end_src
  
** Load Job Bucket
   Now we test out the loading of the job_buckets.  We'll use the most recent api_snoop bucket and two other random ones in the jenkins logs.
 #+NAME: Test It Out pt. 2
 #+BEGIN_SRC sql-mode
  select * from load_job_bucket_via_curl('ci-kubernetes-e2e-gci-gce', '1178464478988079104');
  select * from load_job_bucket_via_curl('ci-kubernetes-e2e-gci-gce', '1148565955786313730');
  select * from load_job_bucket_via_curl('ci-kubernetes-e2e-gci-gce', '1148586594899333122');
 #+END_SRC

 #+RESULTS: Test It Out pt. 2
 #+begin_src sql-mode
  load_job_bucket_via_curl 
 --------------------------
  it worked!
 (1 row)

 #+end_src

 #+BEGIN_SRC sql-mode
 select passed, bucket, job, commit_hash, job_timestamp, ingested_at from job_bucket;
 #+END_SRC

 #+RESULTS:
 #+begin_src sql-mode
  passed |          bucket           |         job         |  commit_hash   | job_timestamp |        ingested_at         
 --------+---------------------------+---------------------+----------------+---------------+----------------------------
         | ci-kubernetes-e2e-gci-gce | 1178464478988079104 | 2b795b9825e485 |               | 2019-10-03 03:18:39.896701
         | ci-kubernetes-e2e-gci-gce | 1178464478988079104 | 2b795b9825e485 |               | 2019-10-03 03:18:40.096853
         | ci-kubernetes-e2e-gci-gce | 1178464478988079104 | 2b795b9825e485 |               | 2019-10-03 03:18:40.281389
  True   | ci-kubernetes-e2e-gci-gce | 1178464478988079104 | 2b795b9825e485 |               | 2019-10-03 03:27:21.688886
  True   | ci-kubernetes-e2e-gci-gce | 1178464478988079104 | 2b795b9825e485 |               | 2019-10-03 03:27:21.872503
  True   | ci-kubernetes-e2e-gci-gce | 1178464478988079104 | 2b795b9825e485 |               | 2019-10-03 03:27:22.065762
 (6 rows)

 #+end_src
  
 Fantastic!

* Next Steps
** TODO Check for primary key in the job bucket, and ensure we don't put in duplicates
   we'll use bucket + job as the primary key.
** TODO Expand api_swagger to include commit hash, job, bucket as means to connect to job_bucket.
** TODO Expand audit_events to include commit hash, job, bucket as means to connect to job_bucket.
** TODO Trigger an insertion into our api_swagger, based on insertion into job_bucket
   We will want to take the job/bucket/commit_hash (or maybe just the commit_hash) and pass this into our load_swagger function, upon a successful insertion into job_bucket.  This feels like a trigger function, but am unsure!
** TODO Trigger an insertion into our raw_audit_events, based on insertion into job_bucket
   This is a bit more involved, and will likely require some prepping of the data before we put it into the db.
* Footnotes
** Connect to Database
    If you already have your db and hasura endpoint up and running:
 - [ ] Connect to your postgres db from within this file
   You'll want execute this code block by moving your cursor within and typing =,,=
  
   #+NAME: Connect org to postgres
   #+BEGIN_SRC emacs-lisp :results silent
     (sql-connect "apisnoop" (concat "*SQL: postgres:data*"))
   #+END_SRC

 - [ ] Test your connection works
   You can run this sql block, and it see a message in your minbuffer like:
   : You are connected to database "apisnoop" as user "apisnoop" on host "localhost" at port "10041".

   #+NAME: Test Connection
   #+BEGIN_SRC sql-mode :results silent
   \conninfo
   #+END_SRC


#+BEGIN_SRC shell
  IFS="+";\
  version=$(curl https://storage.googleapis.com/kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce/1178464478988079104/finished.json | jq .version);\
  commit=${version##*+};\
  commit_hash=${commit%\"*};\
  swagger_link="https://raw.githubusercontent.com/kubernetes/kubernetes/${commit_hash}/api/openapi-spec/swagger.json"
  echo $swagger_link
#+END_SRC

#+RESULTS:
#+begin_EXAMPLE
https://raw.githubusercontent.com/kubernetes/kubernetes/2b795b9825e485/api/openapi-spec/swagger.json
#+end_EXAMPLE

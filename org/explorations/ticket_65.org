#+TITLE: Ticket 65
#+AUTHOR: Zach Mandeville

* Ticket
  [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/65][Gitlab Link]]
  
  #+BEGIN_QUOTE
  We can model our function to load swagger files off the way we load our audit events, where we create a temporary table of data, and then use this to grab metadata and place it in the correct columns.  This will allow us to include the content-hash and url and more for each swagger in its table.
  #+END_QUOTE
* Strategy
  Our audit events are going to be coming from specific buckets and jobs, the artifacts of which hold a metadata.json file that has far more metadata than can be found in the swagger file.  With the swagger, we mostly want to know what the swagger was at the particular commit run for this audit event.  This bucket/job metadata will also help us in doing cross coverage in general.
  
  So the strategy is to create a new table for bucket/job metadata and use the details from that to determine the swagger we pull down.  We can then do a simple join on the swagger to get its commit hash from the metadata.
* Process
** Curl a sample metadata file
   We'll use the most recent job/bucket from apisnoop, following the spyglass link to get to its artifacts.
   
  #+NAME Curl Sample Metadata
  #+BEGIN_SRC shell
  curl 'https://storage.googleapis.com/kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce/1178464478988079104/finished.json'
  #+END_SRC

  #+RESULTS:
  #+begin_EXAMPLE
  {
    "timestamp": 1569805261, 
    "version": "v1.17.0-alpha.0.1912+2b795b9825e485", 
    "result": "SUCCESS", 
    "passed": true, 
    "job-version": "v1.17.0-alpha.0.1912+2b795b9825e485", 
    "metadata": {
      "node_os_image": "cos-73-11647-163-0", 
      "infra-commit": "dba192364", 
      "master_os_image": "cos-73-11647-163-0", 
      "job-version": "v1.17.0-alpha.0.1912+2b795b9825e485", 
      "pod": "f53ddc96-e317-11e9-9585-baa219f9d09d", 
      "revision": "v1.17.0-alpha.0.1912+2b795b9825e485"
    }
  }
  #+end_EXAMPLE
  
  The commit hash is the ~2b795b9825e485~ appended to the version.  We can view the swagger for this commit at this url:
  https://raw.githubusercontent.com/kubernetes/kubernetes/2b795b9825e485/api/openapi-spec/swagger.json
  
** Copy into DB  
  We can load our metadata into our db using the COPY FROM PROGRAM command.  This will just copy over our data as a json blob, so we want to create a temporary table to feed that into, then select from that to generate our actual table.
*** Create job_bucket Table
 #+NAME: job_bucket_full
 #+BEGIN_SRC sql-mode :results silent
   DROP TABLE job_bucket;
   CREATE TABLE job_bucket (
       id int GENERATED BY DEFAULT AS IDENTITY PRIMARY KEY,
       ingested_at timestamp DEFAULT CURRENT_TIMESTAMP,
       bucket text NOT NULL,
       job text NOT NULL,
       commit_hash text NOT NULL,
       passed boolean NOT NULL,
       result text NOT NULL,
       pod text NOT NULL,
       infra_commit text NOT NULL,
       job_version text NOT NULL,
       job_timestamp text NOT NULL,
       node_os_image text NOT NULL,
       master_os_image text NOT NULL
   );
 #+END_SRC
 
*** Create python script for curl|temp table|job_bucket flow
    Am using the audit_event loading as a guide, though put in an explicit exception to help catch sql bugs.
    
 #+NAME: load_job_bucket_via_curl.py
 #+BEGIN_SRC python :eval never :exports code
         try:
             from string import Template
             metadata = ''.join([
                 'curl ',
                 'https://storage.googleapis.com/kubernetes-jenkins/logs/',
                 bucket,
                 '/',
                 job,
                 '/finished.json | jq -c .'])
             sql = Template("""
   DROP TABLE raw_job_bucket_import;
   CREATE TEMPORARY TABLE raw_job_bucket_import (data jsonb not null);
   COPY raw_job_bucket_import (data) FROM PROGRAM '$curl' (DELIMITER e'\x02', FORMAT 'csv', QUOTE e'\x01');
   INSERT INTO job_bucket(
             bucket,
             job,
             commit_hash,
             passed,
             result,
             infra_commit,
             pod,
             job_version,
             job_timestamp,
             node_os_image,
             master_os_image)
     SELECT
             '${bucket}' as bucket,
             '${job}' as job,
             split_part((raw.data ->> 'job-version'), '+', 2) as commit_hash,
             CASE (raw.data ->> 'passed') WHEN 'true' THEN TRUE ELSE FALSE END as passed,
             (raw.data ->> 'result') as result,
             (raw.data -> 'metadata' ->> 'infra-commit') as infra_commit,
             (raw.data -> 'metadata' ->> 'pod') as pod,
             (raw.data -> 'metadata' ->> 'job-version') as job_version,
             (to_timestamp((raw.data ->>'timestamp')::bigint) AT TIME ZONE 'UTC') as job_timestamp,
             (raw.data -> 'metadata' ->> 'node_os_image') as node_os_image,
             (raw.data -> 'metadata' ->> 'master_os_image') as master_os_image
       FROM raw_job_bucket_import raw;
             """).substitute(curl =  metadata, bucket = bucket, job = job)
             rv = plpy.execute(sql)
             return "it worked!"
         except Exception as err:
             return Template("something went wrong, likely this: ${error}").substitute(error = err)
 #+END_SRC
 
 The only real manipulation of the data we do is:
- convert the passed value back into a boolean (it comes in as text)

- convert the unix timestamp into a postgres timestamp

- create commit hash by splitting job_version on '+' and grabbing the last part.
 
We can now create the sql function 
  #+NAME: load_job_bucket_via_curl.sql
  #+BEGIN_SRC sql-mode :noweb yes
    set role dba;
    DROP FUNCTION IF EXISTS load_job_bucket_via_curl;
    CREATE OR REPLACE FUNCTION load_job_bucket_via_curl(bucket text, job text)
    RETURNS text AS $$
    <<load_job_bucket_via_curl.py>>
    $$ LANGUAGE plpython3u ;
    reset role;
  #+END_SRC

  #+RESULTS: load_job_bucket_via_curl.sql
  #+begin_src sql-mode
  SET
  DROP FUNCTION
  apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# apisnoop$# CREATE FUNCTION
  RESET
  #+end_src

*** Load Job Bucket
    Now we test out the loading of the job_buckets.  We'll use the most recent api_snoop bucket and two other random ones in the jenkins logs.
  #+NAME: Test It Out pt. 2
  #+BEGIN_SRC sql-mode
   select * from load_job_bucket_via_curl('ci-kubernetes-e2e-gci-gce', '1178464478988079104');
   select * from load_job_bucket_via_curl('ci-kubernetes-e2e-gci-gce', '1148565955786313730');
   select * from load_job_bucket_via_curl('ci-kubernetes-e2e-gci-gce', '1148586594899333122');
  #+END_SRC

  #+RESULTS: Test It Out pt. 2
  #+begin_src sql-mode
   load_job_bucket_via_curl 
  --------------------------
   it worked!
  (1 row)

  #+end_src

  #+BEGIN_SRC sql-mode
  select commit_hash, job_timestamp, ingested_at from job_bucket;
  #+END_SRC

  #+RESULTS:
  #+begin_src sql-mode
    commit_hash   |    job_timestamp    |        ingested_at         
  ----------------+---------------------+----------------------------
   2b795b9825e485 | 2019-09-30 01:01:01 | 2019-10-02 22:59:50.588151
   99ff994948ff62 | 2019-07-09 12:54:08 | 2019-10-02 22:59:50.697379
   99ff994948ff62 | 2019-07-09 14:17:33 | 2019-10-02 22:59:50.80584
  (3 rows)

  #+end_src
  
  Fantastic!

* Next Steps
** TODO Check for primary key in the job bucket, and ensure we don't put in duplicates
** TODO Expand api_swagger to include commit hash, job, bucket as means to connect to job_bucket.
** TODO Expand audit_events to include commit hash, job, bucket as means to connect to job_bucket.
** TODO Trigger an insertion into our api_swagger, based on insertion into job_bucket
   We will want to take the job/bucket/commit_hash (or maybe just the commit_hash) and pass this into our load_swagger function, upon a successful insertion into job_bucket.  This feels like a trigger function, but am unsure!
** TODO Trigger an insertion into our raw_audit_events, based on insertion into job_bucket
   This is a bit more involved, and will likely require some prepping of the data before we put it into the db.
* Footnotes
** Connect to Database
    If you already have your db and hasura endpoint up and running:
 - [ ] Connect to your postgres db from within this file
   You'll want execute this code block by moving your cursor within and typing =,,=
  
   #+NAME: Connect org to postgres
   #+BEGIN_SRC emacs-lisp :results silent
     (sql-connect "apisnoop" (concat "*SQL: postgres:data*"))
   #+END_SRC

 - [ ] Test your connection works
   You can run this sql block, and it see a message in your minbuffer like:
   : You are connected to database "apisnoop" as user "apisnoop" on host "localhost" at port "10041".

   #+NAME: Test Connection
   #+BEGIN_SRC sql-mode :results silent
   \conninfo
   #+END_SRC

 If the db is not running, or hasn't been setup yet, follow the instructions in [[file:~/ii/apisnoop_v3/org/meta.org::*Welcome,%20ii%20dev!][meta.org]]  , then come back and do the steps above.
 

 #+BEGIN_SRC sql-mode
DROP TABLE raw_job_bucket_import;
 #+END_SRC

 #+RESULTS:
 #+begin_src sql-mode
 DROP TABLE
 #+end_src

 #+NAME: Raw table no variable
 #+BEGIN_SRC sql-mode
   CREATE TABLE raw_job_bucket_import (data jsonb not null);
#+END_SRC

#+BEGIN_SRC sql-mode
   COPY raw_job_bucket_import (data) FROM PROGRAM 'curl https://storage.googleapis.com/kubernetes-jenkins/logs/ci-kubernetes-e2e-gci-gce/1178464478988079104/finished.json | jq -c .' (DELIMITER e'\x02', FORMAT 'csv', QUOTE e'\x01');
#+END_SRC

#+RESULTS:
#+begin_src sql-mode
COPY 1
#+end_src

#+BEGIN_SRC sql-mode
select * from raw_job_bucket_import;
#+END_SRC

#+BEGIN_SRC sql-mode
   INSERT INTO job_bucket(bucket, job, commit_hash, passed, result, infra_commit, pod, job_version, job_timestamp, node_os_image, master_os_image)
     SELECT
             'this' as bucket,
             'works' as job,
             split_part((raw.data ->> 'job-version'), '+', 2) as commit_hash,
             CASE (raw.data ->> 'passed') WHEN 'true' THEN TRUE ELSE FALSE END as passed,
             (raw.data ->> 'result') as result,
             (raw.data -> 'metadata' ->> 'infra-commit') as infra_commit,
             (raw.data -> 'metadata' ->> 'pod') as pod,
             (raw.data -> 'metadata' ->> 'job-version') as job_version,
             (to_timestamp((raw.data ->>'timestamp')::bigint) AT TIME ZONE 'UTC') as job_timestamp,
             (raw.data -> 'metadata' ->> 'node_os_image') as node_os_image,
             (raw.data -> 'metadata' ->> 'master_os_image') as master_os_image
       FROM raw_job_bucket_import raw;
#+END_SRC

#+RESULTS:
#+begin_src sql-mode
INSERT 0 1
#+end_src

#+BEGIN_SRC sql-mode
select (to_timestamp(job_timestamp::bigint) AT TIME ZONE 'UTC') from job_bucket;
#+END_SRC

#+RESULTS:
#+begin_src sql-mode
      timezone       
---------------------
 2019-09-30 01:01:01
 2019-09-30 01:01:01
 2019-09-30 01:01:02
(3 rows)

#+end_src



 #+END_SRC

 #+RESULTS: Raw table no variable
 #+begin_src sql-mode
 CREATE TABLE
 #+end_src

 #+BEGIN_SRC sql-mode
select * from job_bucket;
 #+END_SRC

 #+RESULTS:
 #+begin_src sql-mode
  id | ingested_at | bucket | job | commit_hash | passed | result | pod | infra_commit | job_version | job_timestamp | node_os_image | master_os_image 
 ----+-------------+--------+-----+-------------+--------+--------+-----+--------------+-------------+---------------+---------------+-----------------
 (0 rows)

 #+end_src
** Iteration Loop

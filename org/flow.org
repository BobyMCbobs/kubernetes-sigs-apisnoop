#+TITLE: ApiSnoop_v3 Flow
#+TODO: TODO(t) IN-PROGRESS(i) BLOCKED(b) | DONE(d)
#+PROPERTY: header-args: :noweb yes
#+PROPERTY: header-args:shell+ :results output list

* Purpose
  This org file will serve as a dev diary/primer on apisnoop_v3, to track the days and hours in which we worked on something, and the reasoning behind how we worked on it.  It will be used in tandem with our gitlab tickets, where this org can give further context than the tickets would potentially allow.
* Resources
** Gitlab
   - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3][Our Repo]]
   - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues][Our Issues]]
   - Boards (these double as milestone trackers):
     - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/boards/129?milestone_title=A%2520Shared%2520%2520Schema][A Shared Schema]]
     - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/boards/130?milestone_title=Graphql-Compliant%2520Backend][Graphql-Compliant Backend]]
** Iteration Loop
 These steps assume you have pasted the tmate code into a new terminal, when you started up this org file.
 If you have not done this, type == SPC SPC normal-mode== to reset this org file.  It should give you the command again, which you can paste into a new terminal.
** Navigate to our v3 Repo and start up server
   I would like to be able to get to the root of our apisnoop_v3 repo, based on where someone is reading this org, but am having trouble with the src code block to do so.  This is a rabbit hole i'm avoiding, by hardcoding right now.  This assumes you have the repo at ~/ii/apisnoop_v3
   
   #+NAME: Navigate to Repo
   #+BEGIN_SRC tmate
     cd ~/ii/apisnoop_v3
     node ./src/index.js
   #+END_SRC

** deploy new database model to our prisma database
* Dev Diary
** July
*** 1st
**** DONE Prepare this Org
     CLOSED: [2019-07-01 Mon 10:05]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 10:00]--[2019-07-01 Mon 10:04] =>  0:04
     :END:
     I want to have a basic structure that Josmar and others can follow for what I'm doing and why.
     This will grow as needed, so will not spend too long on it now.
**** DONE Add Dan's Comments to tickets in gitlab
     CLOSED: [2019-07-01 Mon 10:19]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 10:07]--[2019-07-01 Mon 10:19] =>  0:12
     :END:
     This is based off a slack convo [[https://mattermost.ii.coop/files/ihzmi3whb3d4xpssynbnh7sude/public?h=5CpNsQ9EyK3IZeHl2Ue8jdI7vD9ENx_T90EsPthuNSs][shared in mattermost]].  
     The Key Points are:
     - Arrange a call with Aaron, Brian, and Belamaric to see if automated tools like apisnoop would help.
       - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/5][Issue 5]]
     - Get Apisnoop integrated into the beginning of every call
       - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/6][Issue 6]]
     - Get apisnoop integrated into prow so someone can see the conformance increase for a pr.
       - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/7][Issue 7]] 
     
    I would consider the last two to be 'discussion' issues, to figure out how we can design APISnoop for both, so I labeled them as such.  A future task will be to start the discussion here, as I got opinions on how we can do both.
**** DONE Make sure we have a starting board for the schema and server boards
     CLOSED: [2019-07-01 Mon 10:22]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 10:22]--[2019-07-01 Mon 10:22] =>  0:00
     :END:
     We've added this as part of the ticket writing.  The boards have been added to this org, under our [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/1][gitlab resource]]
**** DONE Writeup Tickets for populating this backend.
     CLOSED: [2019-07-01 Mon 10:43]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 10:25]--[2019-07-01 Mon 10:43] =>  0:18
     :END:
     Once we have a backend, it needs to have data in it.  This is a combination of schema design, and writing functions for how we post new data to the backend.  I think it is usefult o have thse outlined as single ticket,s to better track the work, and so will refine issue 1 to be an umbrella issue, and better outline the steps needed to show it's a workable backend.
     
     I've fleshed out issue 1, connecting it to all its sub-tickets.  [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/1][Check out issue 1]]
**** DONE Issue 8: Setup backend, that's tied to a db and has a graphql explorer
     CLOSED: [2019-07-01 Mon 15:52]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 14:43]-[2019-07-01 Mon 15:52] =>  1:09
     CLOCK: [2019-07-01 Mon 10:45]--[2019-07-01 Mon 11:00] =>  0:15
     :END:
     [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/8][Link to Issue 8]]
     
     The majority of this work has been done in this repo on Friday.  The work now is to be able to commit these changes to our repo and start to document how it works.
     
**** DONE Issue 9: Write a mutation for populating db with an audit log.
     CLOSED: [2019-07-01 Mon 17:42]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 15:55]--[2019-07-01 Mon 17:42] =>  1:47
     :END:
     [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/9][Link to Issue 9]]
     
     I am thinking we'l base this off the metadata.json of the audit log, since that's available from the start with the log.   So the flow will be:
- post a new auditlog, based on the schema, that contains the basic metadata.
- post new individual endpoints that have, for the auditlog value, the id of our just posted auditLog

This should allow us to have a nicely connected tables and being able to see the coverage of an endpoint over time (comparing aacross audit logs), while also setting us up nicely for the data-generation script.

I worked off the hackernews tutorial i did, simplifying and refacotring as needed.  I put in the types defined in app.org as part of our 'release metadata', but renamed release to AuditLog, as that's more accurate to what the thing is.
     
**** DONE Issue 10: Write a mutation for populating an endpoint to backend, that's connected to a release.
     CLOSED: [2019-07-01 Mon 19:16]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 18:11]--[2019-07-01 Mon 19:16] =>  1:05
     CLOCK: [2019-07-01 Mon 18:09]--[2019-07-01 Mon 18:10] =>  0:01
     CLOCK: [2019-07-01 Mon 17:50]-[2019-07-01 Mon 18:10] =>  0:20
     :END:
     [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/10][Link to #10]]
     I should be able to follow the path of the auditlog schema.  The tricky part here will be connecting an endpoint to a release, and how to set that up in a resolver.  I feel I can use the graphql tutorial at howtographql.com, and follow their example of a post and a user.
     
     ---
     
     This worked out smoothly, using that example.  I've set up a mutation that requires an auditLog id, it then connects to that auditlog when put in.  This means that when I query an endpoint, I can _also_ see all the info about the audit log its a part of.  when i query audit logs I automatically see all their endpoints.  It's a nice pattern, and simply implemented.

    We just need to remember in the data-gen script we'll take an audit log, and post its metadata to the db.  the db will return an ID...we then need to take that ID and use it as a variable for all the endpoints, useragents, tests, etc, we post next.  This _feels_ simple to me, and doesn't introduce anything new to how we've defined everything.  
**** DONE Issue 12: Write a query for endpoint, to see its info including the audit log its connected to
     CLOSED: [2019-07-01 Mon 19:46]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 19:27]-[2019-07-01 Mon 19:46] =>  0:19
     :END:
     [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/12][Link to #12]]
     This is asking to ping for a specific endpoint, but I feel I should just make it a filter...to put in the ability of starting to add filters like "group is SUCH" or "category is SUCH".
     
     I'll put in a proof of concept for this,w hich will let us return a single endpoint and the auditlog its a part of, and return to it later to figure out how to best structure these filters.
     
     It's now been implemented iwth operationID. I think I could add additional filters for any part of the endpoint,a nd it would be just as easy, but I worrya bout being verbose.  There must be a simpler or cleaner way of providing multiple kinds of filters.  An interesting problem for later, added to future tasks.
**** DONE Update README with dev setup and use.
     CLOSED: [2019-07-01 Mon 19:54]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 19:46]--[2019-07-01 Mon 19:54] =>  0:08
     :END:
     
**** DONE Issue One: Setup Initial Backend
     CLOSED: [2019-07-01 Mon 19:57]
     [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/1][Link to Issue 1]]
    
     The above work fulfilled all the requirements for issue 1.  This isn't a fully-fledged server yet, as it has no real data.  But it's a good sanity check.
* Future Tasks
** NOW: Tasks of the highest priority
*** TODO Write up tickets related to our datagen script work.
*** TODO Work on DataGen script to populate database with actual audit log.
** FUTURE: Cool ideas for the future
*** TODO Add to discusion in issue 6 and issue 7
      - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/6][Issue 6]]
      - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/7][Issue 7]] 
*** TODO Look into adding nested filters to query
    Related to [[**** TODO Issue 12: Write a query for endpoint, to see its info including the audit log its connected to][Issue 12]] 
    


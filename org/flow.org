#+TITLE: ApiSnoop_v3 Flow
#+TODO: TODO(t) IN-PROGRESS(i) BLOCKED(b) | DONE(d)
#+PROPERTY: header-args: :noweb yes
#+PROPERTY: header-args:shell+ :results output list

* Purpose
  This org file will serve as a dev diary/primer on apisnoop_v3, to track the days and hours in which we worked on something, and the reasoning behind how we worked on it.  It will be used in tandem with our gitlab tickets, where this org can give further context than the tickets would potentially allow.
* Resources
** Gitlab
   - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3][Our Repo]]
   - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues][Our Issues]]
   - Boards (these double as milestone trackers):
     - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/boards/129?milestone_title=A%2520Shared%2520%2520Schema][A Shared Schema]]
     - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/boards/130?milestone_title=Graphql-Compliant%2520Backend][Graphql-Compliant Backend]]
** Iteration Loop
 These steps assume you have pasted the tmate code into a new terminal, when you started up this org file.
 If you have not done this, type == SPC SPC normal-mode== to reset this org file.  It should give you the command again, which you can paste into a new terminal.
*** Navigate to our v3 Repo and start up server
    I would like to be able to get to the root of our apisnoop_v3 repo, based on where someone is reading this org, but am having trouble with the src code block to do so.  This is a rabbit hole i'm avoiding, by hardcoding right now.  This assumes you have the repo at ~/ii/apisnoop_v3
   
    #+NAME: Navigate to Repo
    #+BEGIN_SRC tmate
      cd ~/ii/apisnoop_v3
      node ./src/index.js
    #+END_SRC

*** View the Playground
    From your browser, navigate to =localhost:4000=

*** deploy new database model to our prisma database
** Useful Links
   - [[https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/apiserver/pkg/apis/audit/v1/types.go#L72][Audit Log Event Struct in kubernetes go types]]
   - [[https://www.twistlock.com/2019/03/14/kubernetes-auditsink-real-time-k8s-audits-forensics/][Twistlock: example of doing dynamic reading of audit logs]]
   - [[https://scanner.heptio.com/][sonobuoy: good example of the UX/UI we are ultimately aiming for]]
     - being able to run =kubectl apply apisnoop= and it reads teh audit log on the cluster yr working on, and gives you a sunburst and graphql playground for your work.
** Notes from talk with hh
   - Take a look at an auditlog
     - each line of the auditlog we suspect to be valid, consistent json.
       - each line of the auditlog is a serialized json representation of the auditlog entry go type.  and the auditlog entry go type is defined.
     - data-gen adds to db each line, mapped 1-to-1
     - think of the uploading
   - our data structures we have now are not auditlogs but "prowjob logs"
   - look at the Auditlog Event Go Type where it is sharply defined what the structure for an audit log entry is.  We can use this to define the schema for our mutations.
** Sample Audit Log stored in ./data/kube-apiserver-audit.log
   this is only on my local one, since it's a 400mb file.  future flow.org will have script to download it from a set bucket.
* Example Queries
** Running Queries
  -  [[** Navigate to our v3 Repo and start up server][Navigate to our v3 Repo and start up server]] 
  [[*View the Playground][- View the Playground]] 
  - Paste the query into the left side and hit play, or =ctrl-enter= 
   
** All AuditLogs and Endpoints, including endpoints hits
   What's useful for this is to see that you can have an endpoint with the same opId and associated with different AuditLogs
   #+BEGIN_SRC graphql
     query{
       auditLogs{
         id
         version
         endpoints{
           id
           operationID
           hits
           conformanceHits
           testHits
         }
       }
     }
   #+END_SRC

* Dev Diary
** July
*** 1st
#+BEGIN: clocktable :scope subtree
#+CAPTION: Clock summary at [2019-07-01 Mon 20:04]
| Headline     | Time   |
|--------------+--------|
| *Total time* | *5:38* |
|--------------+--------|
#+END:
**** DONE Prepare this Org
     CLOSED: [2019-07-01 Mon 10:05]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 10:00]--[2019-07-01 Mon 10:04] =>  0:04
     :END:
     I want to have a basic structure that Josmar and others can follow for what I'm doing and why.
     This will grow as needed, so will not spend too long on it now.
**** DONE Add Dan's Comments to tickets in gitlab
     CLOSED: [2019-07-01 Mon 10:19]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 10:07]--[2019-07-01 Mon 10:19] =>  0:12
     :END:
     This is based off a slack convo [[https://mattermost.ii.coop/files/ihzmi3whb3d4xpssynbnh7sude/public?h=5CpNsQ9EyK3IZeHl2Ue8jdI7vD9ENx_T90EsPthuNSs][shared in mattermost]].  
     The Key Points are:
     - Arrange a call with Aaron, Brian, and Belamaric to see if automated tools like apisnoop would help.
       - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/5][Issue 5]]
     - Get Apisnoop integrated into the beginning of every call
       - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/6][Issue 6]]
     - Get apisnoop integrated into prow so someone can see the conformance increase for a pr.
       - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/7][Issue 7]] 
     
    I would consider the last two to be 'discussion' issues, to figure out how we can design APISnoop for both, so I labeled them as such.  A future task will be to start the discussion here, as I got opinions on how we can do both.
**** DONE Make sure we have a starting board for the schema and server boards
     CLOSED: [2019-07-01 Mon 10:22]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 10:22]--[2019-07-01 Mon 10:22] =>  0:00
     :END:
     We've added this as part of the ticket writing.  The boards have been added to this org, under our [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/1][gitlab resource]]
**** DONE Writeup Tickets for populating this backend.
     CLOSED: [2019-07-01 Mon 10:43]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 10:25]--[2019-07-01 Mon 10:43] =>  0:18
     :END:
     Once we have a backend, it needs to have data in it.  This is a combination of schema design, and writing functions for how we post new data to the backend.  I think it is usefult o have thse outlined as single ticket,s to better track the work, and so will refine issue 1 to be an umbrella issue, and better outline the steps needed to show it's a workable backend.
     
     I've fleshed out issue 1, connecting it to all its sub-tickets.  [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/1][Check out issue 1]]
**** DONE Issue 8: Setup backend, that's tied to a db and has a graphql explorer
     CLOSED: [2019-07-01 Mon 15:52]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 14:43]-[2019-07-01 Mon 15:52] =>  1:09
     CLOCK: [2019-07-01 Mon 10:45]--[2019-07-01 Mon 11:00] =>  0:15
     :END:
     [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/8][Link to Issue 8]]
     
     The majority of this work has been done in this repo on Friday.  The work now is to be able to commit these changes to our repo and start to document how it works.
     
**** DONE Issue 9: Write a mutation for populating db with an audit log.
     CLOSED: [2019-07-01 Mon 17:42]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 15:55]--[2019-07-01 Mon 17:42] =>  1:47
     :END:
     [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/9][Link to Issue 9]]
     
     I am thinking we'l base this off the metadata.json of the audit log, since that's available from the start with the log.   So the flow will be:
- post a new auditlog, based on the schema, that contains the basic metadata.
- post new individual endpoints that have, for the auditlog value, the id of our just posted auditLog

This should allow us to have a nicely connected tables and being able to see the coverage of an endpoint over time (comparing aacross audit logs), while also setting us up nicely for the data-generation script.

I worked off the hackernews tutorial i did, simplifying and refacotring as needed.  I put in the types defined in app.org as part of our 'release metadata', but renamed release to AuditLog, as that's more accurate to what the thing is.
     
**** DONE Issue 10: Write a mutation for populating an endpoint to backend, that's connected to a release.
     CLOSED: [2019-07-01 Mon 19:16]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 18:11]--[2019-07-01 Mon 19:16] =>  1:05
     CLOCK: [2019-07-01 Mon 18:09]--[2019-07-01 Mon 18:10] =>  0:01
     CLOCK: [2019-07-01 Mon 17:50]-[2019-07-01 Mon 18:10] =>  0:20
     :END:
     [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/10][Link to #10]]
     I should be able to follow the path of the auditlog schema.  The tricky part here will be connecting an endpoint to a release, and how to set that up in a resolver.  I feel I can use the graphql tutorial at howtographql.com, and follow their example of a post and a user.
     
     ---
     
     This worked out smoothly, using that example.  I've set up a mutation that requires an auditLog id, it then connects to that auditlog when put in.  This means that when I query an endpoint, I can _also_ see all the info about the audit log its a part of.  when i query audit logs I automatically see all their endpoints.  It's a nice pattern, and simply implemented.

    We just need to remember in the data-gen script we'll take an audit log, and post its metadata to the db.  the db will return an ID...we then need to take that ID and use it as a variable for all the endpoints, useragents, tests, etc, we post next.  This _feels_ simple to me, and doesn't introduce anything new to how we've defined everything.  
**** DONE Issue 12: Write a query for endpoint, to see its info including the audit log its connected to
     CLOSED: [2019-07-01 Mon 19:46]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 19:27]-[2019-07-01 Mon 19:46] =>  0:19
     :END:
     [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/12][Link to #12]]
     This is asking to ping for a specific endpoint, but I feel I should just make it a filter...to put in the ability of starting to add filters like "group is SUCH" or "category is SUCH".
     
     I'll put in a proof of concept for this,w hich will let us return a single endpoint and the auditlog its a part of, and return to it later to figure out how to best structure these filters.
     
     It's now been implemented iwth operationID. I think I could add additional filters for any part of the endpoint,a nd it would be just as easy, but I worrya bout being verbose.  There must be a simpler or cleaner way of providing multiple kinds of filters.  An interesting problem for later, added to future tasks.
**** DONE Update README with dev setup and use.
     CLOSED: [2019-07-01 Mon 19:54]
     :LOGBOOK:
     CLOCK: [2019-07-01 Mon 19:46]--[2019-07-01 Mon 19:54] =>  0:08
     :END:
     
**** DONE Issue One: Setup Initial Backend
*** 2nd
    I did not do work on v3, as work was needed to be done on the current apisnoop, to add auditlogs for july 1st and june 1st, then debug when the pr preview was not displaying correctly.
*** 3rd
#+BEGIN: clocktable :scope subtree :maxlevel 2
#+CAPTION: Clock summary at [2019-07-04 Thu 11:02]
| Headline     | Time   |
|--------------+--------|
| *Total time* | *4:39* |
|--------------+--------|
#+END:

 
    Today is a small cross-roads.  I could keep working on putting individual entries into our db, to ensure we can successfully post new test_tags, useragents, and tests.  Or I could start to work on the data-gen script, so that we can start to put in actual audit logs.  
    
    If I work on the former, the downside is that our results back will still just be a Proof of Concept, with a single entry for each thing.  The upside is that I'll know exactly what the post should look like.
    
    If I work on the latter, I'll get closr to being able to populate it with real data (and could maybe try to get it to just post an auditlog's metadata and nothing else.)  The downside is that this si some big work to untangle, and I won't yet know how to post every bit of data that the datagen script is generating.
    
    I am making a choice to get our schema finished, with just basic entries for each thing, before moving to data-gen

**** DONE Write out tickets for getting test_tags, useragents, and tests into db.
     CLOSED: [2019-07-03 Wed 09:24]
     :LOGBOOK:
     CLOCK: [2019-07-03 Wed 09:10]--[2019-07-03 Wed 09:24] =>  0:14
     :END:
     Added tickets for 14, 15, 16, 17, 18 and added them to this flow as TODO's.
**** IN-PROGRESS #14 Write a mutation for populating tests in the db, that are connected to auditlogs and endpoints.
     :LOGBOOK:
     CLOCK: [2019-07-04 Thu 08:36]--[2019-07-04 Thu 09:02] =>  0:26
     CLOCK: [2019-07-04 Thu 05:30]--[2019-07-04 Thu 06:55] =>  1:25
     CLOCK: [2019-07-03 Wed 09:24]--[2019-07-03 Wed 11:58] =>  2:34
     :END:
     I want to make sure I can add a test for a specific auditlog, and then only connect it to endpoints that have the same auditlog.  The reasonb eing that there will be tests with the same name across auditlogs, and endpoints with the same name across auditlogs, and we need to ensure we aren't making duplicates.
    
     When I add a test, I won't know each of the endpoints id's.  I think it's well enough to match on operationID, as it should be a constant, and also auditLog.  So it's a question on whether I can do two arguments in the prisma ={ connect }= function. 

***** [40%] Tasks to complete ticket:
      - [X] populate db with another auditlog
        - hoooo boy, this was harder than I expected!  I misunderstood what could be passed along when creating a new auditlog, after I'd made that endpoint connection.  I was thinking I needed to pass along an empty array...since auditLogs should now have an 'endpoints' field...just we don't know what'll go in them.  But you cannot add empty arrays in mutations, and i was getting a (cryptic) error message telling me this.
        - I ended up signing up for a number of prisma forums and channels trying to find the answre to this, and got the answer within [[https://www.prisma.io/forum/t/mutation-failing-problem-with-non-nullable-array/5748][prisma.io/forum]].  This is useful though, as I now know the structure for extending this to include useragents, tests, etc and know what resources I have.
      - [X] add an endpoint with same opId as one in there, but to this new auditlog
        - Check it with this example query: [[** All AuditLogs and Endpoints, including endpoints hits][All AuditLogs and Endpoints, including endpoints hits]]  
      - [ ] Write a test mutation with an AuditLog field, matching the endpoints field, and an endpoints field that connects based on endpoints opId and auditLog.
        - having an issue with this, in that I am trying to connect all these endpoints based on their operationID, but cannot as that is not a @unique field.  We are going to have the same opID's across jobs, releases, buckets etc.
        - The tests are going to end up with a list of endpoints they hit.  It will be the same testname across releases, though somtimes there might be an adjustment (e.g. when promoted to conformance).  They'll often have the same list of endpoints, but not always (if the test is written to hit less, but more specific ones, or more get added to it.)
        - The test json now just includes a list of operationID's for the things they hit.  I'd like to connect this list to the actual endpoints, so if you query a test, you can also grab the relevant endpoint info in the same query.
        - The issue is that the operationID's are not unique.  What's unique is operationID + auditLogID.
        - So Options (ordered by my preference):
          1. figure out how to run a map on just the args.endpoints before we do the mutation, to retrun an array of endpointID's (which are unique).  use this to connect.
          2. create a new field which is opID+auditLogID and make this @unique.  I can then connect immediately to that.
          3. don't connect tests to endpoints within the api, and expect people to do it as a separate query.
        - Of the first two, option 2 seems easiest, but I can timebox how to do 1 first.  If not possible, I'll adjust our endpoints api for 2.
         
      - [ ] Add a new test to DB, with an existing endpoint included
      - [ ] query the test and ensure the right endpoint comes back.

      To test this, I'll need to 
    
    
    
**** TODO #15 Write a mutation for useragent, that's connected to an auditlog and endpoints.
**** TODO #16 Write a query for useragent, to see its info including the audit log its connected to and endpoints it hits
**** TODO #17 Write a query for tests, including its auditlog and endpints
**** TODO #18 Write a query for test_tags, that includes its auditLog and endpoints

    
    
**** TODO Issue #3: Develop Schema for tests, test_tags, useragents 
     [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/3][Link to #3]]
*** 4th
**** DONE Write Up Tickets for our new Tasks, based on [[*Notes from talk with hh][Notes from talk with hh]]  
     CLOSED: [2019-07-05 Fri 04:20]
     :LOGBOOK:
     CLOCK: [2019-07-05 Fri 03:51]--[2019-07-05 Fri 04:11] =>  0:20
     :END:
**** DONE Organize Org file with relevant links for this work
     CLOSED: [2019-07-05 Fri 04:20]
     :LOGBOOK:
     CLOCK: [2019-07-05 Fri 04:12]--[2019-07-05 Fri 04:15] =>  0:03
     :END:
     You can now find useful links within our resources: [[*Useful Links][Useful Links]] 
**** IN-PROGRESS #19 look into our existing 'download-audits.sh' to grab an audit
     :LOGBOOK:
     CLOCK: [2019-07-05 Fri 04:15]
     :END:
   [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/19][link to #19]]  
   I need to remember that the end goal is just to have an auditlog. And so instead of looking at code directly, i'll look at the prow job build logs from a pr, to see the lines actually run and where it's pulling data from.  Then I can do a =gsutil= to grab the relevant log.
   
   To grab an auditlog I:
   - took a look at the build log and saw(remembered) that our main command is now ./apisnoop.sh, and that it stores the audit-logs as part of the --update-cache step
   - I ssh'ed into the zpair box of packet, cloned apisnoop, and did ./apisnoop.sh --install && ./apisnoop.sh --update-cache
   - this gave me a number of audit logs within ./data-gen/cache.  They are arranged by the version, then the job, so I went into a recent job for master (...e23-gci-gce...)
   - The results have a single =kube-apiserver-audit.log= plus some =kube-apiserver-lotsofunumbers.gz=
   - I gunzipped a random one, then ran =cat randomlog | jq keys=, then ran the same command on the =kube-apiserver-audit.log=.  The keys are the same, and so I could see they were the same _type_ of file, but broken into multiple sections to make the d/ling easier (I knew this, but wanted to confirm).
   - I took the kube-apiserver-audit.log and rsynced it down to this repo's =data= folder so we could explore further.
   - I noted the location within the resources of this flow.org (couldn't add a proper file link yet, and it felt like a rabbit hole)
**** TODO #20 Investigate how our d/led audit log maps to the audit log struct type.
    [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/20][link to #20]] 
**** TODO #21 build a mutation for the events in our d'led audit logs to be added to our db.
     [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/21][link to #21]] 
**** TODO #22 build out a data-gen script that eats a d/led audit log and, per each line, posts to our db.
**** TODO #23 Our Backend is populated with raw audit log events from auditlog.txt
    [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/23][Link to #23]] 
* Future Tasks
** NOW: Tasks of the highest priority
** FUTURE: Cool ideas for the future
*** TODO Write up tickets related to our datagen script work.
*** TODO Work on DataGen script to populate database with actual audit log.
*** TODO Add to discusion in issue 6 and issue 7
      - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/6][Issue 6]]
      - [[https://gitlab.ii.coop/apisnoop/apisnoop_v3/issues/7][Issue 7]] 
*** TODO Look into adding nested filters to query
    Related to [[**** TODO Issue 12: Write a query for endpoint, to see its info including the audit log its connected to][Issue 12]] 
    
*** TODO figure out how to add org links to files within repo       :org:foo:
*** TODO add our sample audit.log to a bucket for easy =wget=


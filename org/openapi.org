#+TITLE: Creating a schema for the OpenAPI Spec
#+PROPERTY: header-args:python :tangle yes

* What do we have at a high level

Set the :dir for shell to ~/go/src/k8s.io/kubernetes/api/openapi-spec

#+NAME: swagger keys
#+BEGIN_SRC shell :dir ~/go/src/k8s.io/kubernetes/api/openapi-spec
  cat swagger.json | jq -r keys[]
#+END_SRC

Looks like we have 6 different high levels. Let's walk through them.

#+RESULTS: swagger keys
#+begin_EXAMPLE
definitions
info
paths
security
securityDefinitions
swagger
#+end_EXAMPLE

** notes
  Download the OpenAPI Spec

#+BEGIN_SRC tmate
    # or set your folder to ~/go/src/k8s.io/kubernetes and ensure you are checked out to master etc
    cd ~/ii/apisnoop_v3
    curl -o swagger.json \
         https://raw.githubusercontent.com/kubernetes/kubernetes/master/api/openapi-spec/swagger.json
#+END_SRC

[[file:../swagger.json]]
https://github.com/TomNomNom/gron

#+BEGIN_SRC shell
curl -L https://github.com/tomnomnom/gron/releases/download/v0.6.0/gron-linux-amd64-0.6.0.tgz \
|  tar xvfzC - ~/go/bin
#|  tar xvfzC - /usr/local/bin
# or:
# go get -u github.com/tomnomnom/gron
#+END_SRC

#+RESULTS:
#+begin_EXAMPLE
#+end_EXAMPLE

#+BEGIN_SRC shell
file ~/go/bin/gron
file /usr/local/bin/gron
#+END_SRC

#+RESULTS:
#+begin_EXAMPLE
/zfs/home/hh/go/bin/gron: POSIX tar archive (GNU)
/usr/local/bin/gron: cannot open `/usr/local/bin/gron' (No such file or directory)
#+end_EXAMPLE



* SQL
#+NAME: Start Postgresql Connection
#+BEGIN_SRC emacs-lisp :results silent
  ;; (sql-connect connection (concat "*SQL: postgres:data*"))
  (sql-connect "hasura" (concat "*SQL: postgres:data*"))
#+END_SRC

#+BEGIN_SRC sql-mode
  \conninfo
#+END_SRC

#+RESULTS:
#+begin_src sql-mode
You are connected to database "hh" as user "hh" on host "172.17.0.1" at port "5432".
SSL connection (protocol: TLSv1.3, cipher: TLS_AES_256_GCM_SHA384, bits: 256, compression: off)
#+end_src

#+BEGIN_SRC sql-mode
\d+
#+END_SRC

#+RESULTS:
#+begin_src sql-mode
                       List of relations
 Schema |      Name      | Type  | Owner |  Size  | Description 
--------+----------------+-------+-------+--------+-------------
 public | api_operations | table | hh    | 264 kB | 
 public | audit_events   | table | hh    | 243 MB | 
(2 rows)

#+end_src

#+BEGIN_SRC sql-mode
select count(*) from api_operations;
#+END_SRC

#+RESULTS:
#+begin_src sql-mode
 count 
-------
   889
(1 row)

#+end_src

#+BEGIN_SRC sql-mode
select count(*) from audit_events;
#+END_SRC

#+RESULTS:
#+begin_src sql-mode
 count  
--------
 556561
(1 row)

#+end_src

#+BEGIN_SRC sql-mode
\d audit_events
#+END_SRC

#+RESULTS:
#+begin_src sql-mode
                          Table "public.audit_events"
       Column       |           Type           | Collation | Nullable | Default 
--------------------+--------------------------+-----------+----------+---------
 auditID            | uuid                     |           |          | 
 testrunID          | text                     |           |          | 
 opID               | text                     |           |          | 
 level              | text                     |           | not null | 
 verb               | text                     |           | not null | 
 requestURI         | text                     |           | not null | 
 userAgent          | text                     |           |          | 
 testName           | text                     |           |          | 
 requestkind        | text                     |           | not null | 
 requestapiversion  | text                     |           | not null | 
 requestmeta        | jsonb                    |           | not null | 
 requestspec        | jsonb                    |           | not null | 
 requeststatus      | jsonb                    |           | not null | 
 responsekind       | text                     |           | not null | 
 responseapiversion | text                     |           | not null | 
 responsemeta       | jsonb                    |           | not null | 
 responsespec       | jsonb                    |           | not null | 
 responsestatus     | jsonb                    |           | not null | 
 timeStamp          | timestamp with time zone |           |          | 

#+end_src

#+BEGIN_SRC sql-mode
\d api_operations
#+END_SRC

#+RESULTS:
#+begin_src sql-mode
            Table "public.api_operations"
   Column    | Type | Collation | Nullable | Default 
-------------+------+-----------+----------+---------
 id          | text |           | not null | 
 method      | text |           | not null | 
 path        | text |           | not null | 
 regexp      | text |           | not null | 
 group       | text |           | not null | 
 version     | text |           | not null | 
 kind        | text |           | not null | 
 category    | text |           | not null | 
 description | text |           | not null | 

#+end_src

** uris in order of execution by test
#+BEGIN_SRC sql-mode
  select "requestURI" from audit_events
  where "testName" = ' [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]'
  order by "timeStamp"
#+END_SRC

** list of tests
#+NAME: list of tests
#+BEGIN_SRC sql-mode :wrap "SRC text"
select distinct "testName" from audit_events order by "testName";
#+END_SRC

#+RESULTS: list of tests
#+begin_SRC text
                                                                                                               testName                                                                                                                
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
 
  [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
  [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
  [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
  [k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
  [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  [k8s.io] Container Runtime blackbox test on terminated container should report termination message [LinuxOnly] if TerminationMessagePath is set [NodeConformance]
  [k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull from private registry with secret [NodeConformance]
  [k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from docker hub [NodeConformance]
  [k8s.io] Container Runtime blackbox test when running a container with a new image should be able to pull image from gcr.io [NodeConformance]
  [k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull from private registry without secret [NodeConformance]
  [k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull image from invalid registry [NodeConformance]
  [k8s.io] Container Runtime blackbox test when running a container with a new image should not be able to pull non-existing image from gcr.io [NodeConformance]
  [k8s.io] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
  [k8s.io] Docker Containers should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  [k8s.io] Docker Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  [k8s.io] Docker Containers should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  [k8s.io] Docker Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
  [k8s.io] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
  [k8s.io] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  [k8s.io] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  [k8s.io] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  [k8s.io] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
  [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
  [k8s.io] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
  [k8s.io] Kubelet when scheduling a busybox Pod with hostAliases should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  [k8s.io] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  [k8s.io] NodeLease when the NodeLease feature is enabled the kubelet should create and update a lease in the kube-node-lease namespace
  [k8s.io] NodeLease when the NodeLease feature is enabled the kubelet should report node status infrequently
  [k8s.io] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  [k8s.io] Pods should be submitted and removed [NodeConformance] [Conformance]
  [k8s.io] Pods should be updated [NodeConformance] [Conformance]
  [k8s.io] Pods should contain environment variables for services [NodeConformance] [Conformance]
  [k8s.io] Pods should get a host IP [NodeConformance] [Conformance]
  [k8s.io] Pods should support pod readiness gates [NodeFeature:PodReadinessGate]
  [k8s.io] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
  [k8s.io] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  [k8s.io] PrivilegedPod [NodeConformance] should enable privileged commands [LinuxOnly]
  [k8s.io] Probing container should be restarted with a docker exec liveness probe with timeout 
  [k8s.io] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  [k8s.io] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  [k8s.io] Probing container should be restarted with a local redirect http liveness probe
  [k8s.io] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
  [k8s.io] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  [k8s.io] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  [k8s.io] Probing container should *not* be restarted with a non-local redirect http liveness probe
  [k8s.io] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance]
  [k8s.io] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  [k8s.io] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  [k8s.io] Security Context When creating a container with runAsNonRoot should not run with an explicit root user ID
  [k8s.io] Security Context When creating a container with runAsNonRoot should not run without a specified user ID
  [k8s.io] Security Context When creating a container with runAsNonRoot should run with an explicit non-root user ID
  [k8s.io] Security Context When creating a container with runAsNonRoot should run with an image specified user ID
  [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 0 [LinuxOnly] [NodeConformance]
  [k8s.io] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  [k8s.io] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with readonly rootfs when readOnlyRootFilesystem=true [LinuxOnly] [NodeConformance]
  [k8s.io] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when not explicitly set and uid != 0 [LinuxOnly] [NodeConformance]
  [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should allow privilege escalation when true [LinuxOnly] [NodeConformance]
  [k8s.io] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  [k8s.io] [sig-node] AppArmor load AppArmor profiles can disable an AppArmor profile, using unconfined
  [k8s.io] [sig-node] AppArmor load AppArmor profiles should enforce an AppArmor profile
  [k8s.io] [sig-node] crictl should be able to run crictl on the node
  [k8s.io] [sig-node] Events should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  [k8s.io] [sig-node] kubelet [k8s.io] [sig-node] Clean up pods on node kubelet should be able to delete 10 pods per node in 1m0s.
  [k8s.io] [sig-node] Mount propagation should propagate mounts to the host
  [k8s.io] [sig-node] NodeProblemDetector [DisabledForLargeClusters] should run without error
  [k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period should be submitted and removed [Conformance]
  [k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  [k8s.io] [sig-node] PreStop graceful pod terminated should wait until preStop hook completes the process
  [k8s.io] [sig-node] PreStop should call prestop when killing a pod  [Conformance]
  [k8s.io] [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly]
  [k8s.io] [sig-node] Security Context should support container.SecurityContext.RunAsUser [LinuxOnly]
  [k8s.io] [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly]
  [k8s.io] [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser [LinuxOnly]
  [k8s.io] [sig-node] Security Context should support pod.Spec.SecurityContext.SupplementalGroups [LinuxOnly]
  [k8s.io] [sig-node] SSH should SSH to all nodes and run commands
  [k8s.io] Sysctls [NodeFeature:Sysctls] should not launch unsafe, but not explicitly enabled sysctls on the node
  [k8s.io] Sysctls [NodeFeature:Sysctls] should reject invalid sysctls
  [k8s.io] Sysctls [NodeFeature:Sysctls] should support sysctls
  [k8s.io] Sysctls [NodeFeature:Sysctls] should support unsafe sysctls which are actually whitelisted
  [k8s.io] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
  [k8s.io] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
  [k8s.io] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
  [k8s.io] Variable Expansion should allow substituting values in a volume subpath [sig-storage][NodeFeature:VolumeSubpathEnvExpansion]
  [sig-api-machinery] AdmissionWebhook Should be able to deny attaching pod
  [sig-api-machinery] AdmissionWebhook Should be able to deny custom resource creation and deletion
  [sig-api-machinery] AdmissionWebhook Should be able to deny pod and configmap creation
  [sig-api-machinery] AdmissionWebhook Should deny crd creation
  [sig-api-machinery] AdmissionWebhook Should honor timeout
  [sig-api-machinery] AdmissionWebhook Should mutate configmap
  [sig-api-machinery] AdmissionWebhook Should mutate custom resource
  [sig-api-machinery] AdmissionWebhook Should mutate custom resource with different stored version
  [sig-api-machinery] AdmissionWebhook Should mutate custom resource with pruning
  [sig-api-machinery] AdmissionWebhook Should mutate pod and apply defaults after mutation
  [sig-api-machinery] AdmissionWebhook Should not be able to mutate or prevent deletion of webhook configuration objects
  [sig-api-machinery] AdmissionWebhook Should unconditionally reject operations on fail closed webhook
  [sig-api-machinery] Aggregator Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  [sig-api-machinery] CustomResourceConversionWebhook Should be able to convert a non homogeneous list of CRs
  [sig-api-machinery] CustomResourceConversionWebhook Should be able to convert from CR v1 to CR v2
  [sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
  [sig-api-machinery] CustomResourceDefinition Watch CustomResourceDefinition Watch watch on custom resource definition objects
  [sig-api-machinery] CustomResourcePublishOpenAPI removes definition from spec when one versin gets changed to not be served
  [sig-api-machinery] CustomResourcePublishOpenAPI updates the published spec when one versin gets renamed
  [sig-api-machinery] CustomResourcePublishOpenAPI works for CRD preserving unknown fields at the schema root
  [sig-api-machinery] CustomResourcePublishOpenAPI works for CRD preserving unknown fields in an embedded object
  [sig-api-machinery] CustomResourcePublishOpenAPI works for CRD without validation schema
  [sig-api-machinery] CustomResourcePublishOpenAPI works for CRD with validation schema
  [sig-api-machinery] CustomResourcePublishOpenAPI works for multiple CRDs of different groups
  [sig-api-machinery] CustomResourcePublishOpenAPI works for multiple CRDs of same group and version but different kinds
  [sig-api-machinery] CustomResourcePublishOpenAPI works for multiple CRDs of same group but different versions
  [sig-api-machinery] Discovery Custom resource should have storage version hash
  [sig-api-machinery] Garbage collector should delete jobs and pods created by cronjob
  [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
  [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
  [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
  [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  [sig-api-machinery] Garbage collector should orphan pods created by rc if deleteOptions.OrphanDependents is nil
  [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
  [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  [sig-api-machinery] Garbage collector should support cascading deletion of custom resources
  [sig-api-machinery] Garbage collector should support orphan deletion of custom resources
  [sig-api-machinery] Generated clientset should create pods, set the deletionTimestamp and deletionGracePeriodSeconds of the pod
  [sig-api-machinery] Generated clientset should create v1beta1 cronJobs, delete cronJobs, watch cronJobs
  [sig-api-machinery] ResourceQuota Should be able to update and delete ResourceQuota.
  [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap.
  [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a custom resource.
  [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim. [sig-storage]
  [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a persistent volume claim with a storage class. [sig-storage]
  [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod.
  [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set.
  [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller.
  [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret.
  [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service.
  [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated.
  [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope.
  [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes.
  [sig-api-machinery] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
  [sig-api-machinery] Secrets should be consumable via the environment [NodeConformance] [Conformance]
  [sig-api-machinery] Secrets should fail to create secret due to empty secret key [Conformance]
  [sig-api-machinery] Servers with support for API chunking should return chunks of results for list calls
  [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata
  [sig-api-machinery] Servers with support for Table transformation should return chunks of table results for list calls
  [sig-api-machinery] Servers with support for Table transformation should return generic metadata details across all namespaces for nodes
  [sig-api-machinery] Servers with support for Table transformation should return pod details
  [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
  [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
  [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
  [sig-apps] CronJob should delete successful finished jobs with limit of one successful job
  [sig-apps] CronJob should not emit unexpected warnings
  [sig-apps] CronJob should remove from active list jobs that have been deleted
  [sig-apps] CronJob should replace jobs when ReplaceConcurrent
  [sig-apps] CronJob should schedule multiple jobs concurrently
  [sig-apps] Deployment deployment reaping should cascade to its replica sets and pods
  [sig-apps] Deployment deployment should delete old replica sets [Conformance]
  [sig-apps] Deployment deployment should support proportional scaling [Conformance]
  [sig-apps] Deployment deployment should support rollover [Conformance]
  [sig-apps] Deployment iterative rollouts should eventually progress
  [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
  [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  [sig-apps] Deployment test Deployment ReplicaSet orphaning and adoption regarding controllerRef
  [sig-apps] DisruptionController evictions: enough pods, absolute => should allow an eviction
  [sig-apps] DisruptionController evictions: enough pods, replicaSet, percentage => should allow an eviction
  [sig-apps] DisruptionController evictions: maxUnavailable allow single eviction, percentage => should allow an eviction
  [sig-apps] DisruptionController evictions: maxUnavailable deny evictions, integer => should not allow an eviction
  [sig-apps] DisruptionController evictions: no PDB => should allow an eviction
  [sig-apps] DisruptionController evictions: too few pods, absolute => should not allow an eviction
  [sig-apps] DisruptionController evictions: too few pods, replicaSet, percentage => should not allow an eviction
  [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it
  [sig-apps] DisruptionController should create a PodDisruptionBudget
  [sig-apps] DisruptionController should update PodDisruptionBudget status
  [sig-apps] Job should adopt matching orphans and release non-matching pods
  [sig-apps] Job should delete a job [Conformance]
  [sig-apps] Job should exceed backoffLimit
  [sig-apps] Job should fail when exceeds active deadline
  [sig-apps] Job should remove pods when job is deleted
  [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted
  [sig-apps] Job should run a job to completion when tasks sometimes fail and are not locally restarted
  [sig-apps] Job should run a job to completion when tasks succeed
  [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
  [sig-apps] ReplicaSet should serve a basic image on each replica with a private image
  [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
  [sig-apps] ReplicaSet should surface a failure condition on a common issue like exceeded quota
  [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
  [sig-apps] ReplicationController should release no longer matching pods [Conformance]
  [sig-apps] ReplicationController should serve a basic image on each replica with a private image
  [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
  [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
  [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should adopt matching orphans and release non-matching pods
  [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource
  [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should implement legacy replacement when the update strategy is OnDelete
  [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should not deadlock when a pod's predecessor fails
  [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
  [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
  [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications with PVCs
  [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] should provide basic identity
  [sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
  [sig-auth] Certificates API should support building a client with a CSR
  [sig-auth] Metadata Concealment should run a check-metadata-concealment job to completion
  [sig-auth] PodSecurityPolicy should allow pods under the privileged policy.PodSecurityPolicy
  [sig-auth] PodSecurityPolicy should enforce the restricted policy.PodSecurityPolicy
  [sig-auth] PodSecurityPolicy should forbid pod creation when no PSP is available
  [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
  [sig-auth] ServiceAccounts should ensure a single API token exists
  [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
  [sig-autoscaling] DNS horizontal autoscaling [DisabledForLargeClusters] kube-dns-autoscaler should scale kube-dns pods in both nonfaulty and faulty scenarios
  [sig-autoscaling] [HPA] Horizontal pod autoscaling (scale resource: CPU) [sig-autoscaling] ReplicationController light Should scale from 1 pod to 2 pods
  [sig-cli] Kubectl alpha client [k8s.io] Kubectl run CronJob should create a CronJob
  [sig-cli] Kubectl client [k8s.io] Guestbook application should create and stop a working application  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl api-versions should check if v1 is in available api versions  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl apply apply set/view last-applied
  [sig-cli] Kubectl client [k8s.io] Kubectl apply should apply a new configuration to an existing RC
  [sig-cli] Kubectl client [k8s.io] Kubectl apply should reuse port when apply to an existing SVC
  [sig-cli] Kubectl client [k8s.io] Kubectl client-side validation should create/apply a CR with unknown fields for CRD with no validation schema
  [sig-cli] Kubectl client [k8s.io] Kubectl client-side validation should create/apply a valid CR for CRD with validation schema
  [sig-cli] Kubectl client [k8s.io] Kubectl client-side validation should create/apply a valid CR with arbitrary-extra properties for CRD with partially-specified validation schema
  [sig-cli] Kubectl client [k8s.io] Kubectl cluster-info dump should check if cluster-info dump succeeds
  [sig-cli] Kubectl client [k8s.io] Kubectl cluster-info should check if Kubernetes master services is included in cluster-info  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl copy should copy a file from a running Pod
  [sig-cli] Kubectl client [k8s.io] Kubectl create quota should create a quota without scopes
  [sig-cli] Kubectl client [k8s.io] Kubectl create quota should create a quota with scopes
  [sig-cli] Kubectl client [k8s.io] Kubectl create quota should reject quota with invalid scopes
  [sig-cli] Kubectl client [k8s.io] Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl expose should create services for rc  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl get componentstatuses should get componentstatuses
  [sig-cli] Kubectl client [k8s.io] Kubectl label should update the label on a resource  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl logs should be able to retrieve and filter logs  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl patch should add annotations for pods in rc  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl replace should update a single-container pod's image  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl rolling-update should support rolling-update to same image  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl run 
  [sig-cli] Kubectl client [k8s.io] Kubectl run CronJob should create a CronJob
  [sig-cli] Kubectl client [k8s.io] Kubectl run default should create an rc or deployment from an image  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl run deployment should create a deployment from an image  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl run job should create a job from an image when restart is OnFailure  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl run rc should create an rc from an image  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Kubectl version should check is all data is printed  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Proxy server should support 
  [sig-cli] Kubectl client [k8s.io] Proxy server should support proxy with 
  [sig-cli] Kubectl client [k8s.io] Simple pod should contain last line of the log
  [sig-cli] Kubectl client [k8s.io] Simple pod should handle in-cluster config
  [sig-cli] Kubectl client [k8s.io] Simple pod should return command exit codes
  [sig-cli] Kubectl client [k8s.io] Simple pod should support exec
  [sig-cli] Kubectl client [k8s.io] Simple pod should support exec through an HTTP proxy
  [sig-cli] Kubectl client [k8s.io] Simple pod should support exec through kubectl proxy
  [sig-cli] Kubectl client [k8s.io] Simple pod should support exec using resource/name
  [sig-cli] Kubectl client [k8s.io] Simple pod should support inline execution and attach
  [sig-cli] Kubectl client [k8s.io] Simple pod should support port-forward
  [sig-cli] Kubectl client [k8s.io] Update Demo should create and stop a replication controller  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Update Demo should do a rolling update of a replication controller  [Conformance]
  [sig-cli] Kubectl client [k8s.io] Update Demo should scale a replication controller  [Conformance]
  [sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects a client request should support a client that connects, sends DATA, and disconnects
  [sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects a client request should support a client that connects, sends NO DATA, and disconnects
  [sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 [k8s.io] that expects NO client request should support a client that connects, sends DATA, and disconnects
  [sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on 0.0.0.0 should support forwarding over websockets
  [sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects a client request should support a client that connects, sends DATA, and disconnects
  [sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects a client request should support a client that connects, sends NO DATA, and disconnects
  [sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost [k8s.io] that expects NO client request should support a client that connects, sends DATA, and disconnects
  [sig-cli] Kubectl Port forwarding [k8s.io] With a server listening on localhost should support forwarding over websockets
  [sig-instrumentation] Cadvisor should be healthy on every node.
  [sig-instrumentation] MetricsGrabber should grab all metrics from a ControllerManager.
  [sig-instrumentation] MetricsGrabber should grab all metrics from a Kubelet.
  [sig-instrumentation] MetricsGrabber should grab all metrics from API server.
  [sig-instrumentation] MetricsGrabber should grab all metrics from a Scheduler.
  [sig-network] DNS should provide DNS for ExternalName services [Conformance]
  [sig-network] DNS should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
  [sig-network] DNS should provide DNS for services  [Conformance]
  [sig-network] DNS should provide DNS for the cluster  [Conformance]
  [sig-network] DNS should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly]
  [sig-network] DNS should resolve DNS of partial qualified names for the cluster [LinuxOnly]
  [sig-network] DNS should support configurable pod DNS nameservers
  [sig-network] DNS should support configurable pod resolv.conf
  [sig-network] Firewall rule should have correct firewall rules for e2e cluster
  [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  [sig-network] Networking should check kube-proxy urls
  [sig-network] Networking should provide unchanging, static URL paths for kubernetes api services
  [sig-network] Network should set TCP CLOSE_WAIT timeout
  [sig-network] Proxy version v1 should proxy logs on node using proxy subresource  [Conformance]
  [sig-network] Proxy version v1 should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
  [sig-network] Service endpoints latency should not be very high  [Conformance]
  [sig-network] Services should be able to change the type from ClusterIP to ExternalName
  [sig-network] Services should be able to change the type from ExternalName to ClusterIP
  [sig-network] Services should be able to change the type from ExternalName to NodePort
  [sig-network] Services should be able to change the type from NodePort to ExternalName
  [sig-network] Services should be able to create a functioning NodePort service
  [sig-network] Services should be able to switch session affinity for NodePort service
  [sig-network] Services should be able to switch session affinity for service with type clusterIP
  [sig-network] Services should be able to up and down services
  [sig-network] Services should be able to update NodePorts with two same port numbers but different protocols
  [sig-network] Services should be rejected when no endpoints exist
  [sig-network] Services should check NodePort out-of-range
  [sig-network] Services should create endpoints for unready pods
  [sig-network] Services should have session affinity work for NodePort service
  [sig-network] Services should have session affinity work for service with type clusterIP
  [sig-network] Services should implement service.kubernetes.io/service-proxy-name
  [sig-network] Services should preserve source pod IP for traffic thru service cluster IP
  [sig-network] Services should prevent NodePort collisions
  [sig-network] Services should provide secure master service  [Conformance]
  [sig-network] Services should release NodePorts on delete
  [sig-network] Services should serve a basic endpoint from pods  [Conformance]
  [sig-network] Services should serve multiport endpoints from pods  [Conformance]
  [sig-network] Services should use same NodePort with same port but different protocols
  [sig-network] [sig-windows] Networking Granular Checks: Pods should function for intra-pod communication: http
  [sig-network] [sig-windows] Networking Granular Checks: Pods should function for intra-pod communication: udp
  [sig-network] [sig-windows] Networking Granular Checks: Pods should function for node-pod communication: http
  [sig-network] [sig-windows] Networking Granular Checks: Pods should function for node-pod communication: udp
  [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
  [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
  [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
  [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
  [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
  [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass
  [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass
  [sig-node] RuntimeClass should reject a Pod requesting a RuntimeClass with an unconfigured handler
  [sig-node] RuntimeClass should run a Pod requesting a RuntimeClass with a configured handler [NodeFeature:RuntimeHandler]
  [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied.
  [sig-scheduling] Multi-AZ Clusters should spread the pods of a replication controller across zones
  [sig-scheduling] Multi-AZ Clusters should spread the pods of a service across zones
  [sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should only be allowed to provision PDs in zones where nodes exist
  [sig-scheduling] Multi-AZ Cluster Volumes [sig-storage] should schedule pods in the same zones as statically provisioned PVs
  [sig-scheduling] PreemptionExecutionPath runs ReplicaSets to verify preemption running path
  [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
  [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
  [sig-storage] ConfigMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] ConfigMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeFeature:FSGroup]
  [sig-storage] ConfigMap should be consumable from pods in volume as non-root with FSGroup [NodeFeature:FSGroup]
  [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
  [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root with FSGroup [NodeFeature:FSGroup]
  [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
  [sig-storage] CSI mock volume CSI attach test using mock driver should not require VolumeAttach for drivers without attachment
  [sig-storage] CSI mock volume CSI attach test using mock driver should preserve attachment policy when no CSIDriver present
  [sig-storage] CSI mock volume CSI attach test using mock driver should require VolumeAttach for drivers with attachment
  [sig-storage] CSI mock volume CSI workload information using mock driver should be passed when podInfoOnMount=true
  [sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when CSIDriver does not exist
  [sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when podInfoOnMount=false
  [sig-storage] CSI mock volume CSI workload information using mock driver should not be passed when podInfoOnMount=nil
  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (block volmode)] volumes should store data
  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options
  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory
  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path
  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] CSI Volumes [Driver: csi-hostpath] [Testpattern: Dynamic PV (default fs)] volumes should store data
  [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (block volmode)] volumes should store data
  [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options
  [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory
  [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path
  [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] CSI Volumes [Driver: csi-hostpath-v0] [Testpattern: Dynamic PV (default fs)] volumes should store data
  [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
  [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
  [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
  [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
  [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  [sig-storage] Downward API volume should provide podname as non-root with fsgroup and defaultMode [NodeFeature:FSGroup]
  [sig-storage] Downward API volume should provide podname as non-root with fsgroup [NodeFeature:FSGroup]
  [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
  [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
  [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
  [sig-storage] Dynamic Provisioning DynamicProvisioner allowedTopologies should create persistent volume in the zone specified in allowedTopologies of storageclass
  [sig-storage] Dynamic Provisioning Invalid AWS KMS key should report an error and create no PV
  [sig-storage] Dynamic Provisioning [k8s.io] GlusterDynamicProvisioner should create and delete persistent volumes [fast]
  [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
  [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] files with FSGroup ownership should support (root,0644,tmpfs)
  [sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is non-root
  [sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] new files should be created with FSGroup ownership when container is root
  [sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] nonexistent volume subPath should have the correct mode and owner using FSGroup
  [sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] volume on default medium should have the correct mode using FSGroup
  [sig-storage] EmptyDir volumes when FSGroup is specified [NodeFeature:FSGroup] volume on tmpfs should have the correct mode using FSGroup
  [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
  [sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : configmap
  [sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : projected
  [sig-storage] Ephemeralstorage When pod refers to non-existent ephemeral storage should allow deletion of pod with invalid volume : secret
  [sig-storage] Flexvolumes should be mountable when attachable
  [sig-storage] Flexvolumes should be mountable when non-attachable
  [sig-storage] GCP Volumes GlusterFS should be mountable
  [sig-storage] GCP Volumes NFSv3 should be mountable for NFSv3
  [sig-storage] GCP Volumes NFSv4 should be mountable for NFSv4
  [sig-storage] HostPath should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] HostPath should support r/w [NodeConformance]
  [sig-storage] HostPath should support subPath [NodeConformance]
  [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: emptydir] [Testpattern: Inline-volume (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (block volmode)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext3)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext3)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext4)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Dynamic PV (ext4)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext3)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext3)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext4)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Inline-volume (ext4)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: gcepd] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Inline-volume (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: gluster] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: hostPathSymlink] [Testpattern: Inline-volume (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: hostPath] [Testpattern: Inline-volume (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: blockfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext3)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext3)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext4)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: block] [Testpattern: Pre-provisioned PV (ext4)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link-bindmounted] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir-link] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: dir] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: local][LocalVolumeType: tmpfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (block volmode)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] provisioning should provision storage with mount options
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Dynamic PV (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Inline-volume (default fs)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (block volmode)] volumes should store data
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should be able to unmount after the subpath directory is deleted
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directories when readOnly specified in the volumeSource
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing directory
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support existing single file [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support file as subpath [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support non-existent path
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly directory specified in the volumeMount
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] subPath should support readOnly file specified in the volumeMount [LinuxOnly]
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should allow exec of files on the volume
  [sig-storage] In-tree Volumes [Driver: nfs] [Testpattern: Pre-provisioned PV (default fs)] volumes should store data
  [sig-storage] Mounted volume expand Should verify mounted devices can be resized
  [sig-storage] PersistentVolumes GCEPD should test that deleting a PVC before the pod does not cause pod deletion to fail on PD detach
  [sig-storage] PersistentVolumes GCEPD should test that deleting the Namespace of a PVC and Pod causes the successful detach of Persistent Disk
  [sig-storage] PersistentVolumes GCEPD should test that deleting the PV before the pod does not cause pod deletion to fail on PD detach
  [sig-storage] PersistentVolumes-local  Pod with node different from PV's NodeAffinity should fail scheduling due to different NodeAffinity
  [sig-storage] PersistentVolumes-local  Pod with node different from PV's NodeAffinity should fail scheduling due to different NodeSelector
  [sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
  [sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: blockfswithformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and read from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] One pod requesting one prebound PVC should be able to mount volume and write from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
  [sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: blockfswithoutformat] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and read from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: block] One pod requesting one prebound PVC should be able to mount volume and write from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: block] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
  [sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: block] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
  [sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: dir-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and read from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] One pod requesting one prebound PVC should be able to mount volume and write from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
  [sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: dir-link-bindmounted] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and read from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: dir-link] One pod requesting one prebound PVC should be able to mount volume and write from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: dir-link] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
  [sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: dir-link] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and read from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: dir] One pod requesting one prebound PVC should be able to mount volume and write from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: dir] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
  [sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: dir] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and read from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: tmpfs] One pod requesting one prebound PVC should be able to mount volume and write from pod1
  [sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Set fsGroup for local volume should set different fsGroup for second pod if first pod is deleted
  [sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume at the same time should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes-local  [Volume type: tmpfs] Two pods mounting a local volume one after the other should be able to write from pod1 and read from pod2
  [sig-storage] PersistentVolumes NFS when invoking the Recycle reclaim policy should test that a PV becomes Available and is clean after the PVC is deleted.
  [sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 2 PVs and 4 PVCs: test write access
  [sig-storage] PersistentVolumes NFS with multiple PVs and PVCs all in same ns should create 3 PVs and 3 PVCs: test write access
  [sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PV and a pre-bound PVC: test write access
  [sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and a pre-bound PV: test write access
  [sig-storage] PersistentVolumes NFS with Single PV - PVC pairs create a PVC and non-pre-bound PV: test write access
  [sig-storage] PersistentVolumes NFS with Single PV - PVC pairs should create a non-pre-bound PV and PVC: test write access 
  [sig-storage] PersistentVolumes:vsphere should test that deleting a PVC before the pod does not cause pod deletion to fail on vsphere volume detach
  [sig-storage] PersistentVolumes:vsphere should test that deleting the Namespace of a PVC and Pod causes the successful detach of vsphere volume
  [sig-storage] PersistentVolumes:vsphere should test that deleting the PV before the pod does not cause pod deletion to fail on vspehre volume detach
  [sig-storage] Pod Disks should be able to delete a non-existent PD without error
  [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
  [sig-storage] Projected configMap should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Projected configMap should be consumable from pods in volume as non-root with defaultMode and fsGroup set [NodeFeature:FSGroup]
  [sig-storage] Projected configMap should be consumable from pods in volume as non-root with FSGroup [NodeFeature:FSGroup]
  [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
  [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root with FSGroup [NodeFeature:FSGroup]
  [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
  [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
  [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
  [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
  [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
  [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  [sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup and defaultMode [NodeFeature:FSGroup]
  [sig-storage] Projected downwardAPI should provide podname as non-root with fsgroup [NodeFeature:FSGroup]
  [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
  [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
  [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
  [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
  [sig-storage] Projected secret should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance]
  [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
  [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  [sig-storage] PVC Protection Verify "immediate" deletion of a PVC that is not in active use by a pod
  [sig-storage] PVC Protection Verify that PVC in active use by a pod is not removed immediately
  [sig-storage] PVC Protection Verify that scheduling of a pod that uses PVC that is being deleted fails and the pod becomes Unschedulable
  [sig-storage] PV Protection Verify "immediate" deletion of a PV that is not bound to a PVC
  [sig-storage] PV Protection Verify that PV bound to a PVC is not removed immediately
  [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
  [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
  [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [LinuxOnly] [Conformance]
  [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [LinuxOnly] [Conformance]
  [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [LinuxOnly] [Conformance]
  [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [LinuxOnly] [Conformance]
  [sig-storage] Volume expand should allow expansion of block volumes
  [sig-storage] Volume expand should not allow expansion of pvcs without AllowVolumeExpansion property
  [sig-storage] Volume expand should resize volume when PVC is edited while pod is using it
  [sig-storage] Volume expand Verify if offline PVC expansion works
  [sig-storage] Volume limits should verify that all nodes have volume limits
  [sig-storage] Volume Placement should create and delete pod with multiple volumes from different datastore
  [sig-storage] Volume Placement should create and delete pod with multiple volumes from same datastore
  [sig-storage] Volume Placement should create and delete pod with the same volume source attach/detach to different worker nodes
  [sig-storage] Volume Placement should create and delete pod with the same volume source on the same worker node
  [sig-storage] Volume Placement test back to back pod creation and deletion with different volume sources on the same worker node
  [sig-storage] Volumes ConfigMap should be mountable
  [sig-storage] vsphere statefulset vsphere statefulset testing
  [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on allowed zones specified in storage class 
  [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on a VSAN capability, datastore and compatible zone specified in storage class
  [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on multiple zones specified in storage class 
  [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on multiple zones specified in the storage class. (No shared datastores exist among both zones)
  [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on the allowed zones and datastore specified in storage class
  [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on the allowed zones and storage policy specified in storage class
  [sig-storage] Zone Support Verify a pod is created and attached to a dynamically created PV, based on the allowed zones, datastore and storage policy specified in storage class
  [sig-storage] Zone Support Verify a pod is created on a non-Workspace zone and attached to a dynamically created PV, based on the allowed zones and storage policy specified in storage class
  [sig-storage] Zone Support Verify dynamically created pv with allowed zones specified in storage class, shows the right zone information on its labels
  [sig-storage] Zone Support Verify dynamically created pv with multiple zones specified in the storage class, shows both the zones on its labels
  [sig-storage] Zone Support Verify PVC creation fails if no zones are specified in the storage class (No shared datastores exist among all the nodes)
  [sig-storage] Zone Support Verify PVC creation fails if only datastore is specified in the storage class (No shared datastores exist among all the nodes)
  [sig-storage] Zone Support Verify PVC creation fails if only storage policy is specified in the storage class (No shared datastores exist among all the nodes)
  [sig-storage] Zone Support Verify PVC creation fails if the availability zone specified in the storage class have no shared datastores under it.
  [sig-storage] Zone Support Verify PVC creation with an invalid VSAN capability along with a compatible zone combination specified in storage class fails
  [sig-storage] Zone Support Verify PVC creation with compatible policy and datastore without any zones specified in the storage class fails (No shared datastores exist among all the nodes)
  [sig-storage] Zone Support Verify PVC creation with incompatible datastore and zone combination specified in storage class fails
  [sig-storage] Zone Support Verify PVC creation with incompatible storage policy along with compatible zone and datastore combination specified in storage class fails
  [sig-storage] Zone Support Verify PVC creation with incompatible storagePolicy and zone combination specified in storage class fails
  [sig-storage] Zone Support Verify PVC creation with incompatible zone along with compatible storagePolicy and datastore combination specified in storage class fails
  [sig-storage] Zone Support Verify PVC creation with invalid zone specified in storage class fails
(784 rows)

#+end_SRC



#+BEGIN_SRC sql-mode
SELECT
   COLUMN_NAME
FROM
   information_schema.COLUMNS
WHERE
   TABLE_NAME = 'api_operations';
#+END_SRC

#+RESULTS:
#+begin_src sql-mode
 column_name 
-------------
 id
 method
 path
 regexp
 group
 version
 kind
 category
 description
(9 rows)

#+end_src


** columns
*** id
#+NAME: id
#+BEGIN_SRC sql-mode
  id text NOT NULL,
#+END_SRC
*** method
#+NAME: method
#+BEGIN_SRC sql-mode
  method text NOT NULL,
#+END_SRC
*** path
#+NAME: path
#+BEGIN_SRC sql-mode
  path text NOT NULL,
#+END_SRC
*** regexp
#+NAME: regexp
#+BEGIN_SRC sql-mode
  regexp text NOT NULL,
#+END_SRC
*** group
#+NAME: group
#+BEGIN_SRC sql-mode
  "group" text NOT NULL,
#+END_SRC
*** version
#+NAME: version
#+BEGIN_SRC sql-mode
  version text NOT NULL,
#+END_SRC
*** kind
#+NAME: kind
#+BEGIN_SRC sql-mode
  kind text NOT NULL,
#+END_SRC
*** category
#+NAME: category
#+BEGIN_SRC sql-mode
  "category" text NOT NULL,
#+END_SRC
*** description
#+NAME: description
#+BEGIN_SRC sql-mode
  description text NOT NULL
#+END_SRC
*** tags
I don't think we use tags yet, and it's a list.
Leaving for now.
#+NAME: tags
#+BEGIN_SRC sql-mode
  tags text NOT NULL,
#+END_SRC

** table
#+NAME: CREATE TABLE audit_events
#+BEGIN_SRC sql-mode :noweb yes :tangle ../hasura/migrations/20_table_api_operations.up.sql
  CREATE TABLE public.api_operations (
    <<id>>
    <<method>>
    <<path>>
    <<regexp>>
    <<group>>
    <<version>>
    <<kind>>
    <<category>>
    <<description>>
  );
#+END_SRC

#+RESULTS: CREATE TABLE audit_events
#+begin_src sql-mode
CREATE TABLE
#+end_src

#+NAME: DROP TABLE audit_events
#+BEGIN_SRC sql-mode :noweb yes :tangle ../hasura/migrations/20_table_api_operations.down.sql
  DROP TABLE IF EXISTS api_operations
#+END_SRC

#+NAME: track_table audit_events
#+BEGIN_SRC sql-mode :noweb yes :tangle ../hasura/migrations/21_track_track_api_operations.up.yaml
- type: track_table
  args:
    schema: public
    name: api_operations
#+END_SRC

#+NAME: untrack_table audit_events
#+BEGIN_SRC sql-mode :noweb yes :tangle ../hasura/migrations/21_track_track_api_operations.down.yaml
- type: untrack_table
  args:
    schema: public
    name: api_operations
#+END_SRC

* Paths
:PROPERTIES:
:header-args:python+: :tangle yes
:END:
 
** keys

#+BEGIN_SRC shell :dir ~/go/src/k8s.io/kubernetes/api/openapi-spec :wrap "SRC json"
  cat swagger.json | jq '.paths' | jq -r 'keys[]' \
  | sort -R | head -20
  # | grep -vi alpha\\\|beta \
#+end_src

#+results:
#+begin_SRC json
/api/v1/namespaces/{namespace}/replicationcontrollers/{name}/status
/api/v1/watch/namespaces/{namespace}/serviceaccounts/{name}
/apis/networking.k8s.io/v1beta1/namespaces/{namespace}/ingresses/{name}/status
/apis/coordination.k8s.io/v1/leases
/api/v1/namespaces/{namespace}/limitranges
/apis/scheduling.k8s.io/v1alpha1/priorityclasses
/apis/apps/v1beta2/watch/statefulsets
/apis/apps/v1beta2/namespaces/{namespace}/statefulsets
/apis/certificates.k8s.io/v1beta1/watch/certificatesigningrequests/{name}
/apis/apiextensions.k8s.io/v1beta1/watch/customresourcedefinitions/{name}
/apis/certificates.k8s.io/v1beta1/certificatesigningrequests/{name}/approval
/apis/apps/v1beta2/namespaces/{namespace}/daemonsets/{name}
/apis/rbac.authorization.k8s.io/v1beta1/watch/rolebindings
/apis/settings.k8s.io/v1alpha1/namespaces/{namespace}/podpresets/{name}
/api/v1/watch/namespaces/{namespace}/persistentvolumeclaims
/apis/batch/v1beta1/namespaces/{namespace}/cronjobs/{name}/status
/apis/batch/v2alpha1/watch/cronjobs
/apis/rbac.authorization.k8s.io/v1beta1/namespaces/{namespace}/rolebindings
/apis/networking.k8s.io/v1/watch/namespaces/{namespace}/networkpolicies
/apis/apps/v1/namespaces/{namespace}/controllerrevisions/{name}
#+end_SRC

** what we currently use
*** path_regex

#+NAME: compile_path_regex
#+BEGIN_SRC python :eval never
  import re
  import json

  # k8s appears to allow/expect a trailing {path} variable to capture everything
  # remaining in the path, including '/' characters, which doesn't appear to be
  # allowed according to the openapi 2.0 or 3.0 specs
  # (ref: https://github.com/OAI/OpenAPI-Specification/issues/892)
  K8S_PATH_VARIABLE_PATTERN = re.compile("{(path)}$")
  VARIABLE_PATTERN = re.compile("{([^}]+)}")
  def regex_from_path(path):
      # first replace the special trailing {path} wildcard with a named regex
      path_regex = K8S_PATH_VARIABLE_PATTERN.sub("(?P<\\1>.+)", path).rstrip('/')
      # replace wildcards in {varname} format to a named regex
      # path_regex = VARIABLE_PATTERN.sub("(?P<\\1>[^/]+)", path_regex).rstrip('/')

      # now that we are using POSIX, we can't do {varname}
      path_regex = VARIABLE_PATTERN.sub("([^/]+)", path_regex).rstrip('/')

      # TODO(spiffxp): unsure if trailing / _should_ be counted toward /proxy
      if path_regex.endswith("proxy"): # allow proxy to catch a trailing /
          path_regex += "/?$"
      else:
          path_regex += "$"
      # print('Converted path: %s into path_regex: %s' % (path, path_regex))
      return path_regex
#+END_SRC

*** paths
*** level

#+NAME: parse_level_from_path
#+BEGIN_SRC python :eval nevel
LEVEL_PATTERN = re.compile("/v(?P<api_version>[0-9]+)(?:(?P<api_level>alpha|beta)(?P<api_level_version>[0-9]+))?")
def level_from_path(path):
    # get the level (alpha/beta/stable) and the version from the path
    level = None
    match = LEVEL_PATTERN.search(path)
    if match:
        level = match.groupdict().get("api_level")
    if level is None:
        level = "stable"
    return level
#+END_SRC

*** action / methods
Under each path, each key is the method / action.
** load_openapi_spec

#+NAME: load_openapi_spec
#+BEGIN_SRC python :eval never
  try:
      from urllib.parse import urlparse
  except Exception as e:
      from urlparse import urlparse

  def load_openapi_spec(url):
      if urlparse(url).scheme in ['http', 'https']:
          swagger = requests.get(url).json()
      else: # treat as file on disk
          with open(url, "rb") as f:
              swagger = json.load(f)

      openapi_spec = {}
      openapi_spec['operations'] = {}
      openapi_spec['parameters'] = {}
      openapi_spec['operation_list'] = []
      openapi_spec['parameter_list'] = []

      for path in swagger['paths']:
          path_regex = regex_from_path(path)
          for method, swagger_method in swagger['paths'][path].items():
              if method == "parameters":
                  # List seems beter than dict since we are exporting to SQL
                  for param in swagger_method:
                      param["path"]=path
                      openapi_spec['parameter_list'].append(param)
                  params={}
                  for param in swagger_method:
                      param_name = param['name']
                      del param['name']
                      params[param_name] = param
                  openapi_spec['parameters'][path]=params
                  continue
              if 'deprecated' in swagger_method.get('description', '').lower():
                  # print('Skipping deprecated endpoint %s %s' % (method, path))
                  continue
              if 'consumes' in swagger_method:
                  # usually : ['application/json', 'application/yaml', 'application/vnd.kubernetes.protobuf']
                  # or : ['*/*']
                  # sometimes: ['application/json-patch+json', 'application/merge-patch+json',
                  # 'application/strategic-merge-patch+json']
                  pass
              if 'produces' in swagger_method:
                  # usually : ['application/json', 'application/yaml', 'application/vnd.kubernetes.protobuf']
                  # can additonally include:
                  #   'application/json;stream=watch', 'application/vnd.kubernetes.protobuf;stream=watch']
                  # is seldom: '*/*'
                  # ['text/plain', 
                  pass
              produces = swagger_method.get('produces', [])
              op_data = {}
              tags = sorted(swagger['paths'][path][method].get('tags', list()))
              if len(tags) > 0:
                  op_data['tags'] = tags
                  tag = tags[0]
                  # just use one tag for category
                  category = tag.split("_")[0]
                  op_data['category'] = category
              else:
                  op_data['category'] = ''
              op_id = swagger_method.get('operationId', '')
              op_data['description'] = swagger_method.get('description', '')
              group_version_kind = swagger_method.get('x-kubernetes-group-version-kind', {})
              op_data['group'] = group_version_kind.get('group', '')
              op_data['version'] = group_version_kind.get('version', '')
              op_data['kind'] = group_version_kind.get('kind', '')
              op_data['path'] = path
              op_data['path_regex'] = path_regex
              op_data['level'] = level_from_path(path)
              op_data['method'] = method
              openapi_spec['operations'][op_id]=op_data
              op_data['id'] = op_id
              openapi_spec['operation_list'].append(op_data)
              # openapi_spec['operations'][op_id] = op_data
      return openapi_spec
#+END_SRC

** library_opts
#+BEGIN_SRC python :eval never
from typing import Iterator, Dict, Any, Optional
def clean_csv_value(value: Optional[Any]) -> str:
    if value is None:
        return r'\N'
    return str(value).replace('\n', '\\n')

import io

class StringIteratorIO(io.TextIOBase):

    def __init__(self, iter: Iterator[str]):
        self._iter = iter
        self._buff = ''

    def readable(self) -> bool:
        return True

    def _read1(self, n: Optional[int] = None) -> str:
        while not self._buff:
            try:
                self._buff = next(self._iter)
            except StopIteration:
                break
        ret = self._buff[:n]
        self._buff = self._buff[len(ret):]
        return ret

    def read(self, n: Optional[int] = None) -> str:
        line = []
        if n is None or n < 0:
            while True:
                m = self._read1()
                if not m:
                    break
                line.append(m)
        else:
            while n > 0:
                m = self._read1(n)
                if not m:
                    break
                n -= len(m)
                line.append(m)
        return ''.join(line)
#+END_SRC

** save_openapi_operations

#+BEGIN_SRC python :eval never
  def recreate_api_operations_table(cursor):
      cursor.execute(open('./hasura/migrations/20_table_api_operations.down.sql').read())
      # cursor.execute("CREATE TABLE public.audit_events (event jsonb);")
      cursor.execute(open('./hasura/migrations/20_table_api_operations.up.sql').read())
  import ipdb
  def openapi_operation_iterator(connection,
                           api_operations: Iterator[Dict[str, Any]],
                           size: int = 8192) -> None:
      with connection.cursor() as cursor:
          recreate_api_operations_table(cursor)

          audit_events_string_iterator = StringIteratorIO((
              '|'.join(map(clean_csv_value, (
                  # ipdb.set_trace(context=10),
                  # op["id"],
                  op_id,
                  op["method"],
                  op["path"],
                  op["path_regex"],
                  op["group"],
                  op["version"],
                  op["kind"],
                  op["category"],
                  op["description"],
              ))) + '\n'
              for op_id,op in api_operations.items()
          ))

          cursor.copy_from(audit_events_string_iterator,
                           'api_operations',
                           sep='|', size=size)
#+END_SRC

** load specific file
#+BEGIN_SRC python :eval never
# load_openapi_spec("~/go/src/k8s.io/kubernetes/api/openapi-spec/swagger.json")
#+END_SRC
** actions

#+BEGIN_SRC python :tangle no
  for entry in logfile:
          raw_event = json.loads(entry)
          # map http verb to kubernetes action
          # TODO: request that audit logging record http verb?
          verb_tt = {
                  'get': ['get', 'list', 'proxy'],
                  'patch': ['patch'],
                  'put': ['update'],
                  'post': ['create'],
                  'delete': ['delete', 'deletecollection'],
                  'watch': ['watch', 'watchlist'],
          }
          for method, verbs in verb_tt.items():
                  if raw_event['verb'] in verbs:
                          raw_event['method'] = method
                          break
          if "method" not in raw_event:
                  print("Error parsing event - HTTP method map not defined at \"%s\" for verb \"%s\"" % (raw_event['requestURI'], raw_event['verb']))
#+END_SRC

# json["/api/v1/componentstatuses/{name}"].parameters[0].required = true;

#+NAME: list of required
#+begin_src shell :dir ~/go/src/k8s.io/kubernetes/api/openapi-spec :wrap "SRC json"
  cat ./api/openapi-spec/swagger.json \
      | jq .paths \
      | gron | grep required | gron --ungron \
      | head -100
      #| jq '.[].get.description' -r \
      #| sort -r | uniq | cat
#+END_SRC

#+RESULTS: list of required
#+begin_SRC json
#+end_SRC

#+NAME: list of kubernetes actions
#+begin_src shell :dir ~/go/src/k8s.io/kubernetes/api/openapi-spec :wrap "SRC json"
  cat swagger.json \
  | jq '.paths["/apis/apps/v1/"]'
  # | jq '.paths["/apis/rbac.authorization.k8s.io/v1/namespaces/{namespace}/rolebindings/{name}"] | keys'
#+END_SRC

#+RESULTS: list of kubernetes actions
#+begin_SRC json
{
  "get": {
    "consumes": [
      "application/json",
      "application/yaml",
      "application/vnd.kubernetes.protobuf"
    ],
    "description": "get available resources",
    "operationId": "getAppsV1APIResources",
    "produces": [
      "application/json",
      "application/yaml",
      "application/vnd.kubernetes.protobuf"
    ],
    "responses": {
      "200": {
        "description": "OK",
        "schema": {
          "$ref": "#/definitions/io.k8s.apimachinery.pkg.apis.meta.v1.APIResourceList"
        }
      },
      "401": {
        "description": "Unauthorized"
      }
    },
    "schemes": [
      "https"
    ],
    "tags": [
      "apps_v1"
    ]
  }
}
#+end_SRC

#+NAME: getAppsV1APIResources
#+begin_src shell :dir ~/go/src/k8s.io/kubernetes/api/openapi-spec :wrap "SRC json"
  cat swagger.json \
  | jq '.paths["/apis/apps/v1/"]'
  # | jq '.paths["/apis/rbac.authorization.k8s.io/v1/namespaces/{namespace}/rolebindings/{name}"] | keys'
#+END_SRC

Note that we could use any $ref to see what the response is... ie
When we want to test APIResourceList... here is one that returns it.

#+RESULTS: getAppsV1APIResources
#+begin_SRC json
{
  "get": {
    "consumes": [
      "application/json",
      "application/yaml",
      "application/vnd.kubernetes.protobuf"
    ],
    "description": "get available resources",
    "operationId": "getAppsV1APIResources",
    "produces": [
      "application/json",
      "application/yaml",
      "application/vnd.kubernetes.protobuf"
    ],
    "responses": {
      "200": {
        "description": "OK",
        "schema": {
          "$ref": "#/definitions/io.k8s.apimachinery.pkg.apis.meta.v1.APIResourceList"
        }
      },
      "401": {
        "description": "Unauthorized"
      }
    },
    "schemes": [
      "https"
    ],
    "tags": [
      "apps_v1"
    ]
  }
}
#+end_SRC


These map to kubernetes actions, which are NOT one to one with http verbs.
Our audit logs contain the http verbs, we'll need to map them.

This one has 5 keys, what is  the full set.
  

#+NAME: list of
#+begin_src shell :dir ~/go/src/k8s.io/kubernetes/api/openapi-spec :wrap "SRC json"
  cat swagger.json \
   | jq '.paths["/apis/rbac.authorization.k8s.io/v1/namespaces/{namespace}/rolebindings/{name}"]'
#+END_SRC

#+RESULTS: list of
#+begin_SRC json
{
  "delete": {
    "consumes": [
      "*/*"
    ],
    "description": "delete a RoleBinding",
    "operationId": "deleteRbacAuthorizationV1NamespacedRoleBinding",
    "parameters": [
      {
        "in": "body",
        "name": "body",
        "schema": {
          "$ref": "#/definitions/io.k8s.apimachinery.pkg.apis.meta.v1.DeleteOptions"
        }
      },
      {
        "description": "When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed",
        "in": "query",
        "name": "dryRun",
        "type": "string",
        "uniqueItems": true
      },
      {
        "description": "The duration in seconds before the object should be deleted. Value must be non-negative integer. The value zero indicates delete immediately. If this value is nil, the default grace period for the specified type will be used. Defaults to a per object value if not specified. zero means delete immediately.",
        "in": "query",
        "name": "gracePeriodSeconds",
        "type": "integer",
        "uniqueItems": true
      },
      {
        "description": "Deprecated: please use the PropagationPolicy, this field will be deprecated in 1.7. Should the dependent objects be orphaned. If true/false, the \"orphan\" finalizer will be added to/removed from the object's finalizers list. Either this field or PropagationPolicy may be set, but not both.",
        "in": "query",
        "name": "orphanDependents",
        "type": "boolean",
        "uniqueItems": true
      },
      {
        "description": "Whether and how garbage collection will be performed. Either this field or OrphanDependents may be set, but not both. The default policy is decided by the existing finalizer set in the metadata.finalizers and the resource-specific default policy. Acceptable values are: 'Orphan' - orphan the dependents; 'Background' - allow the garbage collector to delete the dependents in the background; 'Foreground' - a cascading policy that deletes all dependents in the foreground.",
        "in": "query",
        "name": "propagationPolicy",
        "type": "string",
        "uniqueItems": true
      }
    ],
    "produces": [
      "application/json",
      "application/yaml",
      "application/vnd.kubernetes.protobuf"
    ],
    "responses": {
      "200": {
        "description": "OK",
        "schema": {
          "$ref": "#/definitions/io.k8s.apimachinery.pkg.apis.meta.v1.Status"
        }
      },
      "202": {
        "description": "Accepted",
        "schema": {
          "$ref": "#/definitions/io.k8s.apimachinery.pkg.apis.meta.v1.Status"
        }
      },
      "401": {
        "description": "Unauthorized"
      }
    },
    "schemes": [
      "https"
    ],
    "tags": [
      "rbacAuthorization_v1"
    ],
    "x-kubernetes-action": "delete",
    "x-kubernetes-group-version-kind": {
      "group": "rbac.authorization.k8s.io",
      "kind": "RoleBinding",
      "version": "v1"
    }
  },
  "get": {
    "consumes": [
      "*/*"
    ],
    "description": "read the specified RoleBinding",
    "operationId": "readRbacAuthorizationV1NamespacedRoleBinding",
    "produces": [
      "application/json",
      "application/yaml",
      "application/vnd.kubernetes.protobuf"
    ],
    "responses": {
      "200": {
        "description": "OK",
        "schema": {
          "$ref": "#/definitions/io.k8s.api.rbac.v1.RoleBinding"
        }
      },
      "401": {
        "description": "Unauthorized"
      }
    },
    "schemes": [
      "https"
    ],
    "tags": [
      "rbacAuthorization_v1"
    ],
    "x-kubernetes-action": "get",
    "x-kubernetes-group-version-kind": {
      "group": "rbac.authorization.k8s.io",
      "kind": "RoleBinding",
      "version": "v1"
    }
  },
  "parameters": [
    {
      "description": "name of the RoleBinding",
      "in": "path",
      "name": "name",
      "required": true,
      "type": "string",
      "uniqueItems": true
    },
    {
      "description": "object name and auth scope, such as for teams and projects",
      "in": "path",
      "name": "namespace",
      "required": true,
      "type": "string",
      "uniqueItems": true
    },
    {
      "description": "If 'true', then the output is pretty printed.",
      "in": "query",
      "name": "pretty",
      "type": "string",
      "uniqueItems": true
    }
  ],
  "patch": {
    "consumes": [
      "application/json-patch+json",
      "application/merge-patch+json",
      "application/strategic-merge-patch+json"
    ],
    "description": "partially update the specified RoleBinding",
    "operationId": "patchRbacAuthorizationV1NamespacedRoleBinding",
    "parameters": [
      {
        "in": "body",
        "name": "body",
        "required": true,
        "schema": {
          "$ref": "#/definitions/io.k8s.apimachinery.pkg.apis.meta.v1.Patch"
        }
      },
      {
        "description": "When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed",
        "in": "query",
        "name": "dryRun",
        "type": "string",
        "uniqueItems": true
      },
      {
        "description": "fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint. This field is required for apply requests (application/apply-patch) but optional for non-apply patch types (JsonPatch, MergePatch, StrategicMergePatch).",
        "in": "query",
        "name": "fieldManager",
        "type": "string",
        "uniqueItems": true
      },
      {
        "description": "Force is going to \"force\" Apply requests. It means user will re-acquire conflicting fields owned by other people. Force flag must be unset for non-apply patch requests.",
        "in": "query",
        "name": "force",
        "type": "boolean",
        "uniqueItems": true
      }
    ],
    "produces": [
      "application/json",
      "application/yaml",
      "application/vnd.kubernetes.protobuf"
    ],
    "responses": {
      "200": {
        "description": "OK",
        "schema": {
          "$ref": "#/definitions/io.k8s.api.rbac.v1.RoleBinding"
        }
      },
      "401": {
        "description": "Unauthorized"
      }
    },
    "schemes": [
      "https"
    ],
    "tags": [
      "rbacAuthorization_v1"
    ],
    "x-kubernetes-action": "patch",
    "x-kubernetes-group-version-kind": {
      "group": "rbac.authorization.k8s.io",
      "kind": "RoleBinding",
      "version": "v1"
    }
  },
  "put": {
    "consumes": [
      "*/*"
    ],
    "description": "replace the specified RoleBinding",
    "operationId": "replaceRbacAuthorizationV1NamespacedRoleBinding",
    "parameters": [
      {
        "in": "body",
        "name": "body",
        "required": true,
        "schema": {
          "$ref": "#/definitions/io.k8s.api.rbac.v1.RoleBinding"
        }
      },
      {
        "description": "When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed",
        "in": "query",
        "name": "dryRun",
        "type": "string",
        "uniqueItems": true
      },
      {
        "description": "fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.",
        "in": "query",
        "name": "fieldManager",
        "type": "string",
        "uniqueItems": true
      }
    ],
    "produces": [
      "application/json",
      "application/yaml",
      "application/vnd.kubernetes.protobuf"
    ],
    "responses": {
      "200": {
        "description": "OK",
        "schema": {
          "$ref": "#/definitions/io.k8s.api.rbac.v1.RoleBinding"
        }
      },
      "201": {
        "description": "Created",
        "schema": {
          "$ref": "#/definitions/io.k8s.api.rbac.v1.RoleBinding"
        }
      },
      "401": {
        "description": "Unauthorized"
      }
    },
    "schemes": [
      "https"
    ],
    "tags": [
      "rbacAuthorization_v1"
    ],
    "x-kubernetes-action": "put",
    "x-kubernetes-group-version-kind": {
      "group": "rbac.authorization.k8s.io",
      "kind": "RoleBinding",
      "version": "v1"
    }
  }
}
#+end_SRC


#+begin_src tmate
  cd ~/go/src/k8s.io/kubernetes/api/openapi-spec
  
  cat swagger.json |jq '.paths["/apis/rbac.authorization.k8s.io/v1/namespaces/{namespace}/rolebindings/{name}"]'
#+END_SRC

#+RESULTS:
#+begin_SRC json
null
#+end_SRC

** groning

#+BEGIN_SRC shell :dir .. :wrap "SRC json"
  cat swagger.json | jq '.definitions' | gron
#+END_SRC

#+RESULTS:

* Definitions
** Keys
#+BEGIN_SRC shell :dir .. :wrap "SRC json"
  cat swagger.json | jq '.definitions' | jq -r 'keys[]' \
  | sort -R | head -20
  # | grep -vi alpha\\\|beta \
#+END_SRC

#+RESULTS:
#+begin_SRC json
io.k8s.api.apps.v1.ReplicaSetStatus
io.k8s.api.core.v1.GlusterfsVolumeSource
io.k8s.api.core.v1.ServiceAccountList
io.k8s.api.auditregistration.v1alpha1.WebhookThrottleConfig
io.k8s.api.autoscaling.v1.HorizontalPodAutoscalerSpec
io.k8s.api.rbac.v1beta1.RoleList
io.k8s.api.core.v1.ReplicationController
io.k8s.api.apps.v1beta1.RollbackConfig
io.k8s.api.rbac.v1alpha1.ClusterRole
io.k8s.api.core.v1.SecretProjection
io.k8s.api.networking.v1.NetworkPolicyPeer
io.k8s.api.storage.v1beta1.CSIDriverSpec
io.k8s.apiextensions-apiserver.pkg.apis.apiextensions.v1beta1.JSONSchemaProps
io.k8s.api.core.v1.Lifecycle
io.k8s.api.apps.v1.DeploymentStatus
io.k8s.api.apps.v1beta1.ControllerRevision
io.k8s.api.core.v1.DownwardAPIVolumeSource
io.k8s.api.authentication.v1beta1.TokenReview
io.k8s.api.networking.v1beta1.Ingress
io.k8s.apiextensions-apiserver.pkg.apis.apiextensions.v1beta1.CustomResourceDefinitionCondition
#+end_SRC

* Metadata
** Info title: Kubernetes, version: v1.16.0
  
#+BEGIN_SRC shell :dir ~/go/src/k8s.io/kubernetes/api/openapi-spec :wrap "SRC json"
   cat swagger.json | jq '.info'
 #+END_SRC

 #+RESULTS:
 #+begin_SRC json
 {
   "title": "Kubernetes",
   "version": "v1.16.0"
 }
 #+end_SRC
** Swagger: 2.0

#+BEGIN_SRC shell :dir ~/go/src/k8s.io/kubernetes/api/openapi-spec :wrap "SRC json"
   cat swagger.json | jq '.swagger'
 #+END_SRC

 #+RESULTS:
 #+begin_SRC json
 "2.0"
 #+end_SRC
** Security BearerToken: []
  
#+BEGIN_SRC shell :dir ~/go/src/k8s.io/kubernetes/api/openapi-spec :wrap "SRC json"
   cat swagger.json | jq '.security'
 #+END_SRC

 #+RESULTS:
 #+begin_SRC json
 [
   {
     "BearerToken": []
   }
 ]
 #+end_SRC
** securityDefinitions: BearerToken

#+BEGIN_SRC shell :dir ~/go/src/k8s.io/kubernetes/api/openapi-spec :wrap "SRC json"
  cat swagger.json | jq '.securityDefinitions'
#+END_SRC

 #+RESULTS:
 #+begin_SRC json
 {
   "BearerToken": {
     "description": "Bearer Token authentication",
     "in": "header",
     "name": "authorization",
     "type": "apiKey"
   }
 }
 #+end_SRC

# Local Variables:
# eval: (add-hook 'after-save-hook (lambda () (org-babel-tangle-file buffer-file-name)))
# End:

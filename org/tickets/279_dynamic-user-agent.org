#+TITLE: Dynamic user Agent

* Ticket
* Process
** Grab update sources code
   #+begin_src python
     #!/usr/bin/env python
     import yaml
     try:
         from urllib.request import urlopen, urlretrieve
     except Exception as e:
         from urllib import urlopen, urlretrieve
     import re
     from bs4 import BeautifulSoup
     import click
     import json

     gubernator = "https://gubernator.k8s.io/builds/kubernetes-jenkins/logs/"

     gcs_logs="https://storage.googleapis.com/kubernetes-jenkins/logs/"

     def get_json(url):
         body = urlopen(url).read()
         data = json.loads(body)
         return data

     @click.command()
     @click.argument('sources')
     def main(sources):
         # https://github.com/yaml/pyyaml/wiki/PyYAML-yaml.load(input)-Deprecation
         syaml = yaml.load(open(sources).read())
         for bucket, info in syaml['buckets'].items():
             try:
                 testgrid_history = get_json(gcs_logs + bucket + "/jobResultsCache.json")
             except:
                 import ipdb; ipdb.set_trace(context=60)
             latest_success = [x for x in testgrid_history if x['result'] == 'SUCCESS'][-1]['buildnumber']
             syaml['buckets'][bucket]['jobs']=[str(latest_success)]
             if bucket == syaml['default-view']['bucket']:
                 syaml['default-view']['job']=str(latest_success)
         with open(sources, "w") as f:
             yaml_content = yaml.dump(syaml,
                                      indent=4,
                                      default_flow_style=False)
             f.write(yaml_content)
             print(yaml_content)

     if __name__ == "__main__":
         main()
   #+end_src
   
** Parse down to the only code we need
   The majority of this assumes a yaml file that includes multiple buckets and jobs.
   We dont' need to read from or write from yaml, and we don't need to figure out our bucket as it will either be the default one or an argument passed in.
    
   We only need to figure out what is the latest job for a bucket, if the job was not passed in as an argument.  So the main code to lift is 
   ~latest_success = [x for x in testgrid_history if x['result'] == 'SUCCESS'][-1]['buildnumber']~
** Integrate update_sources with current load_swaggers
   
   I'll make a new function to test this out, that posts to the same bucket_job_swagger

 #+NAME: load_swagger.py
 #+BEGIN_SRC python :eval never :exports code
   try:
       from urllib.request import urlopen, urlretrieve
       from string import Template
       import json

       def get_json(url):
           body = urlopen(url).read()
           data = json.loads(body)
           return data

       success_message = "It workd good job!!!" if envbucket is None else envbucket
       gcs_logs="https://storage.googleapis.com/kubernetes-jenkins/logs/"
       bucket = 'ci-kubernetes-e2e-gci-gce' if envbucket is None else envbucket
       testgrid_history = get_json(gcs_logs + bucket + "/jobResultsCache.json")
       latest_success = [x for x in testgrid_history if x['result'] == 'SUCCESS'][-1]['buildnumber']
       job = latest_success if envjob is None else envjob
       metadata_url = ''.join(['https://storage.googleapis.com/kubernetes-jenkins/logs/', bucket, '/', job, '/finished.json'])
       metadata = json.loads(urlopen(metadata_url).read().decode('utf-8'))
       commit_hash = metadata["version"].split("+")[1]
       swagger_url =  ''.join(['https://raw.githubusercontent.com/kubernetes/kubernetes/', commit_hash, '/api/openapi-spec/swagger.json']) 
       swagger = json.loads(urlopen(swagger_url).read().decode('utf-8')) # may change this to ascii
       sql = """
    INSERT INTO bucket_job_swagger(
              bucket,
              job,
              commit_hash, 
              passed,
              job_result,
              pod,
              infra_commit,
              job_version,
              job_timestamp,
              node_os_image,
              master_os_image,
              swagger
       )
      SELECT
              $1 as bucket,
              $2 as job,
              $3 as commit_hash,
              $4 as passed,
              $5 as job_result,
              $6 as pod,
              $7 as infra_commit,
              $8 as job_version,
              (to_timestamp($9)) AT TIME ZONE 'UTC' as job_timestamp,
              $10 as node_os_image,
              $11 as master_os_image,
              $12 as swagger
       """
       plan = plpy.prepare(sql, [
           'text','text','text','text',
           'text','text','text','text',
           'integer','text','text','jsonb'])
       if live:
           rv = plpy.execute(plan, [
               'apisnoop',
               'live',
               commit_hash,
               metadata['passed'],
               metadata['result'],
               metadata['metadata']['pod'],
               metadata['metadata']['infra-commit'],
               metadata['version'],
               int(metadata['timestamp']),
               metadata['metadata']['node_os_image'],
               metadata['metadata']['master_os_image'],
               json.dumps(swagger)
           ])
       else:
           rv = plpy.execute(plan, [
               bucket,
               job,
               commit_hash,
               metadata['passed'],
               metadata['result'],
               metadata['metadata']['pod'],
               metadata['metadata']['infra-commit'],
               metadata['version'],
               int(metadata['timestamp']),
               metadata['metadata']['node_os_image'],
               metadata['metadata']['master_os_image'],
               json.dumps(swagger)
           ])
       return success_message
   except Exception as err:
       return Template("something went wrong, likely this: ${error}").substitute(error = err)
 #+END_SRC
 
   #+NAME: load_bucket_job_swagger_via_curl.sql
   #+BEGIN_SRC sql-mode :noweb yes :results silent
     set role dba;
     DROP FUNCTION IF EXISTS load_swagger;
     CREATE OR REPLACE FUNCTION load_swagger(
       envbucket text default current_setting('custom.bucket', true),
       envjob text default null,
       live boolean default false)
     RETURNS text AS $$
     <<load_swagger.py>>
     $$ LANGUAGE plpython3u ;
     reset role;
   #+END_SRC
   
   #+begin_src sql-mode :results silent
   delete from bucket_job_swagger;
   #+end_src
   
   #+begin_src sql-mode
     select * from load_swagger();
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
        load_swagger     
   ----------------------
    It workd good job!!!
   (1 row)

   #+end_src

** Use pg.options vars as bucket, job argument in load swaggers

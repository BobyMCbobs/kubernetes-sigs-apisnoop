#+TITLE: Dynamic user Agent

* Ticket
* Process
** Grab update sources code
   #+begin_src python
     #!/usr/bin/env python
     import yaml
     try:
         from urllib.request import urlopen, urlretrieve
     except Exception as e:
         from urllib import urlopen, urlretrieve
     import re
     from bs4 import BeautifulSoup
     import click
     import json

     gubernator = "https://gubernator.k8s.io/builds/kubernetes-jenkins/logs/"

     gcs_logs="https://storage.googleapis.com/kubernetes-jenkins/logs/"

     def get_json(url):
         body = urlopen(url).read()
         data = json.loads(body)
         return data

     @click.command()
     @click.argument('sources')
     def main(sources):
         # https://github.com/yaml/pyyaml/wiki/PyYAML-yaml.load(input)-Deprecation
         syaml = yaml.load(open(sources).read())
         for bucket, info in syaml['buckets'].items():
             try:
                 testgrid_history = get_json(gcs_logs + bucket + "/jobResultsCache.json")
             except:
                 import ipdb; ipdb.set_trace(context=60)
             latest_success = [x for x in testgrid_history if x['result'] == 'SUCCESS'][-1]['buildnumber']
             syaml['buckets'][bucket]['jobs']=[str(latest_success)]
             if bucket == syaml['default-view']['bucket']:
                 syaml['default-view']['job']=str(latest_success)
         with open(sources, "w") as f:
             yaml_content = yaml.dump(syaml,
                                      indent=4,
                                      default_flow_style=False)
             f.write(yaml_content)
             print(yaml_content)

     if __name__ == "__main__":
         main()
   #+end_src
   
** Parse down to the only code we need
   The majority of this assumes a yaml file that includes multiple buckets and jobs.
   We dont' need to read from or write from yaml, and we don't need to figure out our bucket as it will either be the default one or an argument passed in.
    
   We only need to figure out what is the latest job for a bucket, if the job was not passed in as an argument.  So the main code to lift is 
   ~latest_success = [x for x in testgrid_history if x['result'] == 'SUCCESS'][-1]['buildnumber']~
** Integrate update_sources with current load_swaggers
   
   I'll make a new function to test this out, that posts to the same bucket_job_swagger

 #+NAME: load_swagger.py
 #+BEGIN_SRC python :eval never :exports code
   try:
       from urllib.request import urlopen, urlretrieve
       from string import Template
       import os
       import json

       def get_json(url):
           body = urlopen(url).read()
           data = json.loads(body)
           return data

       gcs_logs="https://storage.googleapis.com/kubernetes-jenkins/logs/"

       #establish bucket we'll draw test results from.
       baseline_bucket = os.environ['APISNOOP_BASELINE_BUCKET'] if 'APISNOOP_BASELINE_BUCKET' in os.environ.keys() else 'ci-kubernetes-e2e-gci-gce'
       bucket =  baseline_bucket if custom_bucket is None else custom_bucket

       #grab the latest successful test run for our chosen bucket.
       testgrid_history = get_json(gcs_logs + bucket + "/jobResultsCache.json")
       latest_success = [x for x in testgrid_history if x['result'] == 'SUCCESS'][-1]['buildnumber']

       #establish job 
       baseline_job = os.environ['APISNOOP_BASELINE_JOB'] if 'APISNOOP_BASELINE_JOB' in os.environ.keys() else latest_success
       job = baseline_job if custom_job is None else custom_job

       metadata_url = ''.join(['https://storage.googleapis.com/kubernetes-jenkins/logs/', bucket, '/', job, '/finished.json'])
       metadata = json.loads(urlopen(metadata_url).read().decode('utf-8'))
       commit_hash = metadata["version"].split("+")[1]
       swagger_url =  ''.join(['https://raw.githubusercontent.com/kubernetes/kubernetes/', commit_hash, '/api/openapi-spec/swagger.json']) 
       swagger = json.loads(urlopen(swagger_url).read().decode('utf-8')) # may change this to ascii
       sql = """
    INSERT INTO bucket_job_swagger(
              bucket,
              job,
              commit_hash, 
              passed,
              job_result,
              pod,
              infra_commit,
              job_version,
              job_timestamp,
              node_os_image,
              master_os_image,
              swagger
       )
      SELECT
              $1 as bucket,
              $2 as job,
              $3 as commit_hash,
              $4 as passed,
              $5 as job_result,
              $6 as pod,
              $7 as infra_commit,
              $8 as job_version,
              (to_timestamp($9)) AT TIME ZONE 'UTC' as job_timestamp,
              $10 as node_os_image,
              $11 as master_os_image,
              $12 as swagger
       """
       plan = plpy.prepare(sql, [
           'text','text','text','text',
           'text','text','text','text',
           'integer','text','text','jsonb'])
       if live:
           rv = plpy.execute(plan, [
               'apisnoop',
               'live',
               commit_hash,
               metadata['passed'],
               metadata['result'],
               metadata['metadata']['pod'],
               metadata['metadata']['infra-commit'],
               metadata['version'],
               int(metadata['timestamp']),
               metadata['metadata']['node_os_image'],
               metadata['metadata']['master_os_image'],
               json.dumps(swagger)
           ])
       else:
           rv = plpy.execute(plan, [
               bucket,
               job,
               commit_hash,
               metadata['passed'],
               metadata['result'],
               metadata['metadata']['pod'],
               metadata['metadata']['infra-commit'],
               metadata['version'],
               int(metadata['timestamp']),
               metadata['metadata']['node_os_image'],
               metadata['metadata']['master_os_image'],
               json.dumps(swagger)
           ])
       return ''.join(["Success!  Added the swagger for job ", job, " from bucket ", bucket])
   except Exception as err:
       return Template("something went wrong, likely this: ${error}").substitute(error = err)
 #+END_SRC
 
   #+NAME: load_bucket_job_swagger_via_curl.sql
   #+BEGIN_SRC sql-mode :noweb yes :results silent
     set role dba;
     DROP FUNCTION IF EXISTS load_swagger;
     CREATE OR REPLACE FUNCTION load_swagger(
       custom_bucket text default null,
       custom_job text default null,
       live boolean default false)
     RETURNS text AS $$
     <<load_swagger.py>>
     $$ LANGUAGE plpython3u ;
     reset role;
   #+END_SRC
   
   #+begin_src sql-mode :results silent
   delete from bucket_job_swagger;
   #+end_src
   
   #+begin_src sql-mode
     select * from load_swagger('ci-kubernetes-e2e-gce-cos-k8sbeta-default');
     -- select * from load_swagger();
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
                                                    load_swagger                                                  
   ---------------------------------------------------------------------------------------------------------------
    Success!  Added the swagger for job 1201559454126968833 from bucket ci-kubernetes-e2e-gce-cos-k8sbeta-default
   (1 row)

   #+end_src
   
   #+begin_src sql-mode
   select bucket, job from bucket_job_swagger;
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
                     bucket                   |         job         
   -------------------------------------------+---------------------
    ci-kubernetes-e2e-gci-gce                 | 1201590752648368129
    ci-kubernetes-e2e-gce-cos-k8sbeta-default | 1201559454126968833
   (2 rows)

   #+end_src

** Use pg.options vars as bucket, job argument in load swaggers
   
   Along with integrating the job fetching, we altered the arguments being passed to the function with ~current_setting('custom.bucket', true)~.  
   Current Setting lets you view all the settings of the db ([[https://www.postgresql.org/docs/current/functions-admin.html][see documentation]]).  We can also pass along some settings when the server first starts using PGOPTIONS(see [[https://dba.stackexchange.com/questions/52235/how-can-i-use-an-environment-variable-in-a-postgres-function][this stackexchange]]).  So the plan is to set a ~custom.bucket~ and ~custom.job~ using pgoptions as part of the docker-compose. 

In the function, then, we declare a bucket and job variable.  Bucket is set to default master unless custom_bucket is set and job is set to latest success of the declared bucket unless custom_job is set.

So now we create a new docker build with a special environment variable for bucket.  If it works, we'll have loaded a swagger from the latest success of an alternate bucket.


I think we want something like
#+begin_example
  - name: PGOPTIONS
  value: "-c custom.bucket=ci-kubernetes-e2e-gce-cos-k8sbeta-default"
#+end_example

however, if we add this to our raiinbow.yaml, postgres will start but will not do any migrations.  i think there is an error being caused that is a bit hard to see in the logs.

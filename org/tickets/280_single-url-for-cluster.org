#+TITLE: 280, a single url for the cluster
#+AUTHOR: Zach Mandeville
#+AUTHOR: Caleb Woodbine

* Our Ticket
[[https://github.com/cncf/apisnoop/issues/280][github link]]

#+begin_quote
Apisnoop, when applied to a cluster, spins up a postgres database, a hasura api, and a small web app with some basic statistics. Each of these are on a port that, to view, you have to run a port-forward for.

It'd be ergonomically pleasant to have them instead set to a custom url for the cluster, like
cluster.url/apisnoop for the webapp and cluster.url/apisnoop/explorer for the hasura frontend.

Investigate the work required to do this, and see if the benefit of urls is worth that effort.
#+end_quote

* Process
** Look at existing Yaml file

   Our raiinbow yaml is currently set up as:

   #+begin_src yaml
# raiinbow.yaml
   
apiVersion: v1
kind: List
metadata: {}
items:
- apiVersion: v1
  kind: Service
  metadata:
    name: hasura
  spec:
    type: ClusterIP
    clusterIP: None
    selector:
      io.apisnoop.graphql: hasura
    ports:
    - name: "8080"
      port: 8080
      targetPort: 8080
- apiVersion: v1
  kind: Service
  metadata:
    name: postgres
  spec:
    selector:
      io.apisnoop.db: postgres
    ports:
    - name: "5432"
      port: 5432
      targetPort: 5432
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: hasura
  spec:
    replicas: 1
    selector:
      matchLabels:
        io.apisnoop.graphql: hasura
    template:
      metadata:
        labels:
          io.apisnoop.graphql: hasura
      spec:
        restartPolicy: Always
        containers:
        - name: hasura
          image: "raiinbow/hasura:2019-12-08-21-00"
          ports:
          - containerPort: 8080
          env:
          - name: HASURA_GRAPHQL_DATABASE_URL
            value: "postgres://apisnoop:s3cretsauc3@postgres:5432/apisnoop"
          - name: HASURA_GRAPHQL_ENABLE_CONSOLE
            value: "true"
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: postgres
  spec:
    replicas: 1
    selector:
      matchLabels:
        io.apisnoop.db: postgres
    template:
      metadata:
        labels:
          io.apisnoop.db: postgres
      spec:
        restartPolicy: Always
        containers:
        - name: postgres
          image: "raiinbow/postgres:2019-12-03-14-19"
          ports:
          - containerPort: 5432
          livenessProbe:
            exec:
              command:
              - "pg_isready"
              - "-U"
              - "apisnoop"
            failureThreshold: 5
            periodSeconds: 10
            timeoutSeconds: 5
          env:
          - name: POSTGRES_DB
            value: apisnoop
          - name: POSTGRES_USER
            value: apisnoop
          - name: POSTGRES_PASSWORD
            value: s3cretsauc3
          - name: PGDATABASE
            value: apisnoop
          - name: PGUSER
            value: apisnoop
          # - name: APISNOOP_BASELINE_BUCKET
          #   value: ci-kubernetes-e2e-gce-cos-k8sbeta-default
          # - name: APISNOOP_BASELINE_JOB
          #   value: 1141312231231223
- apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: apisnoop-auditlogger
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: apisnoop-auditlogger
    template:
      metadata:
        labels:
          app: apisnoop-auditlogger
      spec:
        containers:
          - name: apisnoop-auditlogger
            image: "raiinbow/auditlogger:2019-12-08-31"
            #command:
            #  - "sleep"
            #args: 
            #  - "+Inf"
            ports:
              - containerPort: 9900
- apiVersion: v1
  kind: Service
  metadata:
    name: apisnoop-auditlogger
  spec:
    ports:
      - port: 9900
        targetPort: 9900
    selector:
      app: apisnoop-auditlogger
    clusterIP: 10.96.96.96
    type: ClusterIP
- apiVersion: auditregistration.k8s.io/v1alpha1
  kind: AuditSink
  metadata:
    name: auditlogger
  spec:
    policy:
      level: Metadata
      stages:
      - ResponseComplete
    webhook:
      throttle:
        qps: 10
        burst: 15
      clientConfig:
        #url: "http://apisnoop-auditlogger.kube-system.svc.cluster.local:9900/events"
        # svc cluster ip of apisnoop-auditlogger
        url: "http://10.96.96.96:9900/events"

   #+end_src
   
   two changes needed to take place for us to have our desired outcome:
- the webclient needed to be added to our yaml
- an ingress added, to properly direct requests to the right service.
** Decide upon ingress strategy
There are three options we see:
- set up an nginx ingress service
- use a lighter proxy service
- write our own custom proxy

  Setting up an nginx ingress may be overkill for our need, and would require an additional dependency on helm, and an update to our cluster setup docs and workflow.

We found a proxy library called [[https://github.com/wunderlist/moxy][moxy]] that seemed to match what we were looking for, but wondered if there'd be an even simpler way.  For our development, we are using kind and so pinged the kind channel to see if they had a recommended method.

There also appears to be [[https://github.com/kubernetes-sigs/kind/blob/master/site/content/docs/user/ingress.md][Kind ingress documentation]].
*** Kind cluster config to allow ingress
  By default, Kind doesn't expose any ports, as such services running in the cluster cannot be accessed from the host on which it runs.
 #+begin_src yaml
# kind-cluster-config.yaml
# #+NAME: kind kubeadm DynamicAuditing configuration

kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
kubeadmConfigPatches:
- |
  apiVersion: kubeadm.k8s.io/v1beta2
  kind: ClusterConfiguration
  metadata:
    name: config
  apiServer:
    extraArgs:
      "feature-gates": "DynamicAuditing=true"
      "runtime-config": "auditregistration.k8s.io/v1alpha1=true"
      "audit-dynamic-configuration": "true"
nodes:
 - role: control-plane
   kubeadmConfigPatches:
   - |
     apiVersion: kubeadm.k8s.io/v1beta2
     kind: InitConfiguration
     nodeRegistration:
       kubeletExtraArgs:
         node-labels: "ingress-ready=true"
         authorization-mode: "AlwaysAllow"
   extraPortMappings:
   - containerPort: 80
     hostPort: 80
   - containerPort: 443
     hostPort: 443 
 #+end_src
 
** Setup docker proxy to nginx proxy
  This is the first step to having a isngle portforward and all the services we need accessible by url.
** Basic NGINX config for proxing a single service
   #+begin_src nginx
server {
  listen 8080;
  location / {
    proxy_pass http://localhost:8081;
  }
}   
   #+end_src

** Add the webclient
We have it available at raiinbow, and so could get it set up just enough to ensure that all our services are running when we apply our yaml ot the cluster.

So the new yaml would look like so:
#+NAME: yaml with webclient
   #+begin_src yaml
  # raiinbow.yaml

  apiVersion: v1
  kind: List
  metadata: {}
  items:
  - apiVersion: v1
    kind: Service
    metadata:
      name: hasura
    spec:
      type: ClusterIP
      clusterIP: None
      selector:
        io.apisnoop.graphql: hasura
      ports:
      - name: "8080"
        port: 8080
        targetPort: 8080
  - apiVersion: v1
    kind: Service
    metadata:
      name: postgres
    spec:
      selector:
        io.apisnoop.db: postgres
      ports:
      - name: "5432"
        port: 5432
        targetPort: 5432
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: hasura
    spec:
      replicas: 1
      selector:
        matchLabels:
          io.apisnoop.graphql: hasura
      template:
        metadata:
          labels:
            io.apisnoop.graphql: hasura
        spec:
          restartPolicy: Always
          containers:
          - name: hasura
            image: "raiinbow/hasura:2019-12-08-21-00"
            ports:
            - containerPort: 8080
            env:
            - name: HASURA_GRAPHQL_DATABASE_URL
              value: "postgres://apisnoop:s3cretsauc3@postgres:5432/apisnoop"
            - name: HASURA_GRAPHQL_ENABLE_CONSOLE
              value: "true"
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: postgres
    spec:
      replicas: 1
      selector:
        matchLabels:
          io.apisnoop.db: postgres
      template:
        metadata:
          labels:
            io.apisnoop.db: postgres
        spec:
          restartPolicy: Always
          containers:
          - name: postgres
            image: "raiinbow/postgres:2019-12-03-14-19"
            ports:
            - containerPort: 5432
            livenessProbe:
              exec:
                command:
                - "pg_isready"
                - "-U"
                - "apisnoop"
              failureThreshold: 5
              periodSeconds: 10
              timeoutSeconds: 5
            env:
            - name: POSTGRES_DB
              value: apisnoop
            - name: POSTGRES_USER
              value: apisnoop
            - name: POSTGRES_PASSWORD
              value: s3cretsauc3
            - name: PGDATABASE
              value: apisnoop
            - name: PGUSER
              value: apisnoop
            # - name: APISNOOP_BASELINE_BUCKET
            #   value: ci-kubernetes-e2e-gce-cos-k8sbeta-default
            # - name: APISNOOP_BASELINE_JOB
            #   value: 1141312231231223
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: apisnoop-auditlogger
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: apisnoop-auditlogger
      template:
        metadata:
          labels:
            app: apisnoop-auditlogger
        spec:
          containers:
            - name: apisnoop-auditlogger
              image: "raiinbow/auditlogger:2019-12-08-31"
              #command:
              #  - "sleep"
              #args: 
              #  - "+Inf"
              ports:
                - containerPort: 9900
  - apiVersion: v1
    kind: Service
    metadata:
      name: apisnoop-auditlogger
    spec:
      ports:
        - port: 9900
          targetPort: 9900
      selector:
        app: apisnoop-auditlogger
      clusterIP: 10.96.96.96
      type: ClusterIP
  - apiVersion: auditregistration.k8s.io/v1alpha1
    kind: AuditSink
    metadata:
      name: auditlogger
    spec:
      policy:
        level: Metadata
        stages:
        - ResponseComplete
      webhook:
        throttle:
          qps: 10
          burst: 15
        clientConfig:
          #url: "http://apisnoop-auditlogger.kube-system.svc.cluster.local:9900/events"
          # svc cluster ip of apisnoop-auditlogger
          url: "http://10.96.96.96:9900/events"
  - apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: apisnoop-webapp
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: apisnoop-webapp
      template:
        metadata:
          labels:
            app: apisnoop-webapp
        spec:
          containers:
            - name: apisnoop-webapp
              image: "raiinbow/auditlogger:latest"
              #command:
              #  - "sleep"
              #args: 
              #  - "+Inf"
              ports:
                - containerPort: 9911
  - apiVersion: v1
    kind: Service
    metadata:
      name: apisnoop-webclient
    spec:
      ports:
        - port: 9911
          targetPort: 9911
      selector:
        app: apisnoop-webclient
      clusterIP: 10.96.96.96
      type: ClusterIP
  - apiVersion: auditregistration.k8s.io/v1alpha1
    metadata:
      name: webclient
    spec:
      policy:
        level: Metadata
        stages:
        - ResponseComplete
      webhook:
        throttle:
          qps: 10
          burst: 15
   #+end_src

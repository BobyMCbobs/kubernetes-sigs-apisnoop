#+TITLE: Test Writing Flow
#+AUTHOR: ii team
#+TODO: TODO(t) NEXT(n) IN-PROGRESS(i) BLOCKED(b) | DONE(d)
#+OPTIONS: toc:nil tags:nil todo:nil
#+EXPORT_SELECT_TAGS: export

* Notes :export:
This issue proposes the writing of a new test for the API endpoints ~ENDPOINT(1)~, ~ENDPOINT(2)~ with the intention of promoting it to Conformance. This issue contains a **very basic mock test** in order to start the conversation about what a good e2e test would look like for this endpoint. The code below is verified to be hitting the intended endpoint, show as per the queries in APIsnoop's live view of the cluster's audit logs.

* Filling the Gaps in Kubernetes Test Coverage

Are the Kubernetes behaviors your applications actually require well tested and guaranteed to be available on all cloud providers?

In this session, you will learn how to ensure your Kubernetes API surface area usage is exercised by tests all Kubernetes Certified Service Providers must pass.

We will cover:
- the e2e test suite
- automation that runs the suite before code is merged into Kubernetes.
- the API surface area covered by these tests
- the API surface area required by several popular applications.
- Identifying the untested API surface area your applications require
- Contributing tests that increase API surface coverage
- Promoting tests to Conformance

* TODO [91%] Cluster Setup :neverexport:
  :PROPERTIES:
  :LOGGING:  nil
  :END:
** DONE Connect demo to right eye

   #+begin_src tmate :session foo:hello :eval never-export
     echo "What parts of Kubernetes do you depend on $USER?"
   #+end_src

** DONE Create a K8s cluster using KIND

[[file:~/cncf/apisnoop/deployment/k8s/kind-cluster-config.yaml::#%20kind-cluster-config.yaml][kind-cluster-config.yaml (enabling Dynamic Audit Logging)]]

   #+BEGIN_SRC tmate :eval never-export :session foo:cluster :prologue "cd ~/cncf/apisnoop/\n"
     # Uncomment the next line if you want to clean up a previously created cluster.
     kind delete cluster
     kind create cluster --config ~/cncf/apisnoop/deployment/k8s/kind-cluster-config.yaml
   #+END_SRC
   
** DONE Grab cluster info, to ensure it is up.
   
   #+BEGIN_SRC shell :results replace 
     kubectl cluster-info
   #+END_SRC

   #+RESULTS:
   #+begin_src shell
   Kubernetes master is running at https://127.0.0.1:41743
   KubeDNS is running at https://127.0.0.1:41743/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy

   To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
   #+end_src

** DONE Prepull our images
*** apisnoop
   #+BEGIN_SRC tmate :eval never-export :session x:img :prologue "cd ~/cncf/apisnoop/\n"
     # Run twice... first time will pull and save, second will load into kind
     kind load image-archive   hasura:2019-12-03-16-31.docker-image \
       || docker pull raiinbow/hasura:2019-12-03-16-31 \
       && docker save raiinbow/hasura:2019-12-03-16-31 -o hasura:2019-12-03-16-31.docker-image

     kind load image-archive   postgres:2019-12-03-14-19.docker-image \
       || docker pull raiinbow/postgres:2019-12-03-14-19 \
       && docker save raiinbow/postgres:2019-12-03-14-19 -o postgres:2019-12-03-14-19.docker-image

     kind load image-archive   auditlogger:2019-12-08-31.docker-image \
       || docker pull raiinbow/auditlogger:2019-12-08-31 \
       && docker save raiinbow/auditlogger:2019-12-08-31 -o auditlogger:2019-12-08-31.docker-image

   #+END_SRC
*** check
   #+begin_src shell :eval never-export :exports both
     #ps ax | grep kind\ load | grep -v grep
     docker exec kind-control-plane crictl img
   #+end_src

  #+RESULTS:
  #+begin_src shell
  IMAGE                                TAG                 IMAGE ID            SIZE
  docker.io/kindest/kindnetd           0.5.3               aa67fec7d7ef7       80.3MB
  docker.io/raiinbow/auditlogger       2019-12-08-31       b725b8a8594c9       381MB
  docker.io/raiinbow/hasura            2019-12-08-21-00    bb2e8f81a4687       36.6MB
  docker.io/raiinbow/auditlogger       2019-12-08-20-49    bb2e8f81a4687       36.6MB
  docker.io/raiinbow/postgres          2019-12-03-14-19    e712ce7cc2a67       461MB
  k8s.gcr.io/coredns                   1.6.2               bf261d1579144       44.2MB
  k8s.gcr.io/etcd                      3.3.15-0            b2756210eeabf       248MB
  k8s.gcr.io/kube-apiserver            v1.16.3             392249bd86967       185MB
  k8s.gcr.io/kube-controller-manager   v1.16.3             808025b3748ef       128MB
  k8s.gcr.io/kube-proxy                v1.16.3             f4fd1d7052b4e       103MB
  k8s.gcr.io/kube-scheduler            v1.16.3             1974a03197540       105MB
  k8s.gcr.io/pause                     3.1                 da86e6ba6ca19       746kB
  #+end_src

** DONE Check on cluster
#+begin_src shell :eval never-export :exports both
docker exec kind-control-plane crictl ps 
#+end_src

#+RESULTS:
#+begin_src shell
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID
968a756bd6afe       bb2e8f81a4687       27 seconds ago      Running             apisnoop-auditlogger      3                   271c5a984b04d
54560b301a462       e712ce7cc2a67       2 minutes ago       Running             postgres                  0                   7e9e1cc2ddb82
af1c54ff5bdef       bf261d1579144       22 minutes ago      Running             coredns                   0                   9a515ddfde7fd
7af864e62df4d       bf261d1579144       22 minutes ago      Running             coredns                   0                   eea023ec31d2a
25bec1a1f8c52       f4fd1d7052b4e       22 minutes ago      Running             kube-proxy                0                   a49246e1e712e
b2db0dd45ab85       aa67fec7d7ef7       22 minutes ago      Running             kindnet-cni               0                   25a185c1dbb9a
7f765253343a0       b2756210eeabf       23 minutes ago      Running             etcd                      0                   1469b2f82e6b4
5c2eb1ca6453c       392249bd86967       23 minutes ago      Running             kube-apiserver            0                   79313d5b5a557
0414a8147685e       808025b3748ef       23 minutes ago      Running             kube-controller-manager   0                   983195c1c3b9d
57331d03cc0a2       1974a03197540       23 minutes ago      Running             kube-scheduler            0                   51bb98d6c3eb2
#+end_src

** DONE Deploy APISnoop                                              :export:
   
   #+begin_src shell :eval never-export :wrap "SRC text"
     kubectl apply -f ~/cncf/apisnoop/deployment/k8s/raiinbow.yaml
   #+end_src

** DONE Verify Pods Running
   #+begin_src shell
     kubectl get pods
   #+end_src

   #+RESULTS:
   #+begin_src shell
   NAME                        READY   STATUS             RESTARTS   AGE
   hasura-5d447cc65d-jbclm     0/1     CrashLoopBackOff   6          10m
   postgres-7b494768d5-f6zvc   1/1     Running            0          10m
   #+end_src
** DONE Setup Port-Forwarding from us to sharing to the cluster

   We'll setup port-forwarding for postgres, to let us easily send queries from within our org file.
   You can check the status of the port-forward in your right eye.
   #+BEGIN_SRC tmate :eval never-export :session foo:postgres
     export GOOGLE_APPLICATION_CREDENTIALS=$HOME/.gcreds.json
     # export K8S_NAMESPACE="kube-system"
     # kubectl config set-context $(kubectl config current-context) --namespace=$K8S_NAMESPACE 2>&1 > /dev/null
     POSTGRES_POD=$(kubectl get pod --selector=io.apisnoop.db=postgres -o name | sed s:pod/::)
     POSTGRES_PORT=$(kubectl get pod $POSTGRES_POD --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{"\n"}}')
     kubectl port-forward $POSTGRES_POD $(id -u)1:$POSTGRES_PORT
   #+END_SRC

   Then we'll setup a port-forward for hasura, so our web app can query it directly.
   #+BEGIN_SRC tmate :eval never-export :session foo:hasura
     HASURA_POD=$(kubectl get pod --selector=io.apisnoop.graphql=hasura -o name | sed s:pod/::)
     HASURA_PORT=$(kubectl get pod $HASURA_POD --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{"\n"}}')
     kubectl port-forward $HASURA_POD --address 0.0.0.0 8080:$HASURA_PORT
   #+END_SRC
** DONE Connect Org to our apisnoop db
   #+NAME: ReConnect org to postgres
   #+BEGIN_SRC emacs-lisp :results silent
     (if (get-buffer "*SQL: postgres:none*")
         (with-current-buffer "*SQL: postgres:none*"
           (kill-buffer)))
     (sql-connect "apisnoop" (concat "*SQL: postgres:none*"))
   #+END_SRC
** DONE Check it all worked

   Once the postgres pod has been up for at least three minutes, you can check if it all works.

   Running ~\d+~ will list all the tables and views in your db, and their size.
   First,you want to ensure that relations _are_ found.  IF not, something happened with postgres and you should check the logs (check out [[#footnotes]] for more info.)

   There should be about a dozen views, and two tables.  The table ~bucket_job_swagger~ should be about 3712kb.  The table ~raw_audit_event~ should be about 416mb.  If either show as 8192 bytes, it means no data loaded.  Check the Hasura logs in this case, to see if there was an issue with the migration.

   #+begin_src sql-mode :results replace
     \d+
   #+end_src

   #+RESULTS:
   #+begin_SRC example
   Did not find any relations.
   #+end_SRC

** DONE Check current coverage
   It can be useful to see the current level of testing according to your baseline audit log (by default the last successful test run on master).

   You can view this with the query:
   #+NAME: stable endpoint stats
   #+begin_src sql-mode
     select * from stable_endpoint_stats where job != 'live';
   #+end_src

   #+RESULTS: stable endpoint stats
   #+begin_src sql-mode
            job         |    date    | total_endpoints | test_hits | conf_hits | percent_tested | percent_conf_tested 
   ---------------------+------------+-----------------+-----------+-----------+----------------+---------------------
    1203778996630720516 | 2019-12-08 |             438 |       183 |       129 |          41.78 |               29.45
   (1 row)

   #+end_src

** TODO Stand up, Stretch, and get a glass of water
   You did it! By hydration and pauses are important.  Take some you time, and drink a full glass of water!
* Identify an untested feature Using APISnoop                        :export:

According to this APIsnoop query, there are still some remaining ConfigMap endpoints which are untested.

  #+NAME: untested_stable_core_endpoints
  #+begin_src sql-mode :eval never-export :exports both :session none
    SELECT
      operation_id,
      -- k8s_action,
      path,
      description
      FROM untested_stable_core_endpoints
      where path not like '%volume%'
      -- and operation_id ilike '%%'
     ORDER BY operation_id desc
     LIMIT 150
           ;
  #+end_src


* Use API Reference to Lightly Document the Feature                  :export:
- [[https://kubernetes.io/docs/reference/kubernetes-api/][Kubernetes API Reference Docs]]
- [client-go - MyResourceReplaceMe](https://github.com/kubernetes/client-go/blob/master/kubernetes/typed/core/v1/resourcename.go)

* The mock test                                                      :export:
** Test outline
1. Create a ConfigMap with a static label

2. Patch the ConfigMap with a new Label and updated data

3. Get the ConfigMap to ensure it's patched

4. List all ConfigMaps in all Namespaces
   find the ConfigMap(1)
   ensure that the ConfigMap is found and is patched

5. Delete Namespaced ConfigMap(1) via a Collection with a LabelSelector

** Example in Go

   #+begin_src shell
     go get -v -u k8s.io/apimachinery/pkg/apis/meta/v1
     go get -v -u k8s.io/client-go/kubernetes
     go get -v -u k8s.io/client-go/tools/clientcmd
   #+end_src
This mock test uses client-go's standard functions for talking to the Event area of the API. 
   #+begin_src go
     package main

     import (
       "fmt"
       "flag"
       "os"
       v1 "k8s.io/api/core/v1"
       metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
       "k8s.io/client-go/kubernetes"
       "k8s.io/apimachinery/pkg/types"
       "k8s.io/client-go/tools/clientcmd"
     )

     func main() {
       // uses the current context in kubeconfig
       kubeconfig := flag.String("kubeconfig", fmt.Sprintf("%v/%v/%v", os.Getenv("HOME"), ".kube", "config"), "(optional) absolute path to the kubeconfig file")
       flag.Parse()
       config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig)
       if err != nil {
         fmt.Println(err)
       }
       // make our work easier to find in the audit_event queries
       config.UserAgent = "live-test-writing"
       // creates the clientset
       ClientSet, _ := kubernetes.NewForConfig(config)

       // write test here

       fmt.Println("[status] complete")

     }
   #+end_src

   #+RESULTS:
   #+begin_src go
   #+end_src

* Verify with APISnoop                                               :export:
Discover useragents:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select distinct useragent from audit_event where bucket='apisnoop' and useragent not like 'kube%' and useragent not like 'coredns%' and useragent not like 'kindnetd%' and useragent like 'live%';
  #+end_src

Display endpoints hit by the test:
#+begin_src sql-mode :exports both :session none
select * from endpoints_hit_by_new_test where useragent like 'live%'; 
#+end_src

Show the change in coverage after the mock test:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select * from projected_change_in_coverage;
  #+end_src

* Final notes :export:
If a test with these calls gets merged, **Conformance coverage will go up by 2 points**

-----  
/sig testing
 
/sig architecture  

/area conformance  


* Open Tasks
  Set any open tasks here, using org-todo
** DONE Live Your Best Life
** Comments
   #+begin_src sql-mode
COMMENT ON TABLE bucket_job_swagger IS 'raw data taken from audit events relevant swagger.json';
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
   COMMENT
   #+end_src
   
   #+begin_src sql-mode
   \d+ 
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
                                                                 List of relations
    Schema |               Name               |       Type        |  Owner   |  Size   |                      Description                       
   --------+----------------------------------+-------------------+----------+---------+--------------------------------------------------------
    public | api_operation_material           | materialized view | apisnoop | 3688 kB | 
    public | api_operation_parameter_material | materialized view | apisnoop | 6016 kB | 
    public | audit_event                      | view              | apisnoop | 0 bytes | 
    public | bucket_job_swagger               | table             | apisnoop | 3712 kB | raw data taken from audit events relevant swagger.json
    public | change_in_coverage               | view              | apisnoop | 0 bytes | 
    public | change_in_tests                  | view              | apisnoop | 0 bytes | 
    public | endpoint_coverage                | view              | apisnoop | 0 bytes | 
    public | endpoints_hit_by_new_test        | view              | apisnoop | 0 bytes | 
    public | projected_change_in_coverage     | view              | apisnoop | 0 bytes | 
    public | raw_audit_event                  | table             | apisnoop | 407 MB  | 
    public | stable_endpoint_stats            | view              | apisnoop | 0 bytes | 
    public | untested_stable_core_endpoints   | view              | apisnoop | 0 bytes | 
   (12 rows)

   #+end_src

* Footnotes :neverexport:
  :PROPERTIES:
  :CUSTOM_ID: footnotes
  :END:
** Load Logs to Help Debug Cluster
   #:PROPERTIES:
   #:header-args:tmate+: :prologue (concat "cd " (file-name-directory buffer-file-name) "../../apisnoop/apps\n. .loadenv\n")
   #:END:
*** hasura logs

    #+BEGIN_SRC tmate :eval never-export :session foo:hasura_logs
      HASURA_POD=$(\
                   kubectl get pod --selector=io.apisnoop.graphql=hasura -o name \
                       | sed s:pod/::)
      kubectl logs $HASURA_POD -f
    #+END_SRC

*** postgres logs

    #+BEGIN_SRC tmate :eval never-export :session foo:postgres_logs
      POSTGRES_POD=$(\
                     kubectl get pod --selector=io.apisnoop.db=postgres -o name \
                         | sed s:pod/::)
      kubectl logs $POSTGRES_POD -f
    #+END_SRC

*** auditlogger logs

    #+BEGIN_SRC tmate :eval never-export :session foo:postgres_logs
      AUDITLOGGER_POD=$(\
                     kubectl get pod --selector=app=apisnoop-auditlogger -o name \
                         | sed s:pod/::)
      kubectl logs $AUDITLOGGER_POD -f
    #+END_SRC

** Manually load swagger or audit events
   If you ran through the full setup, but were getting 0's in the stable_endpint_stats, it means the table migrations were successful, but no data was loaded.

   You can verify data loaded with the below query.  ~bucket_job_swagger~ should have a size around 3600kb and raw_audit_event should have a size around 412mb.

   #+NAME: Verify Data Loaded
   #+begin_src sql-mode
     \dt+
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
     List of relations
       Schema |        Name        | Type  |  Owner   |  Size   | Description
       --------+--------------------+-------+----------+---------+-------------
       public | bucket_job_swagger | table | apisnoop | 3600 kB |
       public | raw_audit_event    | table | apisnoop | 412 MB  |
       (2 rows)

   #+end_src

   If either shows a size of ~8192 bytes~, you'll want to manually load it, refresh materialized views, then check again.

   if you want to load a particular bucket or job, you can name them as the first and second argument of these functions.
   e.g
   : select * from load)swagger('ci-kubernetes-beta', 1122334344);
   will load that specific bucket/job combo.
   : select * from load_swagger('ci-kubernetes-beta');
   will load the latest successful test run for ~ci-kubernetes-beta~
   : select * from load_swagger('ci-kubernetes-beta', null, true);
   will load the latest successful test run for ~ci-kubernetes-beta~, but with bucket and job set to 'apisnoop/live' (used for testing).
   #+NAME: Manually load swaggers
   #+begin_src sql-mode
     select * from load_swagger();
     select * from load_swagger(null, null, true);
   #+end_src

   #+NAME: Manually load audit events
   #+begin_src sql-mode
     select * from load_audit_events();
   #+end_src

   #+NAME: Refresh Materialized Views
   #+begin_src sql-mode
     REFRESH MATERIALIZED VIEW api_operation_material;
     REFRESH MATERIALIZED VIEW api_operation_parameter_material;
   #+end_src
** 200: stuff
*** 250: api_schema view
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/250_view_api_schema.up.sql
    :END:
**** Create

  #+NAME: api_schema view
  #+BEGIN_SRC sql-mode 
    CREATE OR REPLACE VIEW "public"."api_schema" AS 
     SELECT 
        bjs.bucket,
        bjs.job,
        d.key AS schema_name,
        (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'kind'::text) AS k8s_kind,
        (d.value ->> 'type'::text) AS resource_type,
        (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'version'::text) AS k8s_version,
        (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'group'::text) AS k8s_group,
        ARRAY(SELECT jsonb_array_elements_text(d.value -> 'required')) as required_fields,
        (d.value -> 'properties'::text) AS properties,
        d.value
       FROM bucket_job_swagger bjs
         , jsonb_each((bjs.swagger -> 'definitions'::text)) d(key, value)
       GROUP BY bjs.bucket, bjs.job, d.key, d.value;

  #+END_SRC

  #+RESULTS: api_schema view
  #+begin_src sql-mode
  CREATE VIEW
  #+end_src

*** 260: api_schema_field view
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/260_view_api_schema_field.up.sql
    :END:
**** Create
 #+NAME: api_schema_field view
 #+BEGIN_SRC sql-mode 
   CREATE OR REPLACE VIEW "public"."api_schema_field" AS 
     SELECT api_schema.schema_name as field_schema,
            d.key AS field_name,
            replace(
              CASE
              WHEN d.value->>'type' = 'string' THEN 'string'
              WHEN d.value->>'type' IS NULL THEN d.value->>'$ref'
              WHEN d.value->>'type' = 'array'
               AND d.value->'items'->> 'type' IS NULL
                THEN d.value->'items'->>'$ref'
              WHEN d.value->>'type' = 'array'
               AND d.value->'items'->>'$ref' IS NULL
                THEN d.value->'items'->>'type'
              ELSE 'integer'::text
              END, '#/definitions/','') AS field_kind,
            CASE
            WHEN d.value->>'type' IS NULL THEN 'subtype'
            ELSE d.value->>'type'
              END AS field_type,
            d.value->>'description' AS description,
            CASE
            WHEN d.key = ANY(api_schema.required_fields) THEN true
            ELSE false
              END AS required,
            CASE
            WHEN (   d.value->>'description' ilike '%This field is alpha-level%'
                  or d.value->>'description' ilike '%This is an alpha field%'
                  or d.value->>'description' ilike '%This is an alpha feature%') THEN 'alpha'
            WHEN (   d.value->>'description' ilike '%This field is beta-level%'
                  or d.value->>'description' ilike '%This field is beta%'
                  or d.value->>'description' ilike '%This is a beta feature%'
                  or d.value->>'description' ilike '%This is an beta feature%'
                  or d.value->>'description' ilike '%This is an beta field%') THEN 'beta'
            ELSE 'ga'
              END AS release,
            CASE
            WHEN  d.value->>'description' ilike '%deprecated%' THEN true
             ELSE false
             END AS deprecated,
            CASE
            WHEN ( d.value->>'description' ilike '%requires the % feature gate to be enabled%'
                  or d.value->>'description' ilike '%depends on the % feature gate being enabled%'
                  or d.value->>'description' ilike '%requires the % feature flag to be enabled%'
                  or d.value->>'description' ilike '%honored if the API server enables the % feature gate%'
                  or d.value->>'description' ilike '%honored by servers that enable the % feature%'
                  or d.value->>'description' ilike '%requires enabling % feature gate%'
                  or d.value->>'description' ilike '%honored by clusters that enables the % feature%'
                  or d.value->>'description' ilike '%only if the % feature gate is enabled%'
                  ) THEN true
            ELSE false
              END AS feature_gated,
            d.value->>'format' AS format,
            d.value->>'x-kubernetes-patch-merge-key' AS merge_key,
            d.value->>'x-kubernetes-patch-strategy' AS patch_strategy,
            api_schema.bucket,
            api_schema.job,
            d.value
       FROM (api_schema
             JOIN LATERAL jsonb_each(api_schema.properties) d(key, value) ON (true));
 #+END_SRC

 #+RESULTS: api_schema_field view
 #+begin_src sql-mode
 CREATE VIEW
 #+end_src

** 300: grkrv

*** 310: Audit Events By GVKRV(Group, Version, Kind, Resource(s),Verb)
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/310_view_audit_event_by_gvkrv.up.sql
    :END:
  
   This is a slim view, and will need to be updated to contain all useful info if/when we phase out operationID across reports.
     #+NAME: events by gvkrv
     #+BEGIN_SRC sql-mode :results silent
       CREATE OR REPLACE VIEW "public"."audit_events_by_gvkrv" AS
         SELECT
           CASE
           WHEN ((a.data -> 'objectRef' ->> 'apiGroup') IS NULL) THEN ''
           ELSE (a.data -> 'objectRef' ->> 'apiGroup')
                 END as api_group,
           (a.data -> 'objectRef' ->>'apiVersion') as api_version,
           (a.data -> 'requestObject'->>'kind') as kind,
           a.param_schema as body_schema,
           (a.data -> 'objectRef'->>'resource') as resource,
             (a.data -> 'objectRef'->>'subresource') as sub_resource,
           (a.data->>'verb') as event_verb,
           operation_id,
           audit_id,
           split_part(a.useragent, '--', 2) as test,
           split_part(a.useragent, '--', 1) as useragent,
           (a.data -> 'requestObject') as request_object,
           bucket,
           job
           FROM audit_event as a
          where data->'requestObject' is not null;
     #+END_SRC
  
** 400: Podspec Field Views
   :PROPERTIES:
   :header-args:sql-mode+: :results silent
   :END:
*** 400: kind_field_path_recursion
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/400_view_kind_field_recursion.up.sql
    :END:
 #+NAME: Recursive kind_field_path view
 #+BEGIN_SRC sql-mode
   create or replace recursive view kind_field_path_recursion(
     kind,
     field_path,
     field_kind,
     field_type,
     sub_kind,
     release,
     deprecated,
     gated,
     required,
     bucket,
     job
   ) AS
    SELECT DISTINCT
    sf.field_schema AS kind,
    sf.field_name AS field_path, -- this becomes a path
    sf.field_kind AS field_kind,
    sf.field_type AS field_type,
    sf.field_schema AS sub_kind, -- this is the kind at this level
    sf.release AS release,
    sf.deprecated AS deprecated, 
    sf.feature_gated AS feature_gated,
    sf.required AS required,
    sf.bucket as bucket,
    sf.job as job
    from api_schema_field sf
    UNION
    SELECT
     kfpr.kind AS kind,
     ( kfpr.field_path || '.' || f.field_name ) AS field_path,
     f.field_kind AS field_kind,
     f.field_type AS field_type,
     CASE
     WHEN f.field_kind = 'string' OR f.field_kind = 'integer' THEN f.field_schema
     ELSE f.field_kind
      END as sub_kind,
     f.release AS release,
     f.deprecated AS deprecated,
     f.feature_gated AS feature_gated,
     f.required AS required,
     kfpr.bucket,
     kfpr.job
     FROM api_schema_field f
     INNER JOIN kind_field_path_recursion kfpr ON
     f.field_schema = kfpr.field_kind
     AND f.field_kind not like 'io.k8s.apiextensions-apiserver.pkg.apis.apiextensions.%.JSONSchemaProps';
   ;
 #+END_SRC
*** 410: kind_field_path_material
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/410_view_kind_field_path_material.up.sql
    :END:
 #+NAME: kind_field_path material
 #+BEGIN_SRC sql-mode
    create materialized view kind_field_path_material AS
    select
      kind,
      field_path AS field_path,
      field_kind AS field_kind,
      field_type,
      sub_kind,
      release,
      deprecated,
      gated,
      required,
      bucket,
      job
     from kind_field_path_recursion;
   -- drop materialized view kind_field_path_material cascade;
 #+END_SRC
**** kind_field_path_material indexes
 #+NAME: kind_field_path_material indexs
 #+BEGIN_SRC sql-mode
 CREATE INDEX kfpm_kind_idx       ON kind_field_path_material (kind);
 CREATE INDEX kfpm_field_path_idx ON kind_field_path_material (field_path);
 CREATE INDEX kfpm_field_type_idx ON kind_field_path_material (field_type);
 CREATE INDEX kfpm_sub_kind_idx   ON kind_field_path_material (sub_kind);
 -- GIST requires ltree
 -- CREATE INDEX kfpm_kind_idx       ON kind_field_path_material USING GIST (kind);
 -- CREATE INDEX kfpm_field_path_idx ON kind_field_path_material USING GIST (field_path);
 -- CREATE INDEX kfpm_field_type_idx ON kind_field_type_material USING GIST (field_type);
 -- CREATE INDEX kfpm_sub_kind_idx   ON kind_field_path_material USING GIST (sub_kind);
 #+END_SRC

*** 420: kind_field_path view
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/420_view_kind_field_path.up.sql
    :END:
 #+NAME: kind_field_path view
 #+BEGIN_SRC sql-mode
   create or replace view kind_field_path AS
   select
     kind,
     field_path,
     field_kind,
     field_type,
     sub_kind,
     release,
     deprecated,
     gated,
     required,
     bucket,
     job
    from kind_field_path_material where field_kind not like 'io%';
 #+END_SRC

*** 430: PodSpec Materialized View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/430_podspec_field_coverage_material.up.sql
    :END:
    
    #+NAME: view podspec_field_coverage_material
    #+BEGIN_SRC sql-mode :results silent
      CREATE MATERIALIZED VIEW "public"."podspec_field_coverage_material" AS 
      SELECT DISTINCT
        bucket,
        job,
        api_group,
        api_version,
        kind,
        event_verb,
        resource,
        sub_resource,
        test,
        useragent,
        jsonb_object_keys(request_object -> 'spec'::text) AS podspec_field,
        count(event_field.event_field) AS hits
        FROM audit_events_by_gvkrv,
             LATERAL
               jsonb_object_keys(audit_events_by_gvkrv.request_object -> 'spec'::text) event_field(event_field)
       WHERE kind = 'Pod'
         AND NOT (lower(api_version) ~~ ANY('{%alpha%, %beta%}')) -- api_version doesn't contain alpha or beta;
       GROUP BY bucket, job, api_group, api_version, kind, event_verb, resource, sub_resource, test, useragent, podspec_field
            UNION
      SELECT DISTINCT
        bucket,
        job,
        api_group,
        api_version,
        kind,
        event_verb,
        resource,
        sub_resource,
        test,
        useragent,
        jsonb_object_keys(request_object -> 'template' -> 'spec'::text) AS podspec_field,
        count(event_field.event_field) AS hits
        FROM audit_events_by_gvkrv,
             LATERAL
               jsonb_object_keys(audit_events_by_gvkrv.request_object -> 'template'-> 'spec'::text) event_field(event_field)
       WHERE kind = 'PodTemplate'
         AND NOT (lower(api_version) ~~ ANY('{%alpha%, %beta%}'))
       GROUP BY bucket, job, api_group, api_version, kind, event_verb, resource, sub_resource, test, useragent, podspec_field
            UNION
      SELECT DISTINCT
        bucket,
        job,
        api_group,
        api_version,
        kind,
        event_verb,
        resource,
        sub_resource,
        test,
        useragent,
        jsonb_object_keys(request_object -> 'spec' -> 'template' -> 'spec'::text) AS podspec_field,
        count(event_field.event_field) AS hits
        FROM audit_events_by_gvkrv,
             LATERAL
               jsonb_object_keys(audit_events_by_gvkrv.request_object -> 'spec' -> 'template'-> 'spec'::text) event_field(event_field)
       WHERE kind = ANY('{DaemonSet, Deployment, ReplicationController, StatefulSet, Job,ReplicaSet}')
         AND NOT (lower(api_version) ~~ ANY('{%alpha%, %beta%}'))
       GROUP BY bucket, job, api_group, api_version, kind, event_verb, resource, sub_resource, test, useragent, podspec_field; 
   #+END_SRC
  
   #+BEGIN_SRC sql-mode
 select distinct bucket, job from podspec_field_coverage_material;
   #+END_SRC

*** 440: PodSpec Field Coverage View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/440_view_podspec_field_coverage.up.sql
    :END:
 #+NAME: view podspec_field_coverage
 #+BEGIN_SRC sql-mode
 create view podspec_field_coverage as select * from podspec_field_coverage_material;
 #+END_SRC
 
*** 450: PodSpec Field Summary View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/450_view_podspec_field_summary.up.sql
    :END:
 #+NAME: view podspec_field_summary
 #+BEGIN_SRC sql-mode
   create view podspec_field_summary as
     select distinct field_name as podspec_field,
                     0 as other_hits,
                     0 as e2e_hits,
                     0 as conf_hits,
                     bucket,
                     job
       from api_schema_field
      where field_schema like '%PodSpec%'
      UNION
     select
       podspec_field,
       sum(hits) as other_hits,
       0 as e2e_hits,
       0 as conf_hits,
       bucket,
       job
       from podspec_field_coverage
      where useragent not like 'e2e.test%'
      group by podspec_field, bucket, job
      UNION
     select
       podspec_field,
       0 as other_hits,
       sum(hits) as e2e_hits,
       0 as conf_hits,
       bucket,
       job
       from podspec_field_coverage
      where useragent like 'e2e.test%'
        and test not like '%Conformance%'
      group by podspec_field, bucket, job
      UNION
     select
       podspec_field,
       0 as other_hits,
       0 as e2e_hits,
       sum(hits) as conf_hits,
       bucket,
       job
       from podspec_field_coverage
      where useragent like 'e2e.test%'
        and test like '%Conformance%'
      group by podspec_field, bucket, job;
 #+END_SRC
*** 460: PodSpec Field mid Report View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/460_view_podspec_field_mid_report.up.sql
    :END:
  #+NAME: podspec_field_mid_report
  #+BEGIN_SRC sql-mode :results silent
    create or replace view podspec_field_mid_report as
    select distinct podspec_field,
          sum(other_hits) as other_hits,
          sum(e2e_hits) as e2e_hits,
          sum(conf_hits) as conf_hits,
          kfp.release,
          kfp.deprecated,
          kfp.gated,
          kfp.required,
          kfp.field_kind,
          kfp.field_type,
          pfs.bucket, 
          pfs.job
    from podspec_field_summary pfs, kind_field_path_recursion kfp
    where 
      kfp.kind = 'io.k8s.api.core.v1.PodSpec'
      and pfs.podspec_field = kfp.field_path
    group by podspec_field, kfp.release, kfp.deprecated, kfp.gated, kfp.required, kfp.field_kind, kfp.field_type, pfs.bucket, pfs.job
    order by conf_hits, e2e_hits, other_hits;
  #+END_SRC

*** 470: PodSpec Field Report View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/470_view_podspec_field_report.up.sql
    :END:
 #+NAME: podspec_field_hits
 #+BEGIN_SRC sql-mode
   create or replace view podspec_field_report as
   select distinct podspec_field,
         sum(other_hits) as other_hits,
         sum(e2e_hits) as e2e_hits,
         sum(conf_hits) as conf_hits,
         release,
         deprecated,
         gated,
         required,
         field_kind,
         field_type,
         bucket,
         job
   from podspec_field_mid_report
   group by podspec_field, release, deprecated, gated, required, field_kind, field_type, bucket, job
   order by conf_hits, e2e_hits, other_hits;
 #+END_SRC
 
 #+BEGIN_SRC sql-mode :results replace drawer
   select
     podspec_field, e2e_hits, pfr.job, bjs.job_timestamp
     from podspec_field_report pfr
     JOIN bucket_job_swagger bjs on(bjs.bucket = pfr.bucket AND bjs.job = pfr.job) 
    order by podspec_field;
 #+END_SRC

 #+RESULTS:
 :results:
          podspec_field         | e2e_hits |         job         |    job_timestamp    
 -------------------------------+----------+---------------------+---------------------
  activeDeadlineSeconds         |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  activeDeadlineSeconds         |        0 | live                | 2019-12-04 20:14:50
  affinity                      |        0 | live                | 2019-12-04 20:14:50
  affinity                      |     2264 | 1202311785298792448 | 2019-12-04 20:14:50
  automountServiceAccountToken  |      184 | 1202311785298792448 | 2019-12-04 20:14:50
  automountServiceAccountToken  |        0 | live                | 2019-12-04 20:14:50
  containers                    |        0 | live                | 2019-12-04 20:14:50
  containers                    |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  dnsConfig                     |        0 | live                | 2019-12-04 20:14:50
  dnsConfig                     |       32 | 1202311785298792448 | 2019-12-04 20:14:50
  dnsPolicy                     |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  dnsPolicy                     |        0 | live                | 2019-12-04 20:14:50
  enableServiceLinks            |    26592 | 1202311785298792448 | 2019-12-04 20:14:50
  enableServiceLinks            |        0 | live                | 2019-12-04 20:14:50
  ephemeralContainers           |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  ephemeralContainers           |        0 | live                | 2019-12-04 20:14:50
  hostAliases                   |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  hostAliases                   |        0 | live                | 2019-12-04 20:14:50
  hostIPC                       |        0 | live                | 2019-12-04 20:14:50
  hostIPC                       |       64 | 1202311785298792448 | 2019-12-04 20:14:50
  hostname                      |      260 | 1202311785298792448 | 2019-12-04 20:14:50
  hostname                      |        0 | live                | 2019-12-04 20:14:50
  hostNetwork                   |     6296 | 1202311785298792448 | 2019-12-04 20:14:50
  hostNetwork                   |        0 | live                | 2019-12-04 20:14:50
  hostPID                       |        0 | live                | 2019-12-04 20:14:50
  hostPID                       |       64 | 1202311785298792448 | 2019-12-04 20:14:50
  imagePullSecrets              |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  imagePullSecrets              |        0 | live                | 2019-12-04 20:14:50
  initContainers                |     3944 | 1202311785298792448 | 2019-12-04 20:14:50
  initContainers                |        0 | live                | 2019-12-04 20:14:50
  nodeName                      |    18476 | 1202311785298792448 | 2019-12-04 20:14:50
  nodeName                      |        0 | live                | 2019-12-04 20:14:50
  nodeSelector                  |     2252 | 1202311785298792448 | 2019-12-04 20:14:50
  nodeSelector                  |        0 | live                | 2019-12-04 20:14:50
  overhead                      |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  overhead                      |        0 | live                | 2019-12-04 20:14:50
  preemptionPolicy              |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  preemptionPolicy              |        0 | live                | 2019-12-04 20:14:50
  priority                      |      180 | 1202311785298792448 | 2019-12-04 20:14:50
  priority                      |        0 | live                | 2019-12-04 20:14:50
  priorityClassName             |        0 | live                | 2019-12-04 20:14:50
  priorityClassName             |      128 | 1202311785298792448 | 2019-12-04 20:14:50
  readinessGates                |        0 | live                | 2019-12-04 20:14:50
  readinessGates                |       32 | 1202311785298792448 | 2019-12-04 20:14:50
  restartPolicy                 |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  restartPolicy                 |        0 | live                | 2019-12-04 20:14:50
  runtimeClassName              |        0 | live                | 2019-12-04 20:14:50
  runtimeClassName              |      184 | 1202311785298792448 | 2019-12-04 20:14:50
  schedulerName                 |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  schedulerName                 |        0 | live                | 2019-12-04 20:14:50
  securityContext               |        0 | live                | 2019-12-04 20:14:50
  securityContext               |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  serviceAccount                |     5244 | 1202311785298792448 | 2019-12-04 20:14:50
  serviceAccount                |        0 | live                | 2019-12-04 20:14:50
  serviceAccountName            |     5244 | 1202311785298792448 | 2019-12-04 20:14:50
  serviceAccountName            |        0 | live                | 2019-12-04 20:14:50
  shareProcessNamespace         |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  shareProcessNamespace         |        0 | live                | 2019-12-04 20:14:50
  subdomain                     |        0 | live                | 2019-12-04 20:14:50
  subdomain                     |      260 | 1202311785298792448 | 2019-12-04 20:14:50
  terminationGracePeriodSeconds |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  terminationGracePeriodSeconds |        0 | live                | 2019-12-04 20:14:50
  tolerations                   |      180 | 1202311785298792448 | 2019-12-04 20:14:50
  tolerations                   |        0 | live                | 2019-12-04 20:14:50
  topologySpreadConstraints     |        0 | live                | 2019-12-04 20:14:50
  topologySpreadConstraints     |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  volumes                       |    27044 | 1202311785298792448 | 2019-12-04 20:14:50
  volumes                       |        0 | live                | 2019-12-04 20:14:50
 (68 rows)

 :end:

*** 480: materialized kind_field_path_coverage
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/480_kind_field_path_coverage_material.up.sql
    :END:
    This is the base view we use to traverse the paths later.  It grabs all relevant fields from our kind_field_path_recursion and joins it to our audit_events based on where the request_object of the event includes the relevant fieldpath.
   
    #+NAME: kind_field_path_coverage_material_improved
    #+BEGIN_SRC sql-mode
      CREATE MATERIALIZED VIEW "public"."kind_field_path_coverage_material" AS
      SELECT
        kfpr.bucket,
        kfpr.job,
        kfpr.kind,
        kfpr.field_path,
        kfpr.field_kind,
        kfpr.sub_kind,
        (array_length(string_to_array(kfpr.field_path, '.'),1) - 1) as distance,
        ae.audit_id as audit_event_id,
        ae.useragent as useragent,
        ae.operation_id
        FROM kind_field_path_recursion kfpr
            LEFT JOIN LATERAL (select * from audit_event WHERE param_schema = kfpr.kind AND jsonb_path_exists(request_object, ('$.'||kfpr.field_path)::jsonpath)) ae ON true
        GROUP BY kfpr.kind, kfpr.field_path, kfpr.field_kind, kfpr.bucket, kfpr.job, kfpr.sub_kind, ae.audit_id, ae.useragent, ae.operation_id; 
    #+END_SRC
    #+begin_src sql-mode
     refresh materialized view kind_field_path_coverage_material; 
    #+end_src
   
*** 485: kind_field_path_coverage
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/485_kind_field_path_coverage.up.sql
    :END:
    A view into our material,  so hasura can track it.
    #+NAME: kind_field_path_coverage
    #+BEGIN_SRC sql-mode
      CREATE OR REPLACE VIEW "public"."kind_field_path_coverage" AS
       select * from kind_field_path_coverage_material;
    #+END_SRC
*** 490: materialized full_podspec_field_coverage
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/490_full_podspec_field_coverage_material.up.sql
    :END:
    We want a subset of this grand field_coverage view, looking only for fields that come from Podspec.    
    This is going to look across all our buckets and jobs, so it will take a bit of time to materialize.
   
    We are only looking at the stable, core kinds or the GA kinds.
   
    #+NAME: full_podspec_field_coverage_material
    #+BEGIN_SRC sql-mode
      CREATE MATERIALIZED VIEW "public"."full_podspec_field_coverage_material" AS
        WITH podspec_kinds AS (
              SELECT DISTINCT kind, field_path
                FROM kind_field_path_coverage
                 WHERE field_kind = 'io.k8s.api.core.v1.PodSpec'
                 AND kind not like '%alpha%'
                 AND kind not like '%beta%'
                 AND operation_id is not null
        )
        SELECT DISTINCT
          trim(leading 'io.k8s.api.' from c.kind) as kind,
          trim(leading 'io.k8s.api.' from c.sub_kind) as  sub_kind,
          c.field_path,
          distance,
          count(*) FILTER(WHERE c.useragent like 'e2e.test%') as test_hits,
          count(*) FILTER(WHERE c.useragent like '%[Conformance]%') as conf_hits,
          c.field_kind,
          c.job,
          c.bucket
          FROM kind_field_path_coverage c
            INNER JOIN podspec_kinds pk ON (c.kind = pk.kind AND c.field_path like  pk.field_path || '%')
            and sub_kind not like '%VolumeSource'
            GROUP BY c.sub_kind, c.kind, c.field_path, c.field_kind, c.distance, pk.field_path, c.job, c.bucket
            ORDER BY field_path;
    #+END_SRC

    #+begin_src sql-mode
    drop materialized view full_podspec_field_coverage_material cascade;

    #+end_src
*** 495: full_podspec_field_coverage
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/495_full_podspec_field_coverage.up.sql
    :END:
    And we can create a view from this
    #+NAME: full_podspec_field_coverage
    #+BEGIN_SRC sql-mode
     CREATE OR REPLACE VIEW "public"."full_podspec_field_coverage" AS
      select * from full_podspec_field_coverage_material;
    #+END_SRC
   
   
    When using the view, you will want ot make sure to limit it by a job, otherwise you'll get massive results.
   
    for example


     #+begin_src sql-mode :results replace code :wrap EXAMPLE
     \d+ full_podspec_field_coverage;
     #+end_src

     #+RESULTS:
     #+begin_EXAMPLE
                        View "public.full_podspec_field_coverage"
        Column   |  Type   | Collation | Nullable | Default | Storage  | Description 
     ------------+---------+-----------+----------+---------+----------+-------------
      kind       | text    |           |          |         | extended | 
      sub_kind   | text    |           |          |         | extended | 
      field_path | text    |           |          |         | extended | 
      distance   | integer |           |          |         | plain    | 
      test_hits  | bigint  |           |          |         | plain    | 
      conf_hits  | bigint  |           |          |         | plain    | 
      field_kind | text    |           |          |         | extended | 
      job        | text    |           |          |         | extended | 
      bucket     | text    |           |          |         | extended | 
     View definition:
      SELECT full_podspec_field_coverage_material.kind,
         full_podspec_field_coverage_material.sub_kind,
         full_podspec_field_coverage_material.field_path,
         full_podspec_field_coverage_material.distance,
         full_podspec_field_coverage_material.test_hits,
         full_podspec_field_coverage_material.conf_hits,
         full_podspec_field_coverage_material.field_kind,
         full_podspec_field_coverage_material.job,
         full_podspec_field_coverage_material.bucket
        FROM full_podspec_field_coverage_material;

     #+end_EXAMPLE
    #+name: kind_field_coverage_nolive
    #+begin_src sql-mode
      CREATE OR REPLACE VIEW "public"."kind_field_path_coverage" AS
       select * from kind_field_path_coverage_material where job != 'live';
      refresh materialized view kind_field_path_coverage_material; 
    #+end_src
    #+name: full_podspec_field_coverage_nolive
    #+begin_src sql-mode
      CREATE OR REPLACE VIEW "public"."full_podspec_field_path_coverage" AS
       select * from kind_field_path_coverage_material where job != 'live';
      refresh materialized view full_podspec_field_path_coverage_material; 
    #+end_src

   
** for aaron                                                         :export:
    #+begin_src sql-mode :results replace :tangle no :eval never-export :exports both :file results.txt
    select kind, sub_kind, field_path, test_hits, distance from full_podspec_field_coverage where job != 'live';
    #+end_src

    #+RESULTS:
    #+begin_src sql-mode
    [[file:results.txt]]
    #+end_src

** ASKS
*** kindnet-image pull.... requires internet makes sad
*** remove latest tag on auditlogger, replace with date
*** auditlogger depend on hasura (similar to hasura -> pg)
*** :eval ask for kind cluster delete
    or move to code block eval :never or move to own block
*** put the kind image pull stuff in footnotes
 with not to look at [[#footnotes]]
*** fix namespace stuf... maybe use default instead of kube-system
*** kindnetd old
   #+BEGIN_SRC tmate :eval never-export
     kind load image-archive            kindnetd:aa67fec7d7ef7.docker-image \
       || docker pull docker.io/kindest/kindnetd:aa67fec7d7ef7 \
       && docker save docker.io/kindest/kindnetd:aa67fec7d7ef7 -o kindnetd:aa67fec7d7ef7.docker-image
   #+END_SRC
*** kind load via registry
   #+BEGIN_SRC tmate :eval never-export
     # Seems a bit slow... loads from image-archives are much faster
     # kind load docker-image --name=kind-$USER raiinbow/hasura:2019-12-03-16-31 
     # kind load docker-image --name=kind-$USER raiinbow/postgres:2019-12-03-14-19
     # kind load docker-image --name=kind-$USER raiinbow/auditlogger:latest
   #+END_SRC


# Local Variables:
# ii: enabled
# End:

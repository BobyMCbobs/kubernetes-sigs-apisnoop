# -*- ii: apisnoop; -*-
#+TITLE: Mock Ticket Template
#+AUTHOR: ii team
#+TODO: TODO(t) NEXT(n) IN-PROGRESS(i) BLOCKED(b) | DONE(d)
#+OPTIONS: toc:nil tags:nil todo:nil
#+EXPORT_SELECT_TAGS: export
* TODO [77%] In-Cluster Setup                                   :neverexport:
  :PROPERTIES:
  :LOGGING:  nil
  :END:
** DONE Connect demo to right eye

   #+begin_src tmate :session foo:hello :eval never-export
     echo "What parts of Kubernetes do you depend on $USER?"
   #+end_src

** DONE Deploy APISnoop DB/GraphQL                                   :export:
   
   #+begin_src shell :eval never-export :wrap "SRC text"
     kubectl apply -f ~/apisnoop/deployment/k8s/graphql.yaml
   #+end_src

   #+RESULTS:
   #+begin_SRC text
   service/hasura created
   service/postgres created
   deployment.apps/hasura created
   deployment.apps/postgres created
   #+end_SRC
** NEXT Deploy APISnoop AuditSink                                    :export:
   
   #+begin_src shell :eval never-export :wrap "SRC text"
     kubectl apply -f ~/apisnoop/deployment/k8s/audit-sink.yaml
   #+end_src

   #+RESULTS:
   #+begin_SRC text
   deployment.apps/apisnoop-auditlogger created
   service/apisnoop-auditlogger created
   auditsink.auditregistration.k8s.io/auditlogger created
   #+end_SRC

** DONE Verify Pods Running
   #+begin_src shell
     kubectl get pods
   #+end_src

   #+RESULTS:
   #+begin_example
   NAME                       READY   STATUS    RESTARTS   AGE
   hasura-777765b7d4-gzc82    1/1     Running   1          109m
   hhii                       1/1     Running   0          46h
   ii10macs                   1/1     Running   0          5h1m
   ii8macs                    1/1     Running   0          5h34m
   iimacs                     1/1     Running   2          2d4h
   k8macs                     1/1     Running   1          2d2h
   k9macs                     1/1     Running   0          46h
   kmacs12                    1/1     Running   0          128m
   orgmacs                    1/1     Running   3          2d15h
   postgres-b59f6c9c4-zt24s   1/1     Running   0          109m
   #+end_example
** DONE Setup Port-Forwarding for UI into the cluster

   We'll setup port-forwarding for postgres, to let us easily send queries from within our org file.
   You can check the status of the port-forward in your right eye.

   Then we'll setup a port-forward for hasura, so our web app can query it directly.
   #+BEGIN_SRC tmate :eval never-export :session foo:hasura
     HASURA_POD=$(kubectl get pod --selector=io.apisnoop.graphql=hasura -o name | sed s:pod/::)
     HASURA_PORT=$(kubectl get pod $HASURA_POD --template='{{(index (index .spec.containers 0).ports 0).containerPort}}{{"\n"}}')
     kubectl port-forward $HASURA_POD --address 0.0.0.0 8080:$HASURA_PORT
   #+END_SRC
** DONE Check it all worked

   #+begin_src sql-mode :results replace
     \d+
   #+end_src

   #+RESULTS:
   #+begin_SRC example
                                                                              List of relations
    Schema |               Name               |       Type        |  Owner   |  Size   |                                    Description                                    
   --------+----------------------------------+-------------------+----------+---------+-----------------------------------------------------------------------------------
    public | api_operation_material           | materialized view | apisnoop | 3688 kB | details on each operation_id as taken from the openAPI spec
    public | api_operation_parameter_material | materialized view | apisnoop | 6016 kB | the parameters for each operation_id in open API spec
    public | audit_event                      | view              | apisnoop | 0 bytes | a record for each audit event in an audit log
    public | bucket_job_swagger               | table             | apisnoop | 3712 kB | metadata for audit events  and their respective swagger.json
    public | endpoint_coverage                | view              | apisnoop | 0 bytes | the test hits and conformance test hits per operation_id & other useful details
    public | endpoint_coverage_material       | materialized view | apisnoop | 296 kB  | 
    public | endpoints_hit_by_new_test        | view              | apisnoop | 0 bytes | list endpoints hit during our live auditing alongside their current test coverage
    public | projected_change_in_coverage     | view              | apisnoop | 0 bytes | overview of coverage stats if the e2e suite included your tests
    public | raw_audit_event                  | table             | apisnoop | 410 MB  | a record for each audit event in an audit log
    public | stable_endpoint_stats            | view              | apisnoop | 0 bytes | coverage stats for entire test run, looking only at its stable endpoints
    public | untested_stable_core_endpoints   | view              | apisnoop | 0 bytes | list stable core endpoints not hit by any tests, according to their test run
   (11 rows)

   #+end_SRC
** DONE Check current coverage
   #+NAME: stable endpoint stats
   #+begin_src sql-mode
     select * from stable_endpoint_stats where job != 'live';
   #+end_src

   #+RESULTS: stable endpoint stats
   #+begin_SRC example
            job         |    date    | total_endpoints | test_hits | conf_hits | percent_tested | percent_conf_tested 
   ---------------------+------------+-----------------+-----------+-----------+----------------+---------------------
    1201280603970867200 | 2019-12-01 |             438 |       183 |       129 |          41.78 |               29.45
   (1 row)

   #+end_SRC

* Identify an untested feature Using APISnoop                        :export:

According to this APIsnoop query, there are still some remaining ConfigMap endpoints which are untested.

  #+NAME: untested_stable_core_endpoints
  #+begin_src sql-mode :eval never-export :exports both :session none
    SELECT
      operation_id,
      -- k8s_action,
      path,
      description
      FROM untested_stable_core_endpoints
      where path not like '%volume%'
      -- and operation_id ilike '%%'
     ORDER BY operation_id desc
     LIMIT 25
           ;
  #+end_src

  #+RESULTS: untested_stable_core_endpoints
  #+begin_SRC example
                      operation_id                    |                                path                                 |                      description                       
  ----------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------
   replaceCoreV1NamespaceStatus                       | /api/v1/namespaces/{name}/status                                    | replace status of the specified Namespace
   replaceCoreV1NamespaceFinalize                     | /api/v1/namespaces/{name}/finalize                                  | replace finalize of the specified Namespace
   replaceCoreV1NamespacedServiceStatus               | /api/v1/namespaces/{namespace}/services/{name}/status               | replace status of the specified Service
   replaceCoreV1NamespacedResourceQuotaStatus         | /api/v1/namespaces/{namespace}/resourcequotas/{name}/status         | replace status of the specified ResourceQuota
   replaceCoreV1NamespacedReplicationControllerStatus | /api/v1/namespaces/{namespace}/replicationcontrollers/{name}/status | replace status of the specified ReplicationController
   replaceCoreV1NamespacedPodTemplate                 | /api/v1/namespaces/{namespace}/podtemplates/{name}                  | replace the specified PodTemplate
   replaceCoreV1NamespacedPodStatus                   | /api/v1/namespaces/{namespace}/pods/{name}/status                   | replace status of the specified Pod
   replaceCoreV1NamespacedEvent                       | /api/v1/namespaces/{namespace}/events/{name}                        | replace the specified Event
   replaceCoreV1NamespacedEndpoints                   | /api/v1/namespaces/{namespace}/endpoints/{name}                     | replace the specified Endpoints
   readCoreV1NodeStatus                               | /api/v1/nodes/{name}/status                                         | read status of the specified Node
   readCoreV1NamespaceStatus                          | /api/v1/namespaces/{name}/status                                    | read status of the specified Namespace
   readCoreV1NamespacedServiceStatus                  | /api/v1/namespaces/{namespace}/services/{name}/status               | read status of the specified Service
   readCoreV1NamespacedResourceQuotaStatus            | /api/v1/namespaces/{namespace}/resourcequotas/{name}/status         | read status of the specified ResourceQuota
   readCoreV1NamespacedReplicationControllerStatus    | /api/v1/namespaces/{namespace}/replicationcontrollers/{name}/status | read status of the specified ReplicationController
   readCoreV1NamespacedPodTemplate                    | /api/v1/namespaces/{namespace}/podtemplates/{name}                  | read the specified PodTemplate
   readCoreV1NamespacedPodStatus                      | /api/v1/namespaces/{namespace}/pods/{name}/status                   | read status of the specified Pod
   readCoreV1NamespacedEvent                          | /api/v1/namespaces/{namespace}/events/{name}                        | read the specified Event
   readCoreV1ComponentStatus                          | /api/v1/componentstatuses/{name}                                    | read the specified ComponentStatus
   patchCoreV1NodeStatus                              | /api/v1/nodes/{name}/status                                         | partially update status of the specified Node
   patchCoreV1NamespaceStatus                         | /api/v1/namespaces/{name}/status                                    | partially update status of the specified Namespace
   patchCoreV1NamespacedServiceStatus                 | /api/v1/namespaces/{namespace}/services/{name}/status               | partially update status of the specified Service
   patchCoreV1NamespacedServiceAccount                | /api/v1/namespaces/{namespace}/serviceaccounts/{name}               | partially update the specified ServiceAccount
   patchCoreV1NamespacedService                       | /api/v1/namespaces/{namespace}/services/{name}                      | partially update the specified Service
   patchCoreV1NamespacedSecret                        | /api/v1/namespaces/{namespace}/secrets/{name}                       | partially update the specified Secret
   patchCoreV1NamespacedResourceQuotaStatus           | /api/v1/namespaces/{namespace}/resourcequotas/{name}/status         | partially update status of the specified ResourceQuota
  (25 rows)

  #+end_SRC


* Use API Reference to Lightly Document the Feature                  :export:
- [[https://kubernetes.io/docs/reference/kubernetes-api/][Kubernetes API Reference Docs]]
- [client-go - MyResourceReplaceMe](https://github.com/kubernetes/client-go/blob/master/kubernetes/typed/core/v1/resourcename.go)

* The mock test                                                      :export:
** Test outline
1. Create a ConfigMap with a static label

2. Patch the ConfigMap with a new Label and updated data

3. Get the ConfigMap to ensure it's patched

4. List all ConfigMaps in all Namespaces
   find the ConfigMap(1)
   ensure that the ConfigMap is found and is patched

5. Delete Namespaced ConfigMap(1) via a Collection with a LabelSelector

** Example in Go
   #+begin_src go  :imports '("fmt" "flag" "os" "k8s.io/apimachinery/pkg/apis/meta/v1" "k8s.io/client-go/kubernetes" "k8s.io/client-go/tools/clientcmd")
     // uses the current context in kubeconfig
     kubeconfig := flag.String("kubeconfig",
       fmt.Sprintf("%v/%v/%v", os.Getenv("HOME"), ".kube", "config"),
       "(optional) absolute path to the kubeconfig file")
     flag.Parse()
     config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig)
     if err != nil {
       fmt.Println(err)
     }
     // make our work easier to find in the audit_event queries
     config.UserAgent = "live-test-pod-count"
     // creates the clientset
     clientset, _ := kubernetes.NewForConfig(config)
     // access the API to list pods
     pods, _ := clientset.CoreV1().Pods("").List(v1.ListOptions{})
     fmt.Printf("There are %d pods in the cluster\n", len(pods.Items))
     #+end_src

     #+RESULTS:
     #+begin_src go
     There are 20 pods in the cluster
     #+end_src

   #+begin_src go
     package main

     import (
       "fmt"
       "flag"
       "os"
       // v1 "k8s.io/api/core/v1"
       // metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
       "k8s.io/client-go/kubernetes"
       // "k8s.io/apimachinery/pkg/types"
       "k8s.io/client-go/tools/clientcmd"
     )

     func main() {
       // uses the current context in kubeconfig
       kubeconfig := flag.String("kubeconfig", fmt.Sprintf("%v/%v/%v", os.Getenv("HOME"), ".kube", "config"), "(optional) absolute path to the kubeconfig file")
       flag.Parse()
       config, err := clientcmd.BuildConfigFromFlags("", *kubeconfig)
       if err != nil {
         fmt.Println(err)
       }
       // make our work easier to find in the audit_event queries
       config.UserAgent = "live-test-writing"
       // creates the clientset
       ClientSet, _ := kubernetes.NewForConfig(config)

       // write test here
       fmt.Println("[status] complete")

     }
   #+end_src

   #+RESULTS:
   #+begin_src go
   #+end_src

* Verify with APISnoop                                               :export:
Discover useragents:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select distinct useragent from audit_event where bucket='apisnoop' and useragent not like 'kube%' and useragent not like 'coredns%' and useragent not like 'kindnetd%' and useragent like 'live%';
  #+end_src

  #+RESULTS:
  #+begin_SRC example
   useragent 
  -----------
  (0 rows)

  #+end_SRC

Display endpoints hit by the test:
#+begin_src sql-mode :exports both :session none
select * from endpoints_hit_by_new_test where useragent like 'live%'; 
#+end_src

Show the change in coverage after the mock test:
  #+begin_src sql-mode :eval never-export :exports both :session none
    select * from projected_change_in_coverage;
  #+end_src

* Final notes :export:
If a test with these calls gets merged, **Conformance coverage will go up by 2 points**

-----  
/sig testing
 
/sig architecture  

/area conformance  


* Open Tasks
  Set any open tasks here, using org-todo
** DONE Live Your Best Life
** Comments
   #+begin_src sql-mode
COMMENT ON TABLE bucket_job_swagger IS 'raw data taken from audit events relevant swagger.json';
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
   COMMENT
   #+end_src
   
   #+begin_src sql-mode
   \d+ 
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
                                                                 List of relations
    Schema |               Name               |       Type        |  Owner   |  Size   |                      Description                       
   --------+----------------------------------+-------------------+----------+---------+--------------------------------------------------------
    public | api_operation_material           | materialized view | apisnoop | 3688 kB | 
    public | api_operation_parameter_material | materialized view | apisnoop | 6016 kB | 
    public | audit_event                      | view              | apisnoop | 0 bytes | 
    public | bucket_job_swagger               | table             | apisnoop | 3712 kB | raw data taken from audit events relevant swagger.json
    public | change_in_coverage               | view              | apisnoop | 0 bytes | 
    public | change_in_tests                  | view              | apisnoop | 0 bytes | 
    public | endpoint_coverage                | view              | apisnoop | 0 bytes | 
    public | endpoints_hit_by_new_test        | view              | apisnoop | 0 bytes | 
    public | projected_change_in_coverage     | view              | apisnoop | 0 bytes | 
    public | raw_audit_event                  | table             | apisnoop | 407 MB  | 
    public | stable_endpoint_stats            | view              | apisnoop | 0 bytes | 
    public | untested_stable_core_endpoints   | view              | apisnoop | 0 bytes | 
   (12 rows)

   #+end_src

* Footnotes :neverexport:
  :PROPERTIES:
  :CUSTOM_ID: footnotes
  :END:
** Load Logs to Help Debug Cluster
   #:PROPERTIES:
   #:header-args:tmate+: :prologue (concat "cd " (file-name-directory buffer-file-name) "../../apisnoop/apps\n. .loadenv\n")
   #:END:
*** hasura logs

    #+BEGIN_SRC tmate :eval never-export :session foo:hasura_logs
      HASURA_POD=$(\
                   kubectl get pod --selector=io.apisnoop.graphql=hasura -o name \
                       | sed s:pod/::)
      kubectl logs $HASURA_POD -f
    #+END_SRC

*** postgres logs

    #+BEGIN_SRC tmate :eval never-export :session foo:postgres_logs
      POSTGRES_POD=$(\
                     kubectl get pod --selector=io.apisnoop.db=postgres -o name \
                         | sed s:pod/::)
      kubectl logs $POSTGRES_POD -f
    #+END_SRC

*** auditlogger logs

    #+BEGIN_SRC tmate :eval never-export :session foo:postgres_logs
      AUDITLOGGER_POD=$(\
                     kubectl get pod --selector=app=apisnoop-auditlogger -o name \
                         | sed s:pod/::)
      kubectl logs $AUDITLOGGER_POD -f
    #+END_SRC

** Manually load swagger or audit events
   If you ran through the full setup, but were getting 0's in the stable_endpint_stats, it means the table migrations were successful, but no data was loaded.

   You can verify data loaded with the below query.  ~bucket_job_swagger~ should have a size around 3600kb and raw_audit_event should have a size around 412mb.

   #+NAME: Verify Data Loaded
   #+begin_src sql-mode
     \dt+
   #+end_src

   #+RESULTS:
   #+begin_src sql-mode
     List of relations
       Schema |        Name        | Type  |  Owner   |  Size   | Description
       --------+--------------------+-------+----------+---------+-------------
       public | bucket_job_swagger | table | apisnoop | 3600 kB |
       public | raw_audit_event    | table | apisnoop | 412 MB  |
       (2 rows)

   #+end_src

   If either shows a size of ~8192 bytes~, you'll want to manually load it, refresh materialized views, then check again.

   if you want to load a particular bucket or job, you can name them as the first and second argument of these functions.
   e.g
   : select * from load)swagger('ci-kubernetes-beta', 1122334344);
   will load that specific bucket/job combo.
   : select * from load_swagger('ci-kubernetes-beta');
   will load the latest successful test run for ~ci-kubernetes-beta~
   : select * from load_swagger('ci-kubernetes-beta', null, true);
   will load the latest successful test run for ~ci-kubernetes-beta~, but with bucket and job set to 'apisnoop/live' (used for testing).
   #+NAME: Manually load swaggers
   #+begin_src sql-mode
     select * from load_swagger();
     select * from load_swagger(null, null, true);
   #+end_src

   #+NAME: Manually load audit events
   #+begin_src sql-mode
     select * from load_audit_events();
   #+end_src

   #+NAME: Refresh Materialized Views
   #+begin_src sql-mode
     REFRESH MATERIALIZED VIEW api_operation_material;
     REFRESH MATERIALIZED VIEW api_operation_parameter_material;
   #+end_src
** 200: stuff
*** 250: api_schema view
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/250_view_api_schema.up.sql
    :END:
**** Create

  #+NAME: api_schema view
  #+BEGIN_SRC sql-mode 
    CREATE OR REPLACE VIEW "public"."api_schema" AS 
     SELECT 
        bjs.bucket,
        bjs.job,
        d.key AS schema_name,
        (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'kind'::text) AS k8s_kind,
        (d.value ->> 'type'::text) AS resource_type,
        (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'version'::text) AS k8s_version,
        (((d.value -> 'x-kubernetes-group-version-kind'::text) -> 0) ->> 'group'::text) AS k8s_group,
        ARRAY(SELECT jsonb_array_elements_text(d.value -> 'required')) as required_fields,
        (d.value -> 'properties'::text) AS properties,
        d.value
       FROM bucket_job_swagger bjs
         , jsonb_each((bjs.swagger -> 'definitions'::text)) d(key, value)
       GROUP BY bjs.bucket, bjs.job, d.key, d.value;

  #+END_SRC

  #+RESULTS: api_schema view
  #+begin_src sql-mode
  CREATE VIEW
  #+end_src

*** 260: api_schema_field view
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/260_view_api_schema_field.up.sql
    :END:
**** Create
 #+NAME: api_schema_field view
 #+BEGIN_SRC sql-mode 
   CREATE OR REPLACE VIEW "public"."api_schema_field" AS 
     SELECT api_schema.schema_name as field_schema,
            d.key AS field_name,
            replace(
              CASE
              WHEN d.value->>'type' = 'string' THEN 'string'
              WHEN d.value->>'type' IS NULL THEN d.value->>'$ref'
              WHEN d.value->>'type' = 'array'
               AND d.value->'items'->> 'type' IS NULL
                THEN d.value->'items'->>'$ref'
              WHEN d.value->>'type' = 'array'
               AND d.value->'items'->>'$ref' IS NULL
                THEN d.value->'items'->>'type'
              ELSE 'integer'::text
              END, '#/definitions/','') AS field_kind,
            CASE
            WHEN d.value->>'type' IS NULL THEN 'subtype'
            ELSE d.value->>'type'
              END AS field_type,
            d.value->>'description' AS description,
            CASE
            WHEN d.key = ANY(api_schema.required_fields) THEN true
            ELSE false
              END AS required,
            CASE
            WHEN (   d.value->>'description' ilike '%This field is alpha-level%'
                  or d.value->>'description' ilike '%This is an alpha field%'
                  or d.value->>'description' ilike '%This is an alpha feature%') THEN 'alpha'
            WHEN (   d.value->>'description' ilike '%This field is beta-level%'
                  or d.value->>'description' ilike '%This field is beta%'
                  or d.value->>'description' ilike '%This is a beta feature%'
                  or d.value->>'description' ilike '%This is an beta feature%'
                  or d.value->>'description' ilike '%This is an beta field%') THEN 'beta'
            ELSE 'ga'
              END AS release,
            CASE
            WHEN  d.value->>'description' ilike '%deprecated%' THEN true
             ELSE false
             END AS deprecated,
            CASE
            WHEN ( d.value->>'description' ilike '%requires the % feature gate to be enabled%'
                  or d.value->>'description' ilike '%depends on the % feature gate being enabled%'
                  or d.value->>'description' ilike '%requires the % feature flag to be enabled%'
                  or d.value->>'description' ilike '%honored if the API server enables the % feature gate%'
                  or d.value->>'description' ilike '%honored by servers that enable the % feature%'
                  or d.value->>'description' ilike '%requires enabling % feature gate%'
                  or d.value->>'description' ilike '%honored by clusters that enables the % feature%'
                  or d.value->>'description' ilike '%only if the % feature gate is enabled%'
                  ) THEN true
            ELSE false
              END AS feature_gated,
            d.value->>'format' AS format,
            d.value->>'x-kubernetes-patch-merge-key' AS merge_key,
            d.value->>'x-kubernetes-patch-strategy' AS patch_strategy,
            api_schema.bucket,
            api_schema.job,
            d.value
       FROM (api_schema
             JOIN LATERAL jsonb_each(api_schema.properties) d(key, value) ON (true));
 #+END_SRC

 #+RESULTS: api_schema_field view
 #+begin_src sql-mode
 CREATE VIEW
 #+end_src

** 300: grkrv

*** 310: Audit Events By GVKRV(Group, Version, Kind, Resource(s),Verb)
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/310_view_audit_event_by_gvkrv.up.sql
    :END:
  
   This is a slim view, and will need to be updated to contain all useful info if/when we phase out operationID across reports.
     #+NAME: events by gvkrv
     #+BEGIN_SRC sql-mode :results silent
       CREATE OR REPLACE VIEW "public"."audit_events_by_gvkrv" AS
         SELECT
           CASE
           WHEN ((a.data -> 'objectRef' ->> 'apiGroup') IS NULL) THEN ''
           ELSE (a.data -> 'objectRef' ->> 'apiGroup')
                 END as api_group,
           (a.data -> 'objectRef' ->>'apiVersion') as api_version,
           (a.data -> 'requestObject'->>'kind') as kind,
           a.param_schema as body_schema,
           (a.data -> 'objectRef'->>'resource') as resource,
             (a.data -> 'objectRef'->>'subresource') as sub_resource,
           (a.data->>'verb') as event_verb,
           operation_id,
           audit_id,
           split_part(a.useragent, '--', 2) as test,
           split_part(a.useragent, '--', 1) as useragent,
           (a.data -> 'requestObject') as request_object,
           bucket,
           job
           FROM audit_event as a
          where data->'requestObject' is not null;
     #+END_SRC
  
** 400: Podspec Field Views
   :PROPERTIES:
   :header-args:sql-mode+: :results silent
   :END:
*** 400: kind_field_path_recursion
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/400_view_kind_field_recursion.up.sql
    :END:
 #+NAME: Recursive kind_field_path view
 #+BEGIN_SRC sql-mode
   create or replace recursive view kind_field_path_recursion(
     kind,
     field_path,
     field_kind,
     field_type,
     sub_kind,
     release,
     deprecated,
     gated,
     required,
     bucket,
     job
   ) AS
    SELECT DISTINCT
    sf.field_schema AS kind,
    sf.field_name AS field_path, -- this becomes a path
    sf.field_kind AS field_kind,
    sf.field_type AS field_type,
    sf.field_schema AS sub_kind, -- this is the kind at this level
    sf.release AS release,
    sf.deprecated AS deprecated, 
    sf.feature_gated AS feature_gated,
    sf.required AS required,
    sf.bucket as bucket,
    sf.job as job
    from api_schema_field sf
    UNION
    SELECT
     kfpr.kind AS kind,
     ( kfpr.field_path || '.' || f.field_name ) AS field_path,
     f.field_kind AS field_kind,
     f.field_type AS field_type,
     CASE
     WHEN f.field_kind = 'string' OR f.field_kind = 'integer' THEN f.field_schema
     ELSE f.field_kind
      END as sub_kind,
     f.release AS release,
     f.deprecated AS deprecated,
     f.feature_gated AS feature_gated,
     f.required AS required,
     kfpr.bucket,
     kfpr.job
     FROM api_schema_field f
     INNER JOIN kind_field_path_recursion kfpr ON
     f.field_schema = kfpr.field_kind
     AND f.field_kind not like 'io.k8s.apiextensions-apiserver.pkg.apis.apiextensions.%.JSONSchemaProps';
   ;
 #+END_SRC
*** 410: kind_field_path_material
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/410_view_kind_field_path_material.up.sql
    :END:
 #+NAME: kind_field_path material
 #+BEGIN_SRC sql-mode
    create materialized view kind_field_path_material AS
    select
      kind,
      field_path AS field_path,
      field_kind AS field_kind,
      field_type,
      sub_kind,
      release,
      deprecated,
      gated,
      required,
      bucket,
      job
     from kind_field_path_recursion;
   -- drop materialized view kind_field_path_material cascade;
 #+END_SRC
**** kind_field_path_material indexes
 #+NAME: kind_field_path_material indexs
 #+BEGIN_SRC sql-mode
 CREATE INDEX kfpm_kind_idx       ON kind_field_path_material (kind);
 CREATE INDEX kfpm_field_path_idx ON kind_field_path_material (field_path);
 CREATE INDEX kfpm_field_type_idx ON kind_field_path_material (field_type);
 CREATE INDEX kfpm_sub_kind_idx   ON kind_field_path_material (sub_kind);
 -- GIST requires ltree
 -- CREATE INDEX kfpm_kind_idx       ON kind_field_path_material USING GIST (kind);
 -- CREATE INDEX kfpm_field_path_idx ON kind_field_path_material USING GIST (field_path);
 -- CREATE INDEX kfpm_field_type_idx ON kind_field_type_material USING GIST (field_type);
 -- CREATE INDEX kfpm_sub_kind_idx   ON kind_field_path_material USING GIST (sub_kind);
 #+END_SRC

*** 420: kind_field_path view
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/420_view_kind_field_path.up.sql
    :END:
 #+NAME: kind_field_path view
 #+BEGIN_SRC sql-mode
   create or replace view kind_field_path AS
   select
     kind,
     field_path,
     field_kind,
     field_type,
     sub_kind,
     release,
     deprecated,
     gated,
     required,
     bucket,
     job
    from kind_field_path_material where field_kind not like 'io%';
 #+END_SRC

*** 430: PodSpec Materialized View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/430_podspec_field_coverage_material.up.sql
    :END:
    
    #+NAME: view podspec_field_coverage_material
    #+BEGIN_SRC sql-mode :results silent
      CREATE MATERIALIZED VIEW "public"."podspec_field_coverage_material" AS 
      SELECT DISTINCT
        bucket,
        job,
        api_group,
        api_version,
        kind,
        event_verb,
        resource,
        sub_resource,
        test,
        useragent,
        jsonb_object_keys(request_object -> 'spec'::text) AS podspec_field,
        count(event_field.event_field) AS hits
        FROM audit_events_by_gvkrv,
             LATERAL
               jsonb_object_keys(audit_events_by_gvkrv.request_object -> 'spec'::text) event_field(event_field)
       WHERE kind = 'Pod'
         AND NOT (lower(api_version) ~~ ANY('{%alpha%, %beta%}')) -- api_version doesn't contain alpha or beta;
       GROUP BY bucket, job, api_group, api_version, kind, event_verb, resource, sub_resource, test, useragent, podspec_field
            UNION
      SELECT DISTINCT
        bucket,
        job,
        api_group,
        api_version,
        kind,
        event_verb,
        resource,
        sub_resource,
        test,
        useragent,
        jsonb_object_keys(request_object -> 'template' -> 'spec'::text) AS podspec_field,
        count(event_field.event_field) AS hits
        FROM audit_events_by_gvkrv,
             LATERAL
               jsonb_object_keys(audit_events_by_gvkrv.request_object -> 'template'-> 'spec'::text) event_field(event_field)
       WHERE kind = 'PodTemplate'
         AND NOT (lower(api_version) ~~ ANY('{%alpha%, %beta%}'))
       GROUP BY bucket, job, api_group, api_version, kind, event_verb, resource, sub_resource, test, useragent, podspec_field
            UNION
      SELECT DISTINCT
        bucket,
        job,
        api_group,
        api_version,
        kind,
        event_verb,
        resource,
        sub_resource,
        test,
        useragent,
        jsonb_object_keys(request_object -> 'spec' -> 'template' -> 'spec'::text) AS podspec_field,
        count(event_field.event_field) AS hits
        FROM audit_events_by_gvkrv,
             LATERAL
               jsonb_object_keys(audit_events_by_gvkrv.request_object -> 'spec' -> 'template'-> 'spec'::text) event_field(event_field)
       WHERE kind = ANY('{DaemonSet, Deployment, ReplicationController, StatefulSet, Job,ReplicaSet}')
         AND NOT (lower(api_version) ~~ ANY('{%alpha%, %beta%}'))
       GROUP BY bucket, job, api_group, api_version, kind, event_verb, resource, sub_resource, test, useragent, podspec_field; 
   #+END_SRC
  
   #+BEGIN_SRC sql-mode
 select distinct bucket, job from podspec_field_coverage_material;
   #+END_SRC

*** 440: PodSpec Field Coverage View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/440_view_podspec_field_coverage.up.sql
    :END:
 #+NAME: view podspec_field_coverage
 #+BEGIN_SRC sql-mode
 create view podspec_field_coverage as select * from podspec_field_coverage_material;
 #+END_SRC
 
*** 450: PodSpec Field Summary View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/450_view_podspec_field_summary.up.sql
    :END:
 #+NAME: view podspec_field_summary
 #+BEGIN_SRC sql-mode
   create view podspec_field_summary as
     select distinct field_name as podspec_field,
                     0 as other_hits,
                     0 as e2e_hits,
                     0 as conf_hits,
                     bucket,
                     job
       from api_schema_field
      where field_schema like '%PodSpec%'
      UNION
     select
       podspec_field,
       sum(hits) as other_hits,
       0 as e2e_hits,
       0 as conf_hits,
       bucket,
       job
       from podspec_field_coverage
      where useragent not like 'e2e.test%'
      group by podspec_field, bucket, job
      UNION
     select
       podspec_field,
       0 as other_hits,
       sum(hits) as e2e_hits,
       0 as conf_hits,
       bucket,
       job
       from podspec_field_coverage
      where useragent like 'e2e.test%'
        and test not like '%Conformance%'
      group by podspec_field, bucket, job
      UNION
     select
       podspec_field,
       0 as other_hits,
       0 as e2e_hits,
       sum(hits) as conf_hits,
       bucket,
       job
       from podspec_field_coverage
      where useragent like 'e2e.test%'
        and test like '%Conformance%'
      group by podspec_field, bucket, job;
 #+END_SRC
*** 460: PodSpec Field mid Report View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/460_view_podspec_field_mid_report.up.sql
    :END:
  #+NAME: podspec_field_mid_report
  #+BEGIN_SRC sql-mode :results silent
    create or replace view podspec_field_mid_report as
    select distinct podspec_field,
          sum(other_hits) as other_hits,
          sum(e2e_hits) as e2e_hits,
          sum(conf_hits) as conf_hits,
          kfp.release,
          kfp.deprecated,
          kfp.gated,
          kfp.required,
          kfp.field_kind,
          kfp.field_type,
          pfs.bucket, 
          pfs.job
    from podspec_field_summary pfs, kind_field_path_recursion kfp
    where 
      kfp.kind = 'io.k8s.api.core.v1.PodSpec'
      and pfs.podspec_field = kfp.field_path
    group by podspec_field, kfp.release, kfp.deprecated, kfp.gated, kfp.required, kfp.field_kind, kfp.field_type, pfs.bucket, pfs.job
    order by conf_hits, e2e_hits, other_hits;
  #+END_SRC

*** 470: PodSpec Field Report View
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/470_view_podspec_field_report.up.sql
    :END:
 #+NAME: podspec_field_hits
 #+BEGIN_SRC sql-mode
   create or replace view podspec_field_report as
   select distinct podspec_field,
         sum(other_hits) as other_hits,
         sum(e2e_hits) as e2e_hits,
         sum(conf_hits) as conf_hits,
         release,
         deprecated,
         gated,
         required,
         field_kind,
         field_type,
         bucket,
         job
   from podspec_field_mid_report
   group by podspec_field, release, deprecated, gated, required, field_kind, field_type, bucket, job
   order by conf_hits, e2e_hits, other_hits;
 #+END_SRC
 
 #+BEGIN_SRC sql-mode :results replace drawer
   select
     podspec_field, e2e_hits, pfr.job, bjs.job_timestamp
     from podspec_field_report pfr
     JOIN bucket_job_swagger bjs on(bjs.bucket = pfr.bucket AND bjs.job = pfr.job) 
    order by podspec_field;
 #+END_SRC

 #+RESULTS:
 :results:
          podspec_field         | e2e_hits |         job         |    job_timestamp    
 -------------------------------+----------+---------------------+---------------------
  activeDeadlineSeconds         |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  activeDeadlineSeconds         |        0 | live                | 2019-12-04 20:14:50
  affinity                      |        0 | live                | 2019-12-04 20:14:50
  affinity                      |     2264 | 1202311785298792448 | 2019-12-04 20:14:50
  automountServiceAccountToken  |      184 | 1202311785298792448 | 2019-12-04 20:14:50
  automountServiceAccountToken  |        0 | live                | 2019-12-04 20:14:50
  containers                    |        0 | live                | 2019-12-04 20:14:50
  containers                    |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  dnsConfig                     |        0 | live                | 2019-12-04 20:14:50
  dnsConfig                     |       32 | 1202311785298792448 | 2019-12-04 20:14:50
  dnsPolicy                     |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  dnsPolicy                     |        0 | live                | 2019-12-04 20:14:50
  enableServiceLinks            |    26592 | 1202311785298792448 | 2019-12-04 20:14:50
  enableServiceLinks            |        0 | live                | 2019-12-04 20:14:50
  ephemeralContainers           |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  ephemeralContainers           |        0 | live                | 2019-12-04 20:14:50
  hostAliases                   |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  hostAliases                   |        0 | live                | 2019-12-04 20:14:50
  hostIPC                       |        0 | live                | 2019-12-04 20:14:50
  hostIPC                       |       64 | 1202311785298792448 | 2019-12-04 20:14:50
  hostname                      |      260 | 1202311785298792448 | 2019-12-04 20:14:50
  hostname                      |        0 | live                | 2019-12-04 20:14:50
  hostNetwork                   |     6296 | 1202311785298792448 | 2019-12-04 20:14:50
  hostNetwork                   |        0 | live                | 2019-12-04 20:14:50
  hostPID                       |        0 | live                | 2019-12-04 20:14:50
  hostPID                       |       64 | 1202311785298792448 | 2019-12-04 20:14:50
  imagePullSecrets              |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  imagePullSecrets              |        0 | live                | 2019-12-04 20:14:50
  initContainers                |     3944 | 1202311785298792448 | 2019-12-04 20:14:50
  initContainers                |        0 | live                | 2019-12-04 20:14:50
  nodeName                      |    18476 | 1202311785298792448 | 2019-12-04 20:14:50
  nodeName                      |        0 | live                | 2019-12-04 20:14:50
  nodeSelector                  |     2252 | 1202311785298792448 | 2019-12-04 20:14:50
  nodeSelector                  |        0 | live                | 2019-12-04 20:14:50
  overhead                      |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  overhead                      |        0 | live                | 2019-12-04 20:14:50
  preemptionPolicy              |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  preemptionPolicy              |        0 | live                | 2019-12-04 20:14:50
  priority                      |      180 | 1202311785298792448 | 2019-12-04 20:14:50
  priority                      |        0 | live                | 2019-12-04 20:14:50
  priorityClassName             |        0 | live                | 2019-12-04 20:14:50
  priorityClassName             |      128 | 1202311785298792448 | 2019-12-04 20:14:50
  readinessGates                |        0 | live                | 2019-12-04 20:14:50
  readinessGates                |       32 | 1202311785298792448 | 2019-12-04 20:14:50
  restartPolicy                 |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  restartPolicy                 |        0 | live                | 2019-12-04 20:14:50
  runtimeClassName              |        0 | live                | 2019-12-04 20:14:50
  runtimeClassName              |      184 | 1202311785298792448 | 2019-12-04 20:14:50
  schedulerName                 |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  schedulerName                 |        0 | live                | 2019-12-04 20:14:50
  securityContext               |        0 | live                | 2019-12-04 20:14:50
  securityContext               |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  serviceAccount                |     5244 | 1202311785298792448 | 2019-12-04 20:14:50
  serviceAccount                |        0 | live                | 2019-12-04 20:14:50
  serviceAccountName            |     5244 | 1202311785298792448 | 2019-12-04 20:14:50
  serviceAccountName            |        0 | live                | 2019-12-04 20:14:50
  shareProcessNamespace         |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  shareProcessNamespace         |        0 | live                | 2019-12-04 20:14:50
  subdomain                     |        0 | live                | 2019-12-04 20:14:50
  subdomain                     |      260 | 1202311785298792448 | 2019-12-04 20:14:50
  terminationGracePeriodSeconds |    44772 | 1202311785298792448 | 2019-12-04 20:14:50
  terminationGracePeriodSeconds |        0 | live                | 2019-12-04 20:14:50
  tolerations                   |      180 | 1202311785298792448 | 2019-12-04 20:14:50
  tolerations                   |        0 | live                | 2019-12-04 20:14:50
  topologySpreadConstraints     |        0 | live                | 2019-12-04 20:14:50
  topologySpreadConstraints     |        0 | 1202311785298792448 | 2019-12-04 20:14:50
  volumes                       |    27044 | 1202311785298792448 | 2019-12-04 20:14:50
  volumes                       |        0 | live                | 2019-12-04 20:14:50
 (68 rows)

 :end:

*** 480: materialized kind_field_path_coverage
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/480_kind_field_path_coverage_material.up.sql
    :END:
    This is the base view we use to traverse the paths later.  It grabs all relevant fields from our kind_field_path_recursion and joins it to our audit_events based on where the request_object of the event includes the relevant fieldpath.
   
    #+NAME: kind_field_path_coverage_material_improved
    #+BEGIN_SRC sql-mode
      CREATE MATERIALIZED VIEW "public"."kind_field_path_coverage_material" AS
      SELECT
        kfpr.bucket,
        kfpr.job,
        kfpr.kind,
        kfpr.field_path,
        kfpr.field_kind,
        kfpr.sub_kind,
        (array_length(string_to_array(kfpr.field_path, '.'),1) - 1) as distance,
        ae.audit_id as audit_event_id,
        ae.useragent as useragent,
        ae.operation_id
        FROM kind_field_path_recursion kfpr
            LEFT JOIN LATERAL (select * from audit_event WHERE param_schema = kfpr.kind AND jsonb_path_exists(request_object, ('$.'||kfpr.field_path)::jsonpath)) ae ON true
        GROUP BY kfpr.kind, kfpr.field_path, kfpr.field_kind, kfpr.bucket, kfpr.job, kfpr.sub_kind, ae.audit_id, ae.useragent, ae.operation_id; 
    #+END_SRC
    #+begin_src sql-mode
     refresh materialized view kind_field_path_coverage_material; 
    #+end_src
   
*** 485: kind_field_path_coverage
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/485_kind_field_path_coverage.up.sql
    :END:
    A view into our material,  so hasura can track it.
    #+NAME: kind_field_path_coverage
    #+BEGIN_SRC sql-mode
      CREATE OR REPLACE VIEW "public"."kind_field_path_coverage" AS
       select * from kind_field_path_coverage_material;
    #+END_SRC
*** 490: materialized full_podspec_field_coverage
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/490_full_podspec_field_coverage_material.up.sql
    :END:
    We want a subset of this grand field_coverage view, looking only for fields that come from Podspec.    
    This is going to look across all our buckets and jobs, so it will take a bit of time to materialize.
   
    We are only looking at the stable, core kinds or the GA kinds.
   
    #+NAME: full_podspec_field_coverage_material
    #+BEGIN_SRC sql-mode
      CREATE MATERIALIZED VIEW "public"."full_podspec_field_coverage_material" AS
        WITH podspec_kinds AS (
              SELECT DISTINCT kind, field_path
                FROM kind_field_path_coverage
                 WHERE field_kind = 'io.k8s.api.core.v1.PodSpec'
                 AND kind not like '%alpha%'
                 AND kind not like '%beta%'
                 AND operation_id is not null
        )
        SELECT DISTINCT
          trim(leading 'io.k8s.api.' from c.kind) as kind,
          trim(leading 'io.k8s.api.' from c.sub_kind) as  sub_kind,
          c.field_path,
          distance,
          count(*) FILTER(WHERE c.useragent like 'e2e.test%') as test_hits,
          count(*) FILTER(WHERE c.useragent like '%[Conformance]%') as conf_hits,
          c.field_kind,
          c.job,
          c.bucket
          FROM kind_field_path_coverage c
            INNER JOIN podspec_kinds pk ON (c.kind = pk.kind AND c.field_path like  pk.field_path || '%')
            and sub_kind not like '%VolumeSource'
            GROUP BY c.sub_kind, c.kind, c.field_path, c.field_kind, c.distance, pk.field_path, c.job, c.bucket
            ORDER BY field_path;
    #+END_SRC

    #+begin_src sql-mode
    drop materialized view full_podspec_field_coverage_material cascade;

    #+end_src
*** 495: full_podspec_field_coverage
    :PROPERTIES:
    :header-args:sql-mode+: :tangle ../apps/hasura/migrations/495_full_podspec_field_coverage.up.sql
    :END:
    And we can create a view from this
    #+NAME: full_podspec_field_coverage
    #+BEGIN_SRC sql-mode
     CREATE OR REPLACE VIEW "public"."full_podspec_field_coverage" AS
      select * from full_podspec_field_coverage_material;
    #+END_SRC
   
   
    When using the view, you will want ot make sure to limit it by a job, otherwise you'll get massive results.
   
    for example


     #+begin_src sql-mode :results replace code :wrap EXAMPLE
     \d+ full_podspec_field_coverage;
     #+end_src

     #+RESULTS:
     #+begin_EXAMPLE
                        View "public.full_podspec_field_coverage"
        Column   |  Type   | Collation | Nullable | Default | Storage  | Description 
     ------------+---------+-----------+----------+---------+----------+-------------
      kind       | text    |           |          |         | extended | 
      sub_kind   | text    |           |          |         | extended | 
      field_path | text    |           |          |         | extended | 
      distance   | integer |           |          |         | plain    | 
      test_hits  | bigint  |           |          |         | plain    | 
      conf_hits  | bigint  |           |          |         | plain    | 
      field_kind | text    |           |          |         | extended | 
      job        | text    |           |          |         | extended | 
      bucket     | text    |           |          |         | extended | 
     View definition:
      SELECT full_podspec_field_coverage_material.kind,
         full_podspec_field_coverage_material.sub_kind,
         full_podspec_field_coverage_material.field_path,
         full_podspec_field_coverage_material.distance,
         full_podspec_field_coverage_material.test_hits,
         full_podspec_field_coverage_material.conf_hits,
         full_podspec_field_coverage_material.field_kind,
         full_podspec_field_coverage_material.job,
         full_podspec_field_coverage_material.bucket
        FROM full_podspec_field_coverage_material;

     #+end_EXAMPLE
    #+name: kind_field_coverage_nolive
    #+begin_src sql-mode
      CREATE OR REPLACE VIEW "public"."kind_field_path_coverage" AS
       select * from kind_field_path_coverage_material where job != 'live';
      refresh materialized view kind_field_path_coverage_material; 
    #+end_src
    #+name: full_podspec_field_coverage_nolive
    #+begin_src sql-mode
      CREATE OR REPLACE VIEW "public"."full_podspec_field_path_coverage" AS
       select * from kind_field_path_coverage_material where job != 'live';
      refresh materialized view full_podspec_field_path_coverage_material; 
    #+end_src

   
** for aaron                                                         :export:
    #+begin_src sql-mode :results replace :tangle no :eval never-export :exports both :file results.txt
    select kind, sub_kind, field_path, test_hits, distance from full_podspec_field_coverage where job != 'live';
    #+end_src

    #+RESULTS:
    #+begin_src sql-mode
    [[file:results.txt]]
    #+end_src

** ASKS
*** kindnet-image pull.... requires internet makes sad
*** remove latest tag on auditlogger, replace with date
*** auditlogger depend on hasura (similar to hasura -> pg)
*** :eval ask for kind cluster delete
    or move to code block eval :never or move to own block
*** put the kind image pull stuff in footnotes
 with not to look at [[#footnotes]]
*** fix namespace stuf... maybe use default instead of kube-system
*** kindnetd old
   #+BEGIN_SRC tmate :eval never-export
     kind load image-archive            kindnetd:aa67fec7d7ef7.docker-image \
       || docker pull docker.io/kindest/kindnetd:aa67fec7d7ef7 \
       && docker save docker.io/kindest/kindnetd:aa67fec7d7ef7 -o kindnetd:aa67fec7d7ef7.docker-image
   #+END_SRC
*** kind load via registry
   #+BEGIN_SRC tmate :eval never-export
     # Seems a bit slow... loads from image-archives are much faster
     # kind load docker-image --name=kind-$USER raiinbow/hasura:2019-12-03-16-31 
     # kind load docker-image --name=kind-$USER raiinbow/postgres:2019-12-03-14-19
     # kind load docker-image --name=kind-$USER raiinbow/auditlogger:latest
   #+END_SRC


# Local Variables:
# ii: enabled
# End:
